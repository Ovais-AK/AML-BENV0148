{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import json\n",
    "from dataset import Dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocf_blosc2 import Blosc2\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime, time, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selects the gpu if available (when running on Google Colab) otherwise on the local cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block downloads the data from huggingface. This is only required if working on Google Colab OR data has not yet been downloaded locally. Expect this cell to run for up to 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download site locations (indices.json) and data (pv and satellite-hrv)\n",
    "\n",
    "# if not os.path.exists(\"submission\"):\n",
    "#      os.makedirs(\"submission\", exist_ok=True)\n",
    "#      #Installing locally means you do not need to rerun this each time you restart the notebook\n",
    "#      !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/indices.json --output indices.json\n",
    "\n",
    "# if not os.path.exists(\"data\"):\n",
    "#     os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/pv/2021\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2021\", exist_ok=True)\n",
    "\n",
    "#     !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "\n",
    "#      # Download data for June, July, August 2020 and 2021\n",
    "#     for summer_months in range (6,9):\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/{summer_months}.parquet --output data/pv/2020/{summer_months}.parquet\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2021/{summer_months}.parquet --output data/pv/2021/{summer_months}.parquet     \n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/{summer_months}.zarr.zip --output data/satellite-hrv/2020/{summer_months}.zarr.zip\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2021/{summer_months}.zarr.zip --output data/satellite-hrv/2021/{summer_months}.zarr.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load pv, hrv and indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pv data into a single dataframe\n",
    "pv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        pv.append(pd.read_parquet(f\"data/pv/{year}/{month}.parquet\").drop(\"generation_wh\", axis=1))  \n",
    "pv = pd.concat(pv)\n",
    "pv.index = pv.index.set_levels([pv.index.levels[0].tz_localize(None), pv.index.levels[1]])\n",
    " \n",
    "# The parquet data here is similar to a dataframe. The \"power\" is the column with the other data types being indexes. The data is shaped with each timestamp being its own \n",
    "# subframe with the sites having their corresponding power (% of site capacity).  \n",
    "hrv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        hrv.append(xr.open_dataset(f\"data/satellite-hrv/{year}/{month}.zarr.zip\", engine=\"zarr\", chunks=\"auto\"))\n",
    "\n",
    "hrv = xr.concat(hrv, dim=\"time\")\n",
    "\n",
    "# Images are stored as vectors. The vectors are stored as an array of vectors. The arrays have a timestamp. Since there is only one channel (hrv)\n",
    "# the array is a 1D set of vectors with the dimension being time. Read this to help you understand how this is being stored \n",
    "# https://tutorial.xarray.dev/fundamentals/01_datastructures.html\n",
    "with open(\"indices.json\") as f:\n",
    "    site_locations = {\n",
    "        data_source: {\n",
    "            int(site): (int(location[0]), int(location[1]))\n",
    "            for site, location in locations.items() #if site == '2607'#added this to run only 1 site location to understand how it works\n",
    "        }\n",
    "        for data_source, locations in json.load(f).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ChallengeDataset(IterableDataset):\n",
    "    def __init__(self, pv, hrv, site_locations, start_date = \"2020-7-1\", end_date = \"2020-7-30\", sites=None):\n",
    "        self.pv = pv\n",
    "        self.hrv = hrv\n",
    "        self._site_locations = site_locations\n",
    "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())#This gets the individual site ids which are stored as the dict's keys\n",
    "        self.start_date = list(map(int, start_date.split(\"-\")))\n",
    "        self.end_date= list(map(int, end_date.split(\"-\")))\n",
    "\n",
    "    def _get_image_times(self):#This function starts at the minimum date in the set and iterates up to the highest date, this is done as the data set is large and due to the nature of the parquette and xarray\n",
    "        min_date = datetime(self.start_date[0], self.start_date[1], self.start_date[2])\n",
    "        max_date = datetime(self.end_date[0], self.end_date[1], self.end_date[2])\n",
    "        \n",
    "        start_time = time(8)\n",
    "        end_time = time(17)\n",
    "\n",
    "        date = min_date \n",
    "        while date <= max_date: \n",
    "            current_time = datetime.combine(date, start_time)\n",
    "            while current_time.time() < end_time:\n",
    "                if current_time:\n",
    "                    yield current_time\n",
    "\n",
    "                current_time += timedelta(minutes=60)\n",
    "\n",
    "            date += timedelta(days=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for time in self._get_image_times():\n",
    "            \n",
    "            # # generate time ids for predictions to be analysedm after training\n",
    "            # time_ids = pd.date_range(start=time + timedelta(hours=1),\n",
    "            #                          end=time + timedelta(hours=1)+timedelta(minutes=55),\n",
    "            #                          freq='5min')\n",
    "            # time_ids = time_ids.strftime('%Y-%m-%dT%H:%M:%S').tolist()  \n",
    "\n",
    "            # 1 hour leading up to the predicton time        \n",
    "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
    "\n",
    "            # PV power output in first hour\n",
    "            pv_features = pv.xs(first_hour, drop_level=False)\n",
    "\n",
    "            # PV power output in the next 48 hours\n",
    "            pv_targets = pv.xs(\n",
    "                slice(  # type: ignore\n",
    "                    str(time + timedelta(hours=1)),\n",
    "                    str(time + timedelta(hours=1, minutes=55)),\n",
    "                ),\n",
    "                drop_level=False,\n",
    "            )\n",
    "\n",
    "           # hrv satellite images on first hour timestamps setting them up as an input feature\n",
    "            hrv_data = self.hrv[\"data\"].sel(time=first_hour).to_numpy()\n",
    "\n",
    "            for site in self._sites:\n",
    "                try:\n",
    "                    # Get solar PV features and targets, the site_targets is used to find the models loss\n",
    "                    site_features = pv_features.xs(site, level=1).to_numpy().squeeze(-1) # gets the pixel based location of the pv site and then uses this to make predictions based on the individual sites\n",
    "                    site_targets = pv_targets.xs(site, level=1).to_numpy().squeeze(-1)\n",
    "                    assert site_features.shape == (12,) and site_targets.shape == (12,)\n",
    "                 \n",
    "                    # Get a (2*crop_size + 1)^2 HRV crop centred on the site over the previous hour\n",
    "                    x, y = self._site_locations[\"hrv\"][site]\n",
    "                    hrv_features = hrv_data[:, y - 1  : y + 1 ,\n",
    "                                             x - 1  : x + 1 , 0]\n",
    "                    assert hrv_features.shape == (12, 2, 2)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                yield site_features, hrv_features, site_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train, validation and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1 # forecast horizon in hours\n",
    "crop_size = 1 # number of pixels of hrv image to crop around each site location\n",
    "BATCH_SIZE = 64 # number of samples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "train_dataset = ChallengeDataset(pv, hrv, site_locations, start_date=\"2020-06-01\", end_date=\"2020-08-31\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory = True)\n",
    "\n",
    "# # validation\n",
    "# validation_dataset = ChallengeDataset(pv, hrv, site_locations=site_locations,\n",
    "#                                       start_date=\"2021-06-01\", end_date=\"2021-06-08\") \n",
    "# validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "# # test\n",
    "# test_dataset = ChallengeDataset(pv, hrv, site_locations,\n",
    "#                         start_date=\"2021-07-01\", end_date=\"2020-08-31\",\n",
    "#                         crop_size = crop_size, horizon = horizon)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop1_60m(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop1_60m, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Shape after avgpool:\", x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(f\"Sshape of x = {x.shape} shape of pv = {pv.shape}\")\n",
    "        #x = torch.concat((x, pv), dim=-1)\n",
    "        #print(\"Shape after avgpool and flatten:\", x.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        #pv = pv.view(pv.size(0), -1)\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(\"Adjusted PV shape:\", pv.shape)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet_light_deep_crop1_60m(BasicBlock, layers).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "5056\n",
      "5120\n",
      "5184\n",
      "5248\n",
      "5312\n",
      "5376\n",
      "5440\n",
      "5504\n",
      "5568\n",
      "5632\n",
      "5696\n",
      "5760\n",
      "5824\n",
      "5888\n",
      "5952\n",
      "6016\n",
      "6080\n",
      "6144\n",
      "6208\n",
      "6272\n",
      "6336\n",
      "6400\n",
      "6464\n",
      "6528\n",
      "6592\n",
      "6656\n",
      "6720\n",
      "6784\n",
      "6848\n",
      "6912\n",
      "6976\n",
      "7040\n",
      "7104\n",
      "7168\n",
      "7232\n",
      "7296\n",
      "7360\n",
      "7424\n",
      "7488\n",
      "7552\n",
      "7616\n",
      "7680\n",
      "7744\n",
      "7808\n",
      "7872\n",
      "7936\n",
      "8000\n",
      "8064\n",
      "8128\n",
      "8192\n",
      "8256\n",
      "8320\n",
      "8384\n",
      "8448\n",
      "8512\n",
      "8576\n",
      "8640\n",
      "8704\n",
      "8768\n",
      "8832\n",
      "8896\n",
      "8960\n",
      "9024\n",
      "9088\n",
      "9152\n",
      "9216\n",
      "9280\n",
      "9344\n",
      "9408\n",
      "9472\n",
      "9536\n",
      "9600\n",
      "9664\n",
      "9728\n",
      "9792\n",
      "9856\n",
      "9920\n",
      "9984\n",
      "10048\n",
      "10112\n",
      "10176\n",
      "10240\n",
      "10304\n",
      "10368\n",
      "10432\n",
      "10496\n",
      "10560\n",
      "10624\n",
      "10688\n",
      "10752\n",
      "10816\n",
      "10880\n",
      "10944\n",
      "11008\n",
      "11072\n",
      "11136\n",
      "11200\n",
      "11264\n",
      "11328\n",
      "11392\n",
      "11456\n",
      "11520\n",
      "11584\n",
      "11648\n",
      "11712\n",
      "11776\n",
      "11840\n",
      "11904\n",
      "11968\n",
      "12032\n",
      "12096\n",
      "12160\n",
      "12224\n",
      "12288\n",
      "12352\n",
      "12416\n",
      "12480\n",
      "12544\n",
      "12608\n",
      "12672\n",
      "12736\n",
      "12800\n",
      "12864\n",
      "12928\n",
      "12992\n",
      "13056\n",
      "13120\n",
      "13184\n",
      "13248\n",
      "13312\n",
      "13376\n",
      "13440\n",
      "13504\n",
      "13568\n",
      "13632\n",
      "13696\n",
      "13760\n",
      "13824\n",
      "13888\n",
      "13952\n",
      "14016\n",
      "14080\n",
      "14144\n",
      "14208\n",
      "14272\n",
      "14336\n",
      "14400\n",
      "14464\n",
      "14528\n",
      "14592\n",
      "14656\n",
      "14720\n",
      "14784\n",
      "14848\n",
      "14912\n",
      "14976\n",
      "15040\n",
      "15104\n",
      "15168\n",
      "15232\n",
      "15296\n",
      "15360\n",
      "15424\n",
      "15488\n",
      "15552\n",
      "15616\n",
      "15680\n",
      "15744\n",
      "15808\n",
      "15872\n",
      "15936\n",
      "16000\n",
      "16064\n",
      "16128\n",
      "16192\n",
      "16256\n",
      "16320\n",
      "16384\n",
      "16448\n",
      "16512\n",
      "16576\n",
      "16640\n",
      "16704\n",
      "16768\n",
      "16832\n",
      "16896\n",
      "16960\n",
      "17024\n",
      "17088\n",
      "17152\n",
      "17216\n",
      "17280\n",
      "17344\n",
      "17408\n",
      "17472\n",
      "17536\n",
      "17600\n",
      "17664\n",
      "17728\n",
      "17792\n",
      "17856\n",
      "17920\n",
      "17984\n",
      "18048\n",
      "18112\n",
      "18176\n",
      "18240\n",
      "18304\n",
      "18368\n",
      "18432\n",
      "18496\n",
      "18560\n",
      "18624\n",
      "18688\n",
      "18752\n",
      "18816\n",
      "18880\n",
      "18944\n",
      "19008\n",
      "19072\n",
      "19136\n",
      "19200\n",
      "19264\n",
      "19328\n",
      "19392\n",
      "19456\n",
      "19520\n",
      "19584\n",
      "19648\n",
      "19712\n",
      "19776\n",
      "19840\n",
      "19904\n",
      "19968\n",
      "20032\n",
      "20096\n",
      "20160\n",
      "20224\n",
      "20288\n",
      "20352\n",
      "20416\n",
      "20480\n",
      "20544\n",
      "20608\n",
      "20672\n",
      "20736\n",
      "20800\n",
      "20864\n",
      "20928\n",
      "20992\n",
      "21056\n",
      "21120\n",
      "21184\n",
      "21248\n",
      "21312\n",
      "21376\n",
      "21440\n",
      "21504\n",
      "21568\n",
      "21632\n",
      "21696\n",
      "21760\n",
      "21824\n",
      "21888\n",
      "21952\n",
      "22016\n",
      "22080\n",
      "22144\n",
      "22208\n",
      "22272\n",
      "22336\n",
      "22400\n",
      "22464\n",
      "22528\n",
      "22592\n",
      "22656\n",
      "22720\n",
      "22784\n",
      "22848\n",
      "22912\n",
      "22976\n",
      "23040\n",
      "23104\n",
      "23168\n",
      "23232\n",
      "23296\n",
      "23360\n",
      "23424\n",
      "23488\n",
      "23552\n",
      "23616\n",
      "23680\n",
      "23744\n",
      "23808\n",
      "23872\n",
      "23936\n",
      "24000\n",
      "24064\n",
      "24128\n",
      "24192\n",
      "24256\n",
      "24320\n",
      "24384\n",
      "24448\n",
      "24512\n",
      "24576\n",
      "24640\n",
      "24704\n",
      "24768\n",
      "24832\n",
      "24896\n",
      "24960\n",
      "25024\n",
      "25088\n",
      "25152\n",
      "25216\n",
      "25280\n",
      "25344\n",
      "25408\n",
      "25472\n",
      "25536\n",
      "Epoch 1, 200: 0.18065074626356364\n",
      "25600\n",
      "25664\n",
      "25728\n",
      "25792\n",
      "25856\n",
      "25920\n",
      "25984\n",
      "26048\n",
      "26112\n",
      "26176\n",
      "26240\n",
      "26304\n",
      "26368\n",
      "26432\n",
      "26496\n",
      "26560\n",
      "26624\n",
      "26688\n",
      "26752\n",
      "26816\n",
      "26880\n",
      "26944\n",
      "27008\n",
      "27072\n",
      "27136\n",
      "27200\n",
      "27264\n",
      "27328\n",
      "27392\n",
      "27456\n",
      "27520\n",
      "27584\n",
      "27648\n",
      "27712\n",
      "27776\n",
      "27840\n",
      "27904\n",
      "27968\n",
      "28032\n",
      "28096\n",
      "28160\n",
      "28224\n",
      "28288\n",
      "28352\n",
      "28416\n",
      "28480\n",
      "28544\n",
      "28608\n",
      "28672\n",
      "28736\n",
      "28800\n",
      "28864\n",
      "28928\n",
      "28992\n",
      "29056\n",
      "29120\n",
      "29184\n",
      "29248\n",
      "29312\n",
      "29376\n",
      "29440\n",
      "29504\n",
      "29568\n",
      "29632\n",
      "29696\n",
      "29760\n",
      "29824\n",
      "29888\n",
      "29952\n",
      "30016\n",
      "30080\n",
      "30144\n",
      "30208\n",
      "30272\n",
      "30336\n",
      "30400\n",
      "30464\n",
      "30528\n",
      "30592\n",
      "30656\n",
      "30720\n",
      "30784\n",
      "30848\n",
      "30912\n",
      "30976\n",
      "31040\n",
      "31104\n",
      "31168\n",
      "31232\n",
      "31296\n",
      "31360\n",
      "31424\n",
      "31488\n",
      "31552\n",
      "31616\n",
      "31680\n",
      "31744\n",
      "31808\n",
      "31872\n",
      "31936\n",
      "32000\n",
      "32064\n",
      "32128\n",
      "32192\n",
      "32256\n",
      "32320\n",
      "32384\n",
      "32448\n",
      "32512\n",
      "32576\n",
      "32640\n",
      "32704\n",
      "32768\n",
      "32832\n",
      "32896\n",
      "32960\n",
      "33024\n",
      "33088\n",
      "33152\n",
      "33216\n",
      "33280\n",
      "33344\n",
      "33408\n",
      "33472\n",
      "33536\n",
      "33600\n",
      "33664\n",
      "33728\n",
      "33792\n",
      "33856\n",
      "33920\n",
      "33984\n",
      "34048\n",
      "34112\n",
      "34176\n",
      "34240\n",
      "34304\n",
      "34368\n",
      "34432\n",
      "34496\n",
      "34560\n",
      "34624\n",
      "34688\n",
      "34752\n",
      "34816\n",
      "34880\n",
      "34944\n",
      "35008\n",
      "35072\n",
      "35136\n",
      "35200\n",
      "35264\n",
      "35328\n",
      "35392\n",
      "35456\n",
      "35520\n",
      "35584\n",
      "35648\n",
      "35712\n",
      "35776\n",
      "35840\n",
      "35904\n",
      "35968\n",
      "36032\n",
      "36096\n",
      "36160\n",
      "36224\n",
      "36288\n",
      "36352\n",
      "36416\n",
      "36480\n",
      "36544\n",
      "36608\n",
      "36672\n",
      "36736\n",
      "36800\n",
      "36864\n",
      "36928\n",
      "36992\n",
      "37056\n",
      "37120\n",
      "37184\n",
      "37248\n",
      "37312\n",
      "37376\n",
      "37440\n",
      "37504\n",
      "37568\n",
      "37632\n",
      "37696\n",
      "37760\n",
      "37824\n",
      "37888\n",
      "37952\n",
      "38016\n",
      "38080\n",
      "38144\n",
      "38208\n",
      "38272\n",
      "38336\n",
      "38400\n",
      "38464\n",
      "38528\n",
      "38592\n",
      "38656\n",
      "38720\n",
      "38784\n",
      "38848\n",
      "38912\n",
      "38976\n",
      "39040\n",
      "39104\n",
      "39168\n",
      "39232\n",
      "39296\n",
      "39360\n",
      "39424\n",
      "39488\n",
      "39552\n",
      "39616\n",
      "39680\n",
      "39744\n",
      "39808\n",
      "39872\n",
      "39936\n",
      "40000\n",
      "40064\n",
      "40128\n",
      "40192\n",
      "40256\n",
      "40320\n",
      "40384\n",
      "40448\n",
      "40512\n",
      "40576\n",
      "40640\n",
      "40704\n",
      "40768\n",
      "40832\n",
      "40896\n",
      "40960\n",
      "41024\n",
      "41088\n",
      "41152\n",
      "41216\n",
      "41280\n",
      "41344\n",
      "41408\n",
      "41472\n",
      "41536\n",
      "41600\n",
      "41664\n",
      "41728\n",
      "41792\n",
      "41856\n",
      "41920\n",
      "41984\n",
      "42048\n",
      "42112\n",
      "42176\n",
      "42240\n",
      "42304\n",
      "42368\n",
      "42432\n",
      "42496\n",
      "42560\n",
      "42624\n",
      "42688\n",
      "42752\n",
      "42816\n",
      "42880\n",
      "42944\n",
      "43008\n",
      "43072\n",
      "43136\n",
      "43200\n",
      "43264\n",
      "43328\n",
      "43392\n",
      "43456\n",
      "43520\n",
      "43584\n",
      "43648\n",
      "43712\n",
      "43776\n",
      "43840\n",
      "43904\n",
      "43968\n",
      "44032\n",
      "44096\n",
      "44160\n",
      "44224\n",
      "44288\n",
      "44352\n",
      "44416\n",
      "44480\n",
      "44544\n",
      "44608\n",
      "44672\n",
      "44736\n",
      "44800\n",
      "44864\n",
      "44928\n",
      "44992\n",
      "45056\n",
      "45120\n",
      "45184\n",
      "45248\n",
      "45312\n",
      "45376\n",
      "45440\n",
      "45504\n",
      "45568\n",
      "45632\n",
      "45696\n",
      "45760\n",
      "45824\n",
      "45888\n",
      "45952\n",
      "46016\n",
      "46080\n",
      "46144\n",
      "46208\n",
      "46272\n",
      "46336\n",
      "46400\n",
      "46464\n",
      "46528\n",
      "46592\n",
      "46656\n",
      "46720\n",
      "46740\n",
      "Epoch 1: 0.13819251694454404\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "EPOCHS = 1\n",
    "batch_losses = []\n",
    "val_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0 ##sets the starting loss at zero\n",
    "    count = 0 #is used to keep track of the number of batches passed through the training model\n",
    "  \n",
    "    for i, (pv_features, hrv_features, pv_targets) in enumerate(train_dataloader): \n",
    "        \n",
    "        optimiser.zero_grad()#resets the gradient of all the previous weights and biases used in the model, can be changed to alter the type of optimiser we use\n",
    "        predictions = model(\n",
    "            pv_features.to(device, dtype=torch.float),\n",
    "            hrv_features.to(device, dtype=torch.float),\n",
    "        )#makes predictions based off of current batch of hrv and pv inputs\n",
    "        loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))#calculates the loss between the models predictions and the actual pv\n",
    "        loss.backward()#backprops the loss\n",
    "\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item() * pv_targets.size(0)\n",
    "        print(count)\n",
    "        count += pv_targets.size(0)\n",
    "        print(count)\n",
    "        optimiser.step()\n",
    "        \n",
    "        size = int(pv_targets.size(0))\n",
    "        running_loss += float(loss) * size\n",
    "        count += size\n",
    "   \n",
    "        if i % 200 == 199:\n",
    "            print(f\"Epoch {epoch + 1}, {i + 1}: {running_loss / count}\")\n",
    "            \n",
    "    epoch_train_loss = running_loss / count\n",
    "    epoch_train_losses.append(epoch_train_loss)        \n",
    "    print(f\"Epoch {epoch + 1}: {running_loss / count}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
