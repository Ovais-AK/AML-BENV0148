{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import json\n",
    "from dataset import Dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocf_blosc2 import Blosc2\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime, time, timedelta\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selects the gpu if available (when running on Google Colab) otherwise on the local cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block downloads the data from huggingface. This is only required if working on Google Colab OR data has not yet been downloaded locally. Expect this cell to run for up to 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download site locations (indices.json) and data (pv and satellite-hrv)\n",
    "\n",
    "# if not os.path.exists(\"submission\"):\n",
    "#      os.makedirs(\"submission\", exist_ok=True)\n",
    "#      #Installing locally means you do not need to rerun this each time you restart the notebook\n",
    "#      !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/indices.json --output indices.json\n",
    "\n",
    "# if not os.path.exists(\"data\"):\n",
    "#     os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/pv/2021\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2021\", exist_ok=True)\n",
    "\n",
    "#     !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "\n",
    "#      # Download data for June, July, August 2020 and 2021\n",
    "#     for summer_months in range (6,9):\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/{summer_months}.parquet --output data/pv/2020/{summer_months}.parquet\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2021/{summer_months}.parquet --output data/pv/2021/{summer_months}.parquet     \n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/{summer_months}.zarr.zip --output data/satellite-hrv/2020/{summer_months}.zarr.zip\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2021/{summer_months}.zarr.zip --output data/satellite-hrv/2021/{summer_months}.zarr.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load pv, hrv and indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pv data into a single dataframe\n",
    "pv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        pv.append(pd.read_parquet(f\"data/pv/{year}/{month}.parquet\").drop(\"generation_wh\", axis=1))  \n",
    "pv = pd.concat(pv)\n",
    "pv.index = pv.index.set_levels([pv.index.levels[0].tz_localize(None), pv.index.levels[1]])\n",
    " \n",
    "# The parquet data here is similar to a dataframe. The \"power\" is the column with the other data types being indexes. The data is shaped with each timestamp being its own \n",
    "# subframe with the sites having their corresponding power (% of site capacity).  \n",
    "hrv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        hrv.append(xr.open_dataset(f\"data/satellite-hrv/{year}/{month}.zarr.zip\", engine=\"zarr\", chunks=\"auto\"))\n",
    "\n",
    "hrv = xr.concat(hrv, dim=\"time\")\n",
    "\n",
    "# Images are stored as vectors. The vectors are stored as an array of vectors. The arrays have a timestamp. Since there is only one channel (hrv)\n",
    "# the array is a 1D set of vectors with the dimension being time. Read this to help you understand how this is being stored \n",
    "# https://tutorial.xarray.dev/fundamentals/01_datastructures.html\n",
    "with open(\"indices.json\") as f:\n",
    "    site_locations = {\n",
    "        data_source: {\n",
    "            int(site): (int(location[0]), int(location[1]))\n",
    "            for site, location in locations.items() #if site == '2607'#added this to run only 1 site location to understand how it works\n",
    "        }\n",
    "        for data_source, locations in json.load(f).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train, validation and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1 # forecast horizon in hours\n",
    "crop_size = 1 # number of pixels of hrv image to crop around each site location\n",
    "BATCH_SIZE = 32 # number of samples per batch\n",
    "\n",
    "train_start_date = \"2020-06-01\"\n",
    "train_end_date = \"2020-07-01\"\n",
    "validation_start_date = \"2021-06-01\"\n",
    "validation_end_date = \"2021-06-08\"\n",
    "test_start_date = \"2021-07-01\"\n",
    "test_end_date = \"2021-07-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ChallengeDataset\n",
    "\n",
    "# train \n",
    "train_dataset = ChallengeDataset(pv, hrv, site_locations, start_date=train_start_date, end_date=train_end_date)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory = True)\n",
    "\n",
    "# validation\n",
    "validation_dataset = ChallengeDataset(pv, hrv, site_locations=site_locations,\n",
    "                                      start_date=validation_start_date, end_date=validation_end_date) \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "# # test\n",
    "# test_dataset = ChallengeDataset(pv, hrv, site_locations,\n",
    "#                         start_date=\"2021-07-01\", end_date=\"2020-08-31\",\n",
    "#                         crop_size = crop_size, horizon = horizon)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop1_60m(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop1_60m, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet_light_deep_crop1_60m(BasicBlock, layers).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 200: 0.2313368734717369\n",
      "Epoch 1, 400: 0.16830456409603356\n",
      "Epoch 1, 600: 0.1387131819749872\n",
      "Epoch 1, 800: 0.12496616459684447\n",
      "Epoch 1, 1000: 0.12240195009671152\n",
      "Epoch 1, 1200: 0.13098085552298774\n",
      "Epoch 1, 1400: 0.13068471089936792\n",
      "Epoch 1, 1600: 0.12899212823598646\n",
      "Epoch 1, 1800: 0.12609175368005202\n",
      "Epoch 1, 2000: 0.12530114293377848\n",
      "Epoch 1, 2200: 0.12314384496008808\n",
      "Epoch 1, 2400: 0.11878239588501553\n",
      "Epoch 1, 2600: 0.1175394394635581\n",
      "Epoch 1, 2800: 0.115860449936507\n",
      "Epoch 1, 3000: 0.11383964505170782\n",
      "Epoch 1, 3200: 0.11374694888363593\n",
      "Epoch 1, 3400: 0.11471222212318988\n",
      "Epoch 1, 3600: 0.11498372218054202\n",
      "Epoch 1, 3800: 0.11503646319143866\n",
      "Epoch 1, 4000: 0.11460629303008318\n",
      "Epoch 1, 4200: 0.11389125095502961\n",
      "Epoch 1, 4400: 0.11176053846762939\n",
      "Epoch 1, 4600: 0.11184203404285338\n",
      "Epoch 1, 4800: 0.11321890766732395\n",
      "Epoch 1, 5000: 0.11403688724637032\n",
      "Epoch 1, 5200: 0.11431498254649342\n",
      "Epoch 1, 5400: 0.11328563782283002\n",
      "Epoch 1, 5600: 0.11176519883330911\n",
      "Epoch 1, 5800: 0.11026407543762491\n",
      "Epoch 1, 6000: 0.1086000011038656\n",
      "Epoch 1, 6200: 0.10801082073891115\n",
      "Epoch 1, 6400: 0.10853098382387544\n",
      "Epoch 1, 6600: 0.10903613514041133\n",
      "Epoch 1, 6800: 0.10922357220668345\n",
      "Epoch 1, 7000: 0.1087423886084663\n",
      "Epoch 1, 7200: 0.10798943938609834\n",
      "Epoch 1, 7400: 0.10790463799333855\n",
      "Epoch 1: 0.10752463618758669\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "EPOCHS = 1\n",
    "batch_losses = []\n",
    "val_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0 ##sets the starting loss at zero\n",
    "    count = 0 #is used to keep track of the number of batches passed through the training model\n",
    "  \n",
    "    for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(train_dataloader): \n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(\n",
    "            pv_features.to(device, dtype=torch.float),\n",
    "            hrv_features.to(device, dtype=torch.float),\n",
    "        )\n",
    "        loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item() * pv_targets.size(0)\n",
    "        count += pv_targets.size(0)\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            print(f\"Epoch {epoch + 1}, {i + 1}: {running_loss / count}\")\n",
    "            \n",
    "    epoch_train_loss = running_loss / count\n",
    "    epoch_train_losses.append(epoch_train_loss)        \n",
    "    print(f\"Epoch {epoch + 1}: {running_loss / count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ResNet_light_deep_crop1_60m_batch_size_32\"\n",
    "os.makedirs(f\"models/{model_name}\", exist_ok=True)\n",
    "\n",
    "# Save the variables used to make the dataset to a text file\n",
    "with open(f\"models/{model_name}/data_summary.txt\", \"w\") as f:\n",
    "    f.write(\"BATCH_SIZE = \"+ str(BATCH_SIZE)+ \"\\n\"+ \n",
    "            \"train_start_date = \"+ train_start_date+ \"\\n\"+\n",
    "            \"train_end_date = \"+ train_end_date+ \"\\n\"+\n",
    "            \"validation_start_date = \"+ validation_start_date+ \"\\n\"+\n",
    "            \"validation_end_date = \"+ validation_end_date)\n",
    "    \n",
    "# Save the trained model for future predictions\n",
    "torch.save(model.state_dict(), f\"models/{model_name}/trained_model.pt\")\n",
    "\n",
    "# Create a DataFrame from the training_losses and validation_losses lists\n",
    "df = pd.DataFrame({'Training Losses': batch_losses})\n",
    "# df = pd.DataFrame({'Training Losses': batch_losses, 'Validation Losses': validation_losses})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(f'models\\{model_name}\\losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date      time         0         1         2         3         4  \\\n",
      "0      2021-06-01  09:00:00  0.487015  0.447293  0.406362  0.370650  0.341555   \n",
      "1      2021-06-01  09:00:00  0.538949  0.496987  0.453648  0.419015  0.385457   \n",
      "2      2021-06-01  09:00:00  0.277651  0.258244  0.238467  0.218277  0.203425   \n",
      "3      2021-06-01  09:00:00  0.513633  0.471048  0.428421  0.394613  0.363289   \n",
      "4      2021-06-01  09:00:00  0.149854  0.133903  0.125260  0.109811  0.104396   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "57593  2021-06-08  17:00:00  0.056932  0.052510  0.056167  0.053470  0.051600   \n",
      "57594  2021-06-08  17:00:00  0.009336  0.003042  0.010618  0.008158  0.011230   \n",
      "57595  2021-06-08  17:00:00  0.141573  0.137108  0.133267  0.126884  0.115171   \n",
      "57596  2021-06-08  17:00:00  0.092330  0.088262  0.088837  0.083721  0.079957   \n",
      "57597  2021-06-08  17:00:00  0.154036  0.147944  0.142910  0.134896  0.125626   \n",
      "\n",
      "              5         6         7  ...  target_3  target_4  target_5  \\\n",
      "0      0.334907  0.305202  0.296378  ...  0.572424  0.580869  0.594098   \n",
      "1      0.374283  0.345929  0.335459  ...  0.607247  0.604130  0.618467   \n",
      "2      0.194063  0.179073  0.175270  ...  0.344097  0.366654  0.374010   \n",
      "3      0.350800  0.323488  0.313052  ...  0.601608  0.609147  0.620271   \n",
      "4      0.095866  0.086942  0.087188  ...  0.188907  0.202428  0.196845   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "57593  0.038433  0.036439  0.038140  ...  0.212239  0.213089  0.212444   \n",
      "57594 -0.000729  0.001031  0.004911  ...  0.000000  0.000000  0.000000   \n",
      "57595  0.107862  0.099321  0.096305  ...  0.088526  0.078881  0.073598   \n",
      "57596  0.066741  0.063214  0.063545  ...  0.055756  0.048965  0.041543   \n",
      "57597  0.114317  0.105453  0.104018  ...  0.094774  0.083796  0.075988   \n",
      "\n",
      "       target_6  target_7  target_8  target_9  target_10  target_11  RMSE_1 hr  \n",
      "0      0.601445  0.611241  0.621955  0.629678   0.637404   0.648306   0.416958  \n",
      "1      0.616800  0.604830  0.618733  0.636797   0.664893   0.671357   0.409600  \n",
      "2      0.388419  0.323355  0.343359  0.402201   0.434010   0.444117   0.310441  \n",
      "3      0.624504  0.635217  0.644913  0.654606   0.666162   0.676488   0.430000  \n",
      "4      0.198014  0.211061  0.189622  0.205161   0.224654   0.241820   0.176828  \n",
      "...         ...       ...       ...       ...        ...        ...        ...  \n",
      "57593  0.170169  0.039364  0.043220  0.043467   0.043234   0.048604   0.025327  \n",
      "57594  0.000000  0.000000  0.000000  0.000000   0.000000   0.000000   0.002331  \n",
      "57595  0.070356  0.062801  0.053441  0.047561   0.044684   0.043758   0.026749  \n",
      "57596  0.036807  0.032215  0.026591  0.021777   0.016000   0.013468   0.030767  \n",
      "57597  0.068096  0.059215  0.051255  0.046002   0.040695   0.035427   0.040385  \n",
      "\n",
      "[57598 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "def _eval_visual(dataloader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    predictions_list = []\n",
    "    timestamps_list = []\n",
    "    pv_targets_list = []  # List to store pv_targets for each batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(dataloader):\n",
    "            hrv_features = hrv_features.to(device, dtype=torch.float)\n",
    "            pv_features = pv_features.to(device, dtype=torch.float)\n",
    "            pv_targets = pv_targets.to(device, dtype=torch.float)\n",
    "            \n",
    "            batch_predictions = model(pv_features, hrv_features)\n",
    "            batch_predictions = batch_predictions.cpu().numpy()\n",
    "            batch_pv_targets = pv_targets.cpu().numpy()  # Convert pv_targets to numpy array\n",
    "\n",
    "            # Timestamp processing as before\n",
    "            if isinstance(time_ids[0], tuple) or isinstance(time_ids[0], list):\n",
    "                single_timestamp = time_ids[0][0]\n",
    "            else:\n",
    "                single_timestamp = time_ids[0]\n",
    "            if isinstance(single_timestamp, datetime):\n",
    "                timestamp = single_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            else:\n",
    "                timestamp = str(single_timestamp)\n",
    "            \n",
    "            # Append each batch's data to the lists\n",
    "            predictions_list.append(batch_predictions)\n",
    "            pv_targets_list.append(batch_pv_targets)  # Append pv_targets to its list\n",
    "            batch_timestamps = [timestamp] * batch_predictions.shape[0]\n",
    "            timestamps_list.extend(batch_timestamps)\n",
    "\n",
    "    # Concatenate all collected arrays into single numpy arrays\n",
    "    predictions = np.concatenate(predictions_list, axis=0)\n",
    "    pv_targets = np.concatenate(pv_targets_list, axis=0)  # Concatenate all pv_targets\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    pv_targets_df = pd.DataFrame(pv_targets, columns=[f'target_{i}' for i in range(pv_targets.shape[1])])\n",
    "    timestamps_df = pd.DataFrame(timestamps_list, columns=['timestamp'])\n",
    "\n",
    "    # Combine timestamps, predictions, and targets by using index alignment\n",
    "    final_df = pd.concat([timestamps_df, predictions_df, pv_targets_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Usage\n",
    "prediction_df = _eval_visual(validation_dataloader, model, device)\n",
    "prediction_df[['date', 'time']] = prediction_df['timestamp'].str.split('T', expand=True)\n",
    "timestamp_index = prediction_df.columns.get_loc('timestamp')\n",
    "prediction_df.insert(timestamp_index, 'date', prediction_df.pop('date'))\n",
    "prediction_df.insert(timestamp_index + 1, 'time', prediction_df.pop('time'))\n",
    "prediction_df.drop('timestamp', axis=1, inplace=True)\n",
    "predictions_RMSE = prediction_df[11]\n",
    "targets_RMSE = prediction_df['target_11']\n",
    "mse = (predictions_RMSE - targets_RMSE) ** 2\n",
    "rmse = np.sqrt(mse)\n",
    "prediction_df['RMSE_1 hr'] = rmse\n",
    "\n",
    "prediction_df.to_csv('predictions.csv', index=False)\n",
    "print(prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
