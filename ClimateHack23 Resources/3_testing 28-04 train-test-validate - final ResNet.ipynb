{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\james\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import json\n",
    "from dataset import Dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocf_blosc2 import Blosc2\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime, time, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block downloads the data from huggingface. This is only required if working on Google Colab OR data has not yet been downloaded locally. Expect this cell to run for up to 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load pv, hrv and indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\lib\\site-packages\\xarray\\core\\concat.py:504: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n"
     ]
    }
   ],
   "source": [
    "pv_inc = pd.read_csv(\"pv_inc_full_first_method.csv\").set_index([\"timestamp\", \"ss_id\"])\n",
    "pv_df = pv_inc.reset_index()\n",
    "pv_df[\"timestamp\"] = pd.to_datetime(pv_df[\"timestamp\"], utc=True)\n",
    "pv_df = pv_df.set_index([\"timestamp\",\"ss_id\"])\n",
    "pv_inc = pv_df.drop(columns={\"power\", \"angle_of_incidence_radians\",})\n",
    "pv = pv_df.drop(columns={\"power_normalized\", \"angle_of_incidence_radians_normalized\", \"angle_of_incidence_radians\", \"angle_of_incidence_radians_normalized\"})\n",
    "pv.index = pv.index.set_levels([pv.index.levels[0].tz_localize(None), pv.index.levels[1]])\n",
    "pv_inc.index = pv_inc.index.set_levels([pv_inc.index.levels[0].tz_localize(None), pv_inc.index.levels[1]])\n",
    "\n",
    "# The parquet data here is similar to a dataframe. The \"power\" is the column with the other data types being indexes. The data is shaped with each timestamp being its own \n",
    "# subframe with the sites having their corresponding power (% of site capacity).  \n",
    "hrv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        hrv.append(xr.open_dataset(f\"data/satellite-hrv/{year}/{month}.zarr.zip\", engine=\"zarr\", chunks=\"auto\"))\n",
    "\n",
    "hrv = xr.concat(hrv, dim=\"time\")\n",
    "\n",
    "# Images are stored as vectors. The vectors are stored as an array of vectors. The arrays have a timestamp. Since there is only one channel (hrv)\n",
    "# the array is a 1D set of vectors with the dimension being time. Read this to help you understand how this is being stored \n",
    "# https://tutorial.xarray.dev/fundamentals/01_datastructures.html\n",
    "with open(\"indices.json\") as f:\n",
    "    site_locations = {\n",
    "        data_source: {\n",
    "            int(site): (int(location[0]), int(location[1]))\n",
    "            for site, location in locations.items() #if site == '2607'#added this to run only 1 site location to understand how it works\n",
    "        }\n",
    "        for data_source, locations in json.load(f).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeDataset(IterableDataset):\n",
    "    def __init__(self, pv, hrv, site_locations, start_date = \"2020-7-1\", end_date = \"2020-7-30\", sites=None):\n",
    "        self.pv = pv\n",
    "        self.hrv = hrv\n",
    "        self._site_locations = site_locations\n",
    "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())#This gets the individual site ids which are stored as the dict's keys\n",
    "        self.start_date = list(map(int, start_date.split(\"-\")))\n",
    "        self.end_date= list(map(int, end_date.split(\"-\")))\n",
    "\n",
    "    def _get_image_times(self):#This function starts at the minimum date in the set and iterates up to the highest date, this is done as the data set is large and due to the nature of the parquette and xarray\n",
    "        min_date = datetime(self.start_date[0], self.start_date[1], self.start_date[2])\n",
    "        max_date = datetime(self.end_date[0], self.end_date[1], self.end_date[2])\n",
    "        \n",
    "        start_time = time(8)\n",
    "        end_time = time(17)\n",
    "\n",
    "        date = min_date \n",
    "        while date <= max_date: \n",
    "            current_time = datetime.combine(date, start_time)\n",
    "            while current_time.time() < end_time:\n",
    "                if current_time:\n",
    "                    yield current_time\n",
    "\n",
    "                current_time += timedelta(minutes=60)\n",
    "\n",
    "            date += timedelta(days=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for time in self._get_image_times():\n",
    "            \n",
    "             # generate time ids for predictions to be analysedm after training\n",
    "            time_ids = pd.date_range(start=time + timedelta(hours=1),\n",
    "                                      end=time + timedelta(hours=1)+timedelta(minutes=55),\n",
    "                                      freq='5min')\n",
    "            time_ids = time_ids.strftime('%Y-%m-%dT%H:%M:%S').tolist()  \n",
    "\n",
    "            # 1 hour leading up to the predicton time        \n",
    "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
    "\n",
    "            # PV power output in first hour\n",
    "            pv_features = pv.xs(first_hour, drop_level=False)\n",
    "\n",
    "            pv_targets = pv.xs(\n",
    "                slice(  # type: ignore\n",
    "                    str(time + timedelta(hours=1)),\n",
    "                    str(time + timedelta(hours=1, minutes=55)),\n",
    "                ),\n",
    "                drop_level=False,\n",
    "            )\n",
    "\n",
    "           # hrv satellite images on first hour timestamps setting them up as an input feature\n",
    "            hrv_data = self.hrv[\"data\"].sel(time=first_hour).to_numpy()\n",
    "\n",
    "            for site in self._sites:\n",
    "                site_id = site\n",
    "\n",
    "                try:\n",
    "                    site_features = pv_features.xs(site, level=1).to_numpy().squeeze(-1) # gets the pixel based location of the pv site and then uses this to make predictions based on the individual sites\n",
    "                    site_targets = pv_targets.xs(site, level=1).to_numpy().squeeze(-1)\n",
    "                    assert site_features.shape == (12,) and site_targets.shape == (12,)\n",
    "                 \n",
    "                    x, y = self._site_locations[\"hrv\"][site]\n",
    "                    hrv_features = hrv_data[:, y - 4  : y + 4 ,\n",
    "                                             x - 4  : x + 4 , 0]\n",
    "                    assert hrv_features.shape == (12, 8, 8)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                yield time_ids, site_id, site_features, hrv_features, site_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeDataset_inc(IterableDataset):\n",
    "    def __init__(self, pv_inc, hrv, site_locations, start_date=\"2020-7-1\", end_date=\"2020-7-30\", sites=None):\n",
    "        self.pv_inc = pv_inc\n",
    "        self.hrv = hrv\n",
    "        self._site_locations = site_locations\n",
    "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())\n",
    "        self.start_date = list(map(int, start_date.split(\"-\")))\n",
    "        self.end_date = list(map(int, end_date.split(\"-\")))\n",
    "\n",
    "    def _get_image_times(self):\n",
    "        min_date = datetime(self.start_date[0], self.start_date[1], self.start_date[2])\n",
    "        max_date = datetime(self.end_date[0], self.end_date[1], self.end_date[2])\n",
    "        start_time = time(8)\n",
    "        end_time = time(17)\n",
    "        date = min_date\n",
    "        while date <= max_date:\n",
    "            current_time = datetime.combine(date, start_time)\n",
    "            while current_time.time() < end_time:\n",
    "                yield current_time\n",
    "                current_time += timedelta(minutes=60)\n",
    "            date += timedelta(days=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for time in self._get_image_times():\n",
    "            time_ids = pd.date_range(start=time + timedelta(hours=1),\n",
    "                                     end=time + timedelta(hours=5) - timedelta(minutes=5),\n",
    "                                     freq='5min').strftime('%Y-%m-%dT%H:%M:%S').tolist()\n",
    "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
    "            pv_features = self.pv_inc.xs(first_hour, drop_level=False)[[\"power_normalized\", \"angle_of_incidence_radians_normalized\"]]\n",
    "            pv_targets = self.pv_inc.xs(slice(str(time + timedelta(hours=1)), \n",
    "                                     str(time + timedelta(hours=1, minutes=55))),\n",
    "                                     drop_level=False)[\"power_normalized\"]\n",
    "            hrv_data = self.hrv['data'].sel(time=first_hour).to_numpy()\n",
    "\n",
    "            for site in self._sites:\n",
    "                site_id = site\n",
    "                try:\n",
    "                    site_features = pv_features.xs(site, level=1).to_numpy()\n",
    "                    site_targets = pv_targets.xs(site, level=1).to_numpy()\n",
    "                    assert site_features.shape == (12,2) and site_targets.shape == (12,)\n",
    "                    x, y = self._site_locations[\"hrv\"][site]\n",
    "                    hrv_features = hrv_data[:, y - 1 : y + 1, x - 1 : x + 1, 0]\n",
    "                    assert hrv_features.shape == (12, 2, 2)\n",
    "                    yield time_ids, site_id, site_features, hrv_features, site_targets\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train, validation and test datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4]\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_inc(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet_light_inc, self).__init__()\n",
    "        self.in_channels = 12\n",
    "        self.initial = conv_block(12, 12, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Linear(96  + 24, 12)#change back to + 2\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv_inc, hrv):\n",
    "        x = self.initial(hrv)\n",
    "        \n",
    "        #x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "       # print(f\"X Shape {x.shape}\")\n",
    "       \n",
    "        power = pv_inc[:, :, 0]\n",
    "        #print(f\"Power: {pv_inc[:, :, 0]}\")\n",
    "        \n",
    "        angle = pv_inc[:, :, 1]\n",
    "        #print(f\"{angle.shape}\")\n",
    "\n",
    "        #pv_inc = torch.flatten(pv_inc, start_dim=1)\n",
    "        #print(\"Shape of x:\", x.shape)\n",
    "        #print(\"Shape of pv_inc:\", pv_inc.shape)\n",
    "        #print(\"Shape of power:\", power.shape)\n",
    "        #print(\"Shape of angle:\", angle.shape)\n",
    "        \n",
    "        combined = torch.cat((x, power, angle), dim=1)\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        #print(f\"Out shape {out.shape}\")\n",
    "        return out\n",
    "\n",
    "model_light_res_inc_feature = ResNet_light_inc(BasicBlock, layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial HRV Shape: torch.Size([2, 12, 2, 2])\n",
      "Flattened HRV Features Shape: torch.Size([2, 96])\n",
      "Combined shape torch.Size([2, 120])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 12, 2, 2]             156\n",
      "       BatchNorm2d-2             [-1, 12, 2, 2]              24\n",
      "              ReLU-3             [-1, 12, 2, 2]               0\n",
      "         MaxPool2d-4             [-1, 12, 2, 2]               0\n",
      "            Conv2d-5             [-1, 12, 2, 2]             156\n",
      "       BatchNorm2d-6             [-1, 12, 2, 2]              24\n",
      "              ReLU-7             [-1, 12, 2, 2]               0\n",
      "            Conv2d-8             [-1, 12, 2, 2]             156\n",
      "       BatchNorm2d-9             [-1, 12, 2, 2]              24\n",
      "             ReLU-10             [-1, 12, 2, 2]               0\n",
      "       BasicBlock-11             [-1, 12, 2, 2]               0\n",
      "           Conv2d-12             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-13             [-1, 12, 2, 2]              24\n",
      "             ReLU-14             [-1, 12, 2, 2]               0\n",
      "           Conv2d-15             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-16             [-1, 12, 2, 2]              24\n",
      "             ReLU-17             [-1, 12, 2, 2]               0\n",
      "       BasicBlock-18             [-1, 12, 2, 2]               0\n",
      "           Conv2d-19             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-20             [-1, 12, 2, 2]              24\n",
      "             ReLU-21             [-1, 12, 2, 2]               0\n",
      "           Conv2d-22             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-23             [-1, 12, 2, 2]              24\n",
      "             ReLU-24             [-1, 12, 2, 2]               0\n",
      "       BasicBlock-25             [-1, 12, 2, 2]               0\n",
      "           Conv2d-26             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-27             [-1, 12, 2, 2]              24\n",
      "             ReLU-28             [-1, 12, 2, 2]               0\n",
      "           Conv2d-29             [-1, 12, 2, 2]             156\n",
      "      BatchNorm2d-30             [-1, 12, 2, 2]              24\n",
      "             ReLU-31             [-1, 12, 2, 2]               0\n",
      "       BasicBlock-32             [-1, 12, 2, 2]               0\n",
      "           Conv2d-33             [-1, 24, 2, 2]             312\n",
      "      BatchNorm2d-34             [-1, 24, 2, 2]              48\n",
      "             ReLU-35             [-1, 24, 2, 2]               0\n",
      "           Conv2d-36             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-37             [-1, 24, 2, 2]              48\n",
      "             ReLU-38             [-1, 24, 2, 2]               0\n",
      "           Conv2d-39             [-1, 24, 2, 2]             288\n",
      "      BatchNorm2d-40             [-1, 24, 2, 2]              48\n",
      "       BasicBlock-41             [-1, 24, 2, 2]               0\n",
      "           Conv2d-42             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-43             [-1, 24, 2, 2]              48\n",
      "             ReLU-44             [-1, 24, 2, 2]               0\n",
      "           Conv2d-45             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-46             [-1, 24, 2, 2]              48\n",
      "             ReLU-47             [-1, 24, 2, 2]               0\n",
      "       BasicBlock-48             [-1, 24, 2, 2]               0\n",
      "           Conv2d-49             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-50             [-1, 24, 2, 2]              48\n",
      "             ReLU-51             [-1, 24, 2, 2]               0\n",
      "           Conv2d-52             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-53             [-1, 24, 2, 2]              48\n",
      "             ReLU-54             [-1, 24, 2, 2]               0\n",
      "       BasicBlock-55             [-1, 24, 2, 2]               0\n",
      "           Conv2d-56             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-57             [-1, 24, 2, 2]              48\n",
      "             ReLU-58             [-1, 24, 2, 2]               0\n",
      "           Conv2d-59             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-60             [-1, 24, 2, 2]              48\n",
      "             ReLU-61             [-1, 24, 2, 2]               0\n",
      "       BasicBlock-62             [-1, 24, 2, 2]               0\n",
      "           Conv2d-63             [-1, 48, 1, 1]           1,200\n",
      "      BatchNorm2d-64             [-1, 48, 1, 1]              96\n",
      "             ReLU-65             [-1, 48, 1, 1]               0\n",
      "           Conv2d-66             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-67             [-1, 48, 1, 1]              96\n",
      "             ReLU-68             [-1, 48, 1, 1]               0\n",
      "           Conv2d-69             [-1, 48, 1, 1]           1,152\n",
      "      BatchNorm2d-70             [-1, 48, 1, 1]              96\n",
      "       BasicBlock-71             [-1, 48, 1, 1]               0\n",
      "           Conv2d-72             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-73             [-1, 48, 1, 1]              96\n",
      "             ReLU-74             [-1, 48, 1, 1]               0\n",
      "           Conv2d-75             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-76             [-1, 48, 1, 1]              96\n",
      "             ReLU-77             [-1, 48, 1, 1]               0\n",
      "       BasicBlock-78             [-1, 48, 1, 1]               0\n",
      "           Conv2d-79             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-80             [-1, 48, 1, 1]              96\n",
      "             ReLU-81             [-1, 48, 1, 1]               0\n",
      "           Conv2d-82             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-83             [-1, 48, 1, 1]              96\n",
      "             ReLU-84             [-1, 48, 1, 1]               0\n",
      "       BasicBlock-85             [-1, 48, 1, 1]               0\n",
      "           Conv2d-86             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-87             [-1, 48, 1, 1]              96\n",
      "             ReLU-88             [-1, 48, 1, 1]               0\n",
      "           Conv2d-89             [-1, 48, 1, 1]           2,352\n",
      "      BatchNorm2d-90             [-1, 48, 1, 1]              96\n",
      "             ReLU-91             [-1, 48, 1, 1]               0\n",
      "       BasicBlock-92             [-1, 48, 1, 1]               0\n",
      "           Conv2d-93             [-1, 96, 1, 1]           4,704\n",
      "      BatchNorm2d-94             [-1, 96, 1, 1]             192\n",
      "             ReLU-95             [-1, 96, 1, 1]               0\n",
      "           Conv2d-96             [-1, 96, 1, 1]           9,312\n",
      "      BatchNorm2d-97             [-1, 96, 1, 1]             192\n",
      "             ReLU-98             [-1, 96, 1, 1]               0\n",
      "           Conv2d-99             [-1, 96, 1, 1]           4,608\n",
      "     BatchNorm2d-100             [-1, 96, 1, 1]             192\n",
      "      BasicBlock-101             [-1, 96, 1, 1]               0\n",
      "          Conv2d-102             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-103             [-1, 96, 1, 1]             192\n",
      "            ReLU-104             [-1, 96, 1, 1]               0\n",
      "          Conv2d-105             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-106             [-1, 96, 1, 1]             192\n",
      "            ReLU-107             [-1, 96, 1, 1]               0\n",
      "      BasicBlock-108             [-1, 96, 1, 1]               0\n",
      "          Conv2d-109             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-110             [-1, 96, 1, 1]             192\n",
      "            ReLU-111             [-1, 96, 1, 1]               0\n",
      "          Conv2d-112             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-113             [-1, 96, 1, 1]             192\n",
      "            ReLU-114             [-1, 96, 1, 1]               0\n",
      "      BasicBlock-115             [-1, 96, 1, 1]               0\n",
      "          Conv2d-116             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-117             [-1, 96, 1, 1]             192\n",
      "            ReLU-118             [-1, 96, 1, 1]               0\n",
      "          Conv2d-119             [-1, 96, 1, 1]           9,312\n",
      "     BatchNorm2d-120             [-1, 96, 1, 1]             192\n",
      "            ReLU-121             [-1, 96, 1, 1]               0\n",
      "      BasicBlock-122             [-1, 96, 1, 1]               0\n",
      "AdaptiveMaxPool2d-123             [-1, 96, 1, 1]               0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15016\\2723693016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_light_res_inc_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\james\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mtotal_input_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# x2 for gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\james\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3098\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3099\u001b[0m     \"\"\"\n\u001b[1;32m-> 3100\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3101\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\james\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "summary(model_light_res_inc_feature, input_size=[(2,12), (12, 2, 2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4,] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop1_60m(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop1_60m, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "model_pv = ResNet_light_deep_crop1_60m(BasicBlock, layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the flatterned layer</br>\n",
    "Send shape of resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34402607, 2)\n"
     ]
    }
   ],
   "source": [
    "print(pv_inc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))\n",
    "\n",
    "\n",
    "def model_validation(model, criterion, validation_dataloader):\n",
    "    model.eval() # This is used to set the model to evaluation mode\n",
    "    with torch.no_grad(): # This is used to stop the model from storing gradients\n",
    "        losses = []\n",
    "        for pv_features, hrv_features, pv_targets in validation_dataloader:\n",
    "            pv_features, hrv_features, pv_targets = pv_features.to(device, dtype=torch.float), hrv_features.to(device, dtype=torch.float), pv_targets.to(device, dtype=torch.float)\n",
    "            predictions = model(pv_features, hrv_features)\n",
    "            loss = criterion(predictions, pv_targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    model.train() # This is used to set the model back to training mode\n",
    "    \n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def model_validation_indv(model, criterion, validation_dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    individual_losses = []  # List to store each individual loss\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for pv_features, hrv_features, pv_targets in validation_dataloader:\n",
    "            pv_features, hrv_features, pv_targets = pv_features.to(device, dtype=torch.float), hrv_features.to(device, dtype=torch.float), pv_targets.to(device, dtype=torch.float)\n",
    "\n",
    "            predictions = model(pv_features, hrv_features)  # Get model predictions\n",
    "            \n",
    "            # Calculate loss for each individual in the batch\n",
    "            individual_batch_losses = criterion(predictions, pv_targets, reduction='none')  # This should return a tensor of losses for each item in the batch\n",
    "            \n",
    "            individual_losses.extend(individual_batch_losses.tolist())  # Convert tensor to list and append to the list of losses\n",
    "            \n",
    "    model.train()  # Set the model back to training mode\n",
    "    return individual_losses\n",
    "criterion = RMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_light_res_inc_feature\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model_pv \n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_dataset = ChallengeDataset_inc(pv_inc, hrv, site_locations=site_locations,\n",
    "                                 start_date=\"2020-6-1\", end_date=\"2020-8-31\")  # controls which data is loaded in\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)#try with shuffle\n",
    "\n",
    "validation_dataset = ChallengeDataset_inc(pv_inc, hrv, site_locations=site_locations,\n",
    "                                      start_date=\"2021-7-01\", end_date=\"2021-7-31\") \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "# Example using StepLR\n",
    "##scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Example using ReduceLROnPlateau\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently running 2x2 buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 200: 0.2131335947662592\n",
      "Epoch 1, 400: 0.17861237705685198\n",
      "Epoch 1, 600: 0.1792036269667248\n",
      "Epoch 1, 800: 0.17375027772504836\n",
      "Epoch 1, 1000: 0.16229482089355587\n",
      "Epoch 1, 1200: 0.1577114818741878\n",
      "Epoch 1, 1400: 0.15638374395402416\n",
      "Epoch 1, 1600: 0.15496777579188348\n",
      "Epoch 1, 1800: 0.15215345681127573\n",
      "Epoch 1, 2000: 0.1548297593705356\n",
      "Epoch 1, 2200: 0.15159770031544295\n",
      "Epoch 1, 2400: 0.14843309002617994\n",
      "Epoch 1, 2600: 0.14960618941829754\n",
      "Epoch 1, 2800: 0.14774595565960877\n",
      "Epoch 1, 3000: 0.14756656464437642\n",
      "Epoch 1, 3200: 0.1450859531026799\n",
      "Epoch 1, 3400: 0.1468070563925978\n",
      "Epoch 1, 3600: 0.14499559611702958\n",
      "Epoch 1, 3800: 0.14643378915265204\n",
      "Epoch 1, 4000: 0.1461453473707661\n",
      "Epoch 1, 4200: 0.1444194189405867\n",
      "Epoch 1, 4400: 0.14440454070456327\n",
      "Epoch 1, 4600: 0.14544404218783197\n",
      "Epoch 1, 4800: 0.14605372358191138\n",
      "Epoch 1, 5000: 0.14541700266823174\n",
      "Epoch 1, 5200: 0.1455001534256511\n",
      "Epoch 1, 5400: 0.14550061373591974\n",
      "Epoch 1, 5600: 0.14486867137120238\n",
      "Epoch 1, 5800: 0.1449904645349959\n",
      "Epoch 1, 6000: 0.14549459515387814\n",
      "Epoch 1, 6200: 0.14484732005144318\n",
      "Epoch 1, 6400: 0.14451611532247624\n",
      "Epoch 1, 6600: 0.14343356089271378\n",
      "Epoch 1, 6800: 0.14247974231400912\n",
      "Epoch 1, 7000: 0.14109928607940675\n",
      "Epoch 1, 7200: 0.14001900679431856\n",
      "Epoch 1, 7400: 0.13992051252390483\n",
      "Epoch 1, 7600: 0.13990354762290066\n",
      "Epoch 1, 7800: 0.1399140358498941\n",
      "Epoch 1, 8000: 0.13953939406224528\n",
      "Epoch 1, 8200: 0.138778018741982\n",
      "Epoch 1, 8400: 0.13888398552091705\n",
      "Epoch 2, 200: 0.10989569367840886\n",
      "Epoch 2, 400: 0.12052609103731811\n",
      "Epoch 2, 600: 0.13738445178295175\n",
      "Epoch 2, 800: 0.13994103939272462\n",
      "Epoch 2, 1000: 0.13372734655067325\n",
      "Epoch 2, 1200: 0.1327646828442812\n",
      "Epoch 2, 1400: 0.13412550713068672\n",
      "Epoch 2, 1600: 0.13494262319989503\n",
      "Epoch 2, 1800: 0.13359504788079196\n",
      "Epoch 2, 2000: 0.13755612458847463\n",
      "Epoch 2, 2200: 0.1352214327928695\n",
      "Epoch 2, 2400: 0.1326704700337723\n",
      "Epoch 2, 2600: 0.13466009732192524\n",
      "Epoch 2, 2800: 0.13348988513462245\n",
      "Epoch 2, 3000: 0.13389757205918432\n",
      "Epoch 2, 3200: 0.13191480207431597\n",
      "Epoch 2, 3400: 0.1341489853989333\n",
      "Epoch 2, 3600: 0.1328210169221792\n",
      "Epoch 2, 3800: 0.13474426379721416\n",
      "Epoch 2, 4000: 0.13472884841635824\n",
      "Epoch 2, 4200: 0.1333515089776899\n",
      "Epoch 2, 4400: 0.13356360410848125\n",
      "Epoch 2, 4600: 0.1348727414943278\n",
      "Epoch 2, 4800: 0.13575901672787344\n",
      "Epoch 2, 5000: 0.13535894498974085\n",
      "Epoch 2, 5200: 0.1356880356751096\n",
      "Epoch 2, 5400: 0.13593009794751804\n",
      "Epoch 2, 5600: 0.13552071920729108\n",
      "Epoch 2, 5800: 0.1358749956483471\n",
      "Epoch 2, 6000: 0.13644239620181423\n",
      "Epoch 2, 6200: 0.13588862950282712\n",
      "Epoch 2, 6400: 0.13581552758521867\n",
      "Epoch 2, 6600: 0.1349301027715432\n",
      "Epoch 2, 6800: 0.13418720891644412\n",
      "Epoch 2, 7000: 0.13295470330305398\n",
      "Epoch 2, 7200: 0.13195390875218435\n",
      "Epoch 2, 7400: 0.13201899518913313\n",
      "Epoch 2, 7600: 0.1321649600977176\n",
      "Epoch 2, 7800: 0.13233836421647516\n",
      "Epoch 2, 8000: 0.13210952743981033\n",
      "Epoch 2, 8200: 0.13146408475072283\n",
      "Epoch 2, 8400: 0.13176294923582602\n",
      "Epoch 3, 200: 0.11180283356457948\n",
      "Epoch 3, 400: 0.1209510094858706\n",
      "Epoch 3, 600: 0.13776139373580615\n",
      "Epoch 3, 800: 0.1400631700316444\n",
      "Epoch 3, 1000: 0.13363638014160098\n",
      "Epoch 3, 1200: 0.13238227481332918\n",
      "Epoch 3, 1400: 0.1336018559789019\n",
      "Epoch 3, 1600: 0.134293580588419\n",
      "Epoch 3, 1800: 0.1329153272592359\n",
      "Epoch 3, 2000: 0.137014506008476\n",
      "Epoch 3, 2200: 0.13464174853637814\n",
      "Epoch 3, 2400: 0.1320929800129185\n",
      "Epoch 3, 2600: 0.13414363655333336\n",
      "Epoch 3, 2800: 0.1329949791702841\n",
      "Epoch 3, 3000: 0.13343106081957617\n",
      "Epoch 3, 3200: 0.13148139538418036\n",
      "Epoch 3, 3400: 0.133723695821074\n",
      "Epoch 3, 3600: 0.13241451140958815\n",
      "Epoch 3, 3800: 0.13430896140585996\n",
      "Epoch 3, 4000: 0.13434032211033628\n",
      "Epoch 3, 4200: 0.13295767190954869\n",
      "Epoch 3, 4400: 0.1331536657773805\n",
      "Epoch 3, 4600: 0.13443906094271527\n",
      "Epoch 3, 4800: 0.13530569226675046\n",
      "Epoch 3, 5000: 0.13490920007415116\n",
      "Epoch 3, 5200: 0.13523121517330694\n",
      "Epoch 3, 5400: 0.13547857836958158\n",
      "Epoch 3, 5600: 0.1350341453040684\n",
      "Epoch 3, 5800: 0.13536911370136356\n",
      "Epoch 3, 6000: 0.1359385605637605\n",
      "Epoch 3, 6200: 0.13540921601226494\n",
      "Epoch 3, 6400: 0.13531737744488054\n",
      "Epoch 3, 6600: 0.1344200789287799\n",
      "Epoch 3, 6800: 0.13362949154750609\n",
      "Epoch 3, 7000: 0.13241797515643494\n",
      "Epoch 3, 7200: 0.13142532016616315\n",
      "Epoch 3, 7400: 0.13149637652812776\n",
      "Epoch 3, 7600: 0.13162072720545295\n",
      "Epoch 3, 7800: 0.13178984868316315\n",
      "Epoch 3, 8000: 0.13156036902964116\n",
      "Epoch 3, 8200: 0.1309280375729701\n",
      "Epoch 3, 8400: 0.13122744818728063\n",
      "Epoch 4, 200: 0.10993498574942351\n",
      "Epoch 4, 400: 0.11964372582733632\n",
      "Epoch 4, 600: 0.13668828199307123\n",
      "Epoch 4, 800: 0.13919338305480777\n",
      "Epoch 4, 1000: 0.13296417780593037\n",
      "Epoch 4, 1200: 0.13175834858324378\n",
      "Epoch 4, 1400: 0.1330248209900622\n",
      "Epoch 4, 1600: 0.13374584242817947\n",
      "Epoch 4, 1800: 0.13242913496887518\n",
      "Epoch 4, 2000: 0.13653600867744536\n",
      "Epoch 4, 2200: 0.13419054114598442\n",
      "Epoch 4, 2400: 0.1316805116389878\n",
      "Epoch 4, 2600: 0.13373861923455618\n",
      "Epoch 4, 2800: 0.13260980832989194\n",
      "Epoch 4, 3000: 0.1330639946181327\n",
      "Epoch 4, 3200: 0.13112726315739565\n",
      "Epoch 4, 3400: 0.13337536162203725\n",
      "Epoch 4, 3600: 0.13208287987030215\n",
      "Epoch 4, 3800: 0.13399988600494045\n",
      "Epoch 4, 4000: 0.13404453129321336\n",
      "Epoch 4, 4200: 0.13266925508954694\n",
      "Epoch 4, 4400: 0.13288967627947304\n",
      "Epoch 4, 4600: 0.13418683653373434\n",
      "Epoch 4, 4800: 0.13506205390129858\n",
      "Epoch 4, 5000: 0.13467398084327578\n",
      "Epoch 4, 5200: 0.13500705734826624\n",
      "Epoch 4, 5400: 0.1352595580721067\n",
      "Epoch 4, 5600: 0.13481782213518662\n",
      "Epoch 4, 5800: 0.13515188506258458\n",
      "Epoch 4, 6000: 0.1357262636696299\n",
      "Epoch 4, 6200: 0.13519229600085847\n",
      "Epoch 4, 6400: 0.1351088981720386\n",
      "Epoch 4, 6600: 0.13421236241800766\n",
      "Epoch 4, 6800: 0.13342509749728967\n",
      "Epoch 4, 7000: 0.13222029424086212\n",
      "Epoch 4, 7200: 0.1312305686555596\n",
      "Epoch 4, 7400: 0.13131102915299503\n",
      "Epoch 4, 7600: 0.13144741417619546\n",
      "Epoch 4, 7800: 0.1316200806403485\n",
      "Epoch 4, 8000: 0.13139403677498923\n",
      "Epoch 4, 8200: 0.13076521979417743\n",
      "Epoch 4, 8400: 0.13106963523840975\n",
      "Epoch 5, 200: 0.11083150180056692\n",
      "Epoch 5, 400: 0.12024646512232721\n",
      "Epoch 5, 600: 0.13706352956593038\n",
      "Epoch 5, 800: 0.13947274344507604\n",
      "Epoch 5, 1000: 0.13319961359910668\n",
      "Epoch 5, 1200: 0.131922313196895\n",
      "Epoch 5, 1400: 0.13313865773512848\n",
      "Epoch 5, 1600: 0.13384700064198113\n",
      "Epoch 5, 1800: 0.13248629899798997\n",
      "Epoch 5, 2000: 0.13656243813689797\n",
      "Epoch 5, 2200: 0.13420307340693066\n",
      "Epoch 5, 2400: 0.1316719450048792\n",
      "Epoch 5, 2600: 0.13373140080545384\n",
      "Epoch 5, 2800: 0.13259749757192496\n",
      "Epoch 5, 3000: 0.13304384898704788\n",
      "Epoch 5, 3200: 0.13111923789023422\n",
      "Epoch 5, 3400: 0.13336856613593068\n",
      "Epoch 5, 3600: 0.13205619018317924\n",
      "Epoch 5, 3800: 0.1339745566111646\n",
      "Epoch 5, 4000: 0.13401512144505978\n",
      "Epoch 5, 4200: 0.13264023069735795\n",
      "Epoch 5, 4400: 0.13284197047691454\n",
      "Epoch 5, 4600: 0.13414070191266744\n",
      "Epoch 5, 4800: 0.1350022676214576\n",
      "Epoch 5, 5000: 0.13461568021327258\n",
      "Epoch 5, 5200: 0.13494872647122694\n",
      "Epoch 5, 5400: 0.1352051729536443\n",
      "Epoch 5, 5600: 0.13475227870978415\n",
      "Epoch 5, 5800: 0.13508668467659374\n",
      "Epoch 5, 6000: 0.13566147315440077\n",
      "Epoch 5, 6200: 0.13512864361667346\n",
      "Epoch 5, 6400: 0.13501983726164327\n",
      "Epoch 5, 6600: 0.13411277656584528\n",
      "Epoch 5, 6800: 0.1333245829218889\n",
      "Epoch 5, 7000: 0.13215019850991666\n",
      "Epoch 5, 7200: 0.13117577807538006\n",
      "Epoch 5, 7400: 0.131259046944261\n",
      "Epoch 5, 7600: 0.13140066387278862\n",
      "Epoch 5, 7800: 0.1315732937416014\n",
      "Epoch 5, 8000: 0.1313483085343614\n",
      "Epoch 5, 8200: 0.1307204713018202\n",
      "Epoch 5, 8400: 0.13102665937550012\n",
      "Epoch 6, 200: 0.11002339972183109\n",
      "Epoch 6, 400: 0.11993656907230615\n",
      "Epoch 6, 600: 0.13691164568687478\n",
      "Epoch 6, 800: 0.13934937090612948\n",
      "Epoch 6, 1000: 0.13309499122202395\n",
      "Epoch 6, 1200: 0.131843735370785\n",
      "Epoch 6, 1400: 0.13309917043362346\n",
      "Epoch 6, 1600: 0.13380613421089949\n",
      "Epoch 6, 1800: 0.1324544087259306\n",
      "Epoch 6, 2000: 0.13651787517219782\n",
      "Epoch 6, 2200: 0.1341603426465934\n",
      "Epoch 6, 2400: 0.13161520491509388\n",
      "Epoch 6, 2600: 0.1336776145103459\n",
      "Epoch 6, 2800: 0.13253860076224167\n",
      "Epoch 6, 3000: 0.13298057116443912\n",
      "Epoch 6, 3200: 0.13105772690672893\n",
      "Epoch 6, 3400: 0.1333062809031895\n",
      "Epoch 6, 3600: 0.13199189185113128\n",
      "Epoch 6, 3800: 0.13390907917318767\n",
      "Epoch 6, 4000: 0.13395168310170993\n",
      "Epoch 6, 4200: 0.1325753009031039\n",
      "Epoch 6, 4400: 0.13277488529301165\n",
      "Epoch 6, 4600: 0.13407351536068904\n",
      "Epoch 6, 4800: 0.13493729296877668\n",
      "Epoch 6, 5000: 0.13455444205217063\n",
      "Epoch 6, 5200: 0.13488936022031478\n",
      "Epoch 6, 5400: 0.13514776808485665\n",
      "Epoch 6, 5600: 0.1346906411591252\n",
      "Epoch 6, 5800: 0.13502471090615567\n",
      "Epoch 6, 6000: 0.13560205468628556\n",
      "Epoch 6, 6200: 0.135072507292573\n",
      "Epoch 6, 6400: 0.13498208055418218\n",
      "Epoch 6, 6600: 0.13408315963450482\n",
      "Epoch 6, 6800: 0.13329040189574964\n",
      "Epoch 6, 7000: 0.1321216566147549\n",
      "Epoch 6, 7200: 0.1311495065774458\n",
      "Epoch 6, 7400: 0.1312307488764762\n",
      "Epoch 6, 7600: 0.13137643265219306\n",
      "Epoch 6, 7800: 0.13155245785696956\n",
      "Epoch 6, 8000: 0.13132494907896033\n",
      "Epoch 6, 8200: 0.13069771608360475\n",
      "Epoch 6, 8400: 0.13100082332423577\n",
      "Epoch 7, 200: 0.11070861913263798\n",
      "Epoch 7, 400: 0.12017859140411019\n",
      "Epoch 7, 600: 0.13700896938641866\n",
      "Epoch 7, 800: 0.1394227552320808\n",
      "Epoch 7, 1000: 0.13314841168373823\n",
      "Epoch 7, 1200: 0.131859507279781\n",
      "Epoch 7, 1400: 0.13307534498162568\n",
      "Epoch 7, 1600: 0.13376608831924386\n",
      "Epoch 7, 1800: 0.1323939126243608\n",
      "Epoch 7, 2000: 0.1364393689939752\n",
      "Epoch 7, 2200: 0.1340888720606877\n",
      "Epoch 7, 2400: 0.13154352445388212\n",
      "Epoch 7, 2600: 0.13359762786887586\n",
      "Epoch 7, 2800: 0.13246223110905184\n",
      "Epoch 7, 3000: 0.1329077847531686\n",
      "Epoch 7, 3200: 0.13098861497128383\n",
      "Epoch 7, 3400: 0.13323437841499552\n",
      "Epoch 7, 3600: 0.13192472928565824\n",
      "Epoch 7, 3800: 0.13384500142628034\n",
      "Epoch 7, 4000: 0.13389243558468297\n",
      "Epoch 7, 4200: 0.13251939784513697\n",
      "Epoch 7, 4400: 0.13272076394451274\n",
      "Epoch 7, 4600: 0.13402420194779077\n",
      "Epoch 7, 4800: 0.13489008279090436\n",
      "Epoch 7, 5000: 0.1345101892065257\n",
      "Epoch 7, 5200: 0.1348497706827206\n",
      "Epoch 7, 5400: 0.13511087466272767\n",
      "Epoch 7, 5600: 0.13462477172359025\n",
      "Epoch 7, 5800: 0.13495811355364476\n",
      "Epoch 7, 6000: 0.13553555277455598\n",
      "Epoch 7, 6200: 0.13500817420291564\n",
      "Epoch 7, 6400: 0.1349302007377264\n",
      "Epoch 7, 6600: 0.13403425081237924\n",
      "Epoch 7, 6800: 0.13324600507039577\n",
      "Epoch 7, 7000: 0.13205727978210363\n",
      "Epoch 7, 7200: 0.13108160234884256\n",
      "Epoch 7, 7400: 0.13116134821261102\n",
      "Epoch 7, 7600: 0.13131536553809908\n",
      "Epoch 7, 7800: 0.1314944917240586\n",
      "Epoch 7, 8000: 0.13126914742426016\n",
      "Epoch 7, 8200: 0.1306422472552101\n",
      "Epoch 7, 8400: 0.13094430164323145\n",
      "Epoch 8, 200: 0.11083336230367422\n",
      "Epoch 8, 400: 0.1202480329759419\n",
      "Epoch 8, 600: 0.13706924422333638\n",
      "Epoch 8, 800: 0.13945750351995229\n",
      "Epoch 8, 1000: 0.13319297566451133\n",
      "Epoch 8, 1200: 0.1319011125123749\n",
      "Epoch 8, 1400: 0.13311512216925622\n",
      "Epoch 8, 1600: 0.133801313561853\n",
      "Epoch 8, 1800: 0.13240340180281135\n",
      "Epoch 8, 2000: 0.13643916030228137\n",
      "Epoch 8, 2200: 0.13409942166033118\n",
      "Epoch 8, 2400: 0.13156220419798045\n",
      "Epoch 8, 2600: 0.1336151464469731\n",
      "Epoch 8, 2800: 0.13248314138767975\n",
      "Epoch 8, 3000: 0.13292881620054445\n",
      "Epoch 8, 3200: 0.13100835047487636\n",
      "Epoch 8, 3400: 0.13325615431291654\n",
      "Epoch 8, 3600: 0.1319409270149966\n",
      "Epoch 8, 3800: 0.13386354888642304\n",
      "Epoch 8, 4000: 0.13390759165678173\n",
      "Epoch 8, 4200: 0.13253302310726472\n",
      "Epoch 8, 4400: 0.13273196696134454\n",
      "Epoch 8, 4600: 0.1340337295352441\n",
      "Epoch 8, 4800: 0.13489102624123916\n",
      "Epoch 8, 5000: 0.13450705753043293\n",
      "Epoch 8, 5200: 0.13484144501531353\n",
      "Epoch 8, 5400: 0.1351016208005172\n",
      "Epoch 8, 5600: 0.13464989126393836\n",
      "Epoch 8, 5800: 0.13499037674403397\n",
      "Epoch 8, 6000: 0.13556739071570337\n",
      "Epoch 8, 6200: 0.13503398065004618\n",
      "Epoch 8, 6400: 0.13495316616958009\n",
      "Epoch 8, 6600: 0.13405311756344004\n",
      "Epoch 8, 6800: 0.13326139507541324\n",
      "Epoch 8, 7000: 0.13206923259368963\n",
      "Epoch 8, 7200: 0.13109622140699584\n",
      "Epoch 8, 7400: 0.13117355481922827\n",
      "Epoch 8, 7600: 0.13133757939390642\n",
      "Epoch 8, 7800: 0.1315172204697648\n",
      "Epoch 8, 8000: 0.13129073263145982\n",
      "Epoch 8, 8200: 0.13066210384753238\n",
      "Epoch 8, 8400: 0.13096658961337415\n",
      "Epoch 9, 200: 0.11054692555218935\n",
      "Epoch 9, 400: 0.1200430266559124\n",
      "Epoch 9, 600: 0.1369263298312823\n",
      "Epoch 9, 800: 0.13935061560478063\n",
      "Epoch 9, 1000: 0.1331101447995752\n",
      "Epoch 9, 1200: 0.13181818276798973\n",
      "Epoch 9, 1400: 0.13305989944109958\n",
      "Epoch 9, 1600: 0.13375181542593054\n",
      "Epoch 9, 1800: 0.1323559260709832\n",
      "Epoch 9, 2000: 0.13639855302590878\n",
      "Epoch 9, 2200: 0.13406136525032872\n",
      "Epoch 9, 2400: 0.13152364740225797\n",
      "Epoch 9, 2600: 0.13357576217860556\n",
      "Epoch 9, 2800: 0.13244198780174235\n",
      "Epoch 9, 3000: 0.13288820228042703\n",
      "Epoch 9, 3200: 0.13097281525435392\n",
      "Epoch 9, 3400: 0.13321982574210886\n",
      "Epoch 9, 3600: 0.13190522852560713\n",
      "Epoch 9, 3800: 0.13383169881901458\n",
      "Epoch 9, 4000: 0.13387699074205012\n",
      "Epoch 9, 4200: 0.1325022968073331\n",
      "Epoch 9, 4400: 0.13270083216780967\n",
      "Epoch 9, 4600: 0.13400083851231182\n",
      "Epoch 9, 4800: 0.13486648293367276\n",
      "Epoch 9, 5000: 0.13448860036209226\n",
      "Epoch 9, 5200: 0.13482405607087108\n",
      "Epoch 9, 5400: 0.13508735815201092\n",
      "Epoch 9, 5600: 0.13463137089740485\n",
      "Epoch 9, 5800: 0.1349714960999273\n",
      "Epoch 9, 6000: 0.13554709608107804\n",
      "Epoch 9, 6200: 0.1350129262285848\n",
      "Epoch 9, 6400: 0.13493129504437093\n",
      "Epoch 9, 6600: 0.13402975419141128\n",
      "Epoch 9, 6800: 0.13323737971265526\n",
      "Epoch 9, 7000: 0.1320488935932517\n",
      "Epoch 9, 7200: 0.13107710193195898\n",
      "Epoch 9, 7400: 0.13115382305698822\n",
      "Epoch 9, 7600: 0.1313115163936623\n",
      "Epoch 9, 7800: 0.1314894748507784\n",
      "Epoch 9, 8000: 0.13126417996035888\n",
      "Epoch 9, 8200: 0.13063643830192342\n",
      "Epoch 9, 8400: 0.13094092637278318\n",
      "Epoch 10, 200: 0.11045953776687384\n",
      "Epoch 10, 400: 0.11999926573596895\n",
      "Epoch 10, 600: 0.13684458746264377\n",
      "Epoch 10, 800: 0.1392740404047072\n",
      "Epoch 10, 1000: 0.13304520081169904\n",
      "Epoch 10, 1200: 0.1317503608049204\n",
      "Epoch 10, 1400: 0.13298864472923536\n",
      "Epoch 10, 1600: 0.13368524851975963\n",
      "Epoch 10, 1800: 0.13228659405476517\n",
      "Epoch 10, 2000: 0.1363317955210805\n",
      "Epoch 10, 2200: 0.13400077577003025\n",
      "Epoch 10, 2400: 0.1314641151530668\n",
      "Epoch 10, 2600: 0.133518591132015\n",
      "Epoch 10, 2800: 0.132388262655586\n",
      "Epoch 10, 3000: 0.1328378265413145\n",
      "Epoch 10, 3200: 0.13092728189018088\n",
      "Epoch 10, 3400: 0.13317646306501155\n",
      "Epoch 10, 3600: 0.13186245599606386\n",
      "Epoch 10, 3800: 0.1337915069467731\n",
      "Epoch 10, 4000: 0.13383394340006635\n",
      "Epoch 10, 4200: 0.13246298574221632\n",
      "Epoch 10, 4400: 0.1326591286939484\n",
      "Epoch 10, 4600: 0.13395773660513047\n",
      "Epoch 10, 4800: 0.1348205895050584\n",
      "Epoch 10, 5000: 0.1344428044293076\n",
      "Epoch 10, 5200: 0.1347771337438518\n",
      "Epoch 10, 5400: 0.1350406561664271\n",
      "Epoch 10, 5600: 0.1345760690025054\n",
      "Epoch 10, 5800: 0.13491326605612092\n",
      "Epoch 10, 6000: 0.13548311406901728\n",
      "Epoch 10, 6200: 0.13495004069751068\n",
      "Epoch 10, 6400: 0.13486245666950708\n",
      "Epoch 10, 6600: 0.1339562163041961\n",
      "Epoch 10, 6800: 0.13316446140490693\n",
      "Epoch 10, 7000: 0.13196954644551234\n",
      "Epoch 10, 7200: 0.1309892816738122\n",
      "Epoch 10, 7400: 0.13107006713345243\n",
      "Epoch 10, 7600: 0.13122786818856472\n",
      "Epoch 10, 7800: 0.13140771784939062\n",
      "Epoch 10, 8000: 0.13118346554855817\n",
      "Epoch 10, 8200: 0.13055639491779958\n",
      "Epoch 10, 8400: 0.1308589033904441\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "EPOCHS = 10\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_validation_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0 ##sets the starting loss at zero\n",
    "    count = 0 #is used to keep track of the number of batches passed through the training model\n",
    "  \n",
    "    for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(train_dataloader): \n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(\n",
    "            pv_features.to(device, dtype=torch.float),\n",
    "            hrv_features.to(device, dtype=torch.float),\n",
    "        )\n",
    "        loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item() * pv_targets.size(0)\n",
    "        count += pv_targets.size(0)\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            \n",
    "            batch_loss = running_loss / count\n",
    "            training_losses.append(batch_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, {i + 1}: {batch_loss}\")\n",
    "            \n",
    "            # print(f\"     Training Loss: {batch_loss}\")\n",
    "\n",
    "            # validation_loss = model_validation(model, criterion, validation_dataloader)\n",
    "            # validation_losses.append(validation_loss)\n",
    "            # print(f\"     Validation Loss: {validation_loss}\\n\")\n",
    "            \n",
    "    \n",
    "    epoch_train_loss = running_loss / count\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # epoch_validation_loss = model_validation(model, criterion, validation_dataloader)\n",
    "    # epoch_validation_losses.append(epoch_validation_loss)\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}, Training Loss: {epoch_train_loss}\")\n",
    "    # print(f\"Epoch {epoch + 1}, Validation Loss: {epoch_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ResNet_light_deep_crop2_inc_10epoch\"\n",
    "os.makedirs(f\"models/{model_name}\", exist_ok=True)\n",
    "\n",
    "# Save the variables used to make the dataset to a text file\n",
    "with open(f\"models/{model_name}/data_summary.txt\", \"w\") as f:\n",
    "    f.write(\"BATCH_SIZE = \"+ str(BATCH_SIZE))\n",
    "            \n",
    "    \n",
    "# Save the trained model for future predictions\n",
    "torch.save(model.state_dict(), f\"models/{model_name}/trained_model.pt\")\n",
    "\n",
    "# Create a DataFrame from the training_losses and validation_losses lists\n",
    "df = pd.DataFrame({'Training Losses': training_losses})\n",
    "# df = pd.DataFrame({'Training Losses': batch_losses, 'Validation Losses': validation_losses})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(f'models\\{model_name}\\losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_light_inc(\n",
       "  (initial): Sequential(\n",
       "    (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=100, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/ResNet_light_deep_crop1_inc_128/trained_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_visual(dataloader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    predictions_list = []\n",
    "    timestamps_list = []\n",
    "    pv_targets_list = []  # List to store pv_targets for each batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(dataloader):\n",
    "            hrv_features = hrv_features.to(device, dtype=torch.float)\n",
    "            pv_features = pv_features.to(device, dtype=torch.float)\n",
    "            pv_targets = pv_targets.to(device, dtype=torch.float)\n",
    "            \n",
    "            batch_predictions = model(pv_features, hrv_features)\n",
    "            batch_predictions = batch_predictions.cpu().numpy()\n",
    "            batch_pv_targets = pv_targets.cpu().numpy()  # Convert pv_targets to numpy array\n",
    "\n",
    "            # Timestamp processing as before\n",
    "            if isinstance(time_ids[0], tuple) or isinstance(time_ids[0], list):\n",
    "                single_timestamp = time_ids[0][0]\n",
    "            else:\n",
    "                single_timestamp = time_ids[0]\n",
    "            if isinstance(single_timestamp, datetime):\n",
    "                timestamp = single_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            else:\n",
    "                timestamp = str(single_timestamp)\n",
    "            \n",
    "            # Append each batch's data to the lists\n",
    "            predictions_list.append(batch_predictions)\n",
    "            pv_targets_list.append(batch_pv_targets)  # Append pv_targets to its list\n",
    "            batch_timestamps = [timestamp] * batch_predictions.shape[0]\n",
    "            timestamps_list.extend(batch_timestamps)\n",
    "\n",
    "    # Concatenate all collected arrays into single numpy arrays\n",
    "    predictions = np.concatenate(predictions_list, axis=0)\n",
    "    pv_targets = np.concatenate(pv_targets_list, axis=0)  # Concatenate all pv_targets\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    pv_targets_df = pd.DataFrame(pv_targets, columns=[f'target_{i}' for i in range(pv_targets.shape[1])])\n",
    "    timestamps_df = pd.DataFrame(timestamps_list, columns=['timestamp'])\n",
    "\n",
    "    # Combine timestamps, predictions, and targets by using index alignment\n",
    "    final_df = pd.concat([timestamps_df, predictions_df, pv_targets_df], axis=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = _eval_visual(validation_dataloader, model, device)\n",
    "prediction_df[['date', 'time']] = prediction_df['timestamp'].str.split('T', expand=True)\n",
    "timestamp_index = prediction_df.columns.get_loc('timestamp')\n",
    "prediction_df.insert(timestamp_index, 'date', prediction_df.pop('date'))\n",
    "prediction_df.insert(timestamp_index + 1, 'time', prediction_df.pop('time'))\n",
    "prediction_df.drop('timestamp', axis=1, inplace=True)\n",
    "predictions_RMSE = prediction_df[11]\n",
    "targets_RMSE = prediction_df['target_11']\n",
    "mse = (predictions_RMSE - targets_RMSE) ** 2\n",
    "rmse = np.sqrt(mse)\n",
    "prediction_df['RMSE_1 hr'] = rmse\n",
    "\n",
    "prediction_df.to_csv('predictions.csv', index=False)\n",
    "print(prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
