{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6ExjmoPXoLU"
   },
   "source": [
    "## Importing packages\n",
    "\n",
    "Here, we import a number of packages we will need to train our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3F_1sE0rXoLU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, time, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "from ocf_blosc2 import Blosc2\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pvlib \n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this block to install all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRDvMchFXoLU",
    "outputId": "1ff0eb44-3f42-471b-8af2-3a9fb4f32ded"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device\n",
    "if not os.path.exists(\"submission\"):\n",
    "     os.makedirs(\"submission\", exist_ok=True)\n",
    "     #Installing locally means you do not need to rerun this each time you restart the notebook\n",
    "     !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/submission/competition.py --output submission/competition.py\n",
    "     !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/submission/doxa.yaml --output submission/doxa.yaml\n",
    "     !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/submission/model.py --output submission/model.py\n",
    "     !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/submission/run.py --output submission/run.py\n",
    "     !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/indices.json --output indices.json\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "    os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/7.parquet --output data/pv/2020/7.parquet\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/7.zarr.zip --output data/satellite-hrv/2020/7.zarr.zip\n",
    "pv = pd.read_parquet(\"data/pv/2020/7.parquet\").drop(\"generation_wh\", axis=1)\n",
    "pv.index = pv.index.set_levels([pv.index.levels[0].tz_localize(None), pv.index.levels[1]])\n",
    "\n",
    "#The parquet data here is similar to a dataframe. The \"power\" is the column with the other data types being indexes. The data is shaped with each timestamp being its own sub frame with the sites having their corresponding power (I think this is the % of their total possible yield).  \n",
    "hrv = xr.open_dataset(\n",
    "    \"data/satellite-hrv/2020/7.zarr.zip\", engine=\"zarr\", chunks=\"auto\"\n",
    ")\n",
    "#The way that this works is that it stores the image as a vector. The vectors are stored as an array of vectors. These then have a timestamp, as we only have one channel the array is a 1D set of vectors with the dimension being time. Read this to help you understand how this is being stored https://tutorial.xarray.dev/fundamentals/01_datastructures.html\n",
    "# To access I have included some examples below\n",
    "#The float value (float16-float64) shows the precision with which data is stored. Later on it is important to make sure that when you are feeding in data into the model that the float type matches between data types, this currently is not a problem when only using the HRV data. I am not yet sure if this will be a problem when using the NWP data.\n",
    "with open(\"indices.json\") as f:\n",
    "    site_locations = {\n",
    "        data_source: {\n",
    "            int(site): (int(location[0]), int(location[1]))\n",
    "            for site, location in locations.items()# if site == '2657'#added this to run only 1 site location to understand how it works\n",
    "        }\n",
    "        for data_source, locations in json.load(f).items()\n",
    "    }\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "    os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/7.parquet --output data/pv/2020/7.parquet\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/7.zarr.zip --output data/satellite-hrv/2020/7.zarr.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hrv': {2607: (481, 224), 2626: (343, 345), 2631: (322, 187), 2657: (497, 174), 2660: (579, 214), 2729: (461, 180), 2760: (355, 414), 2766: (265, 380), 2770: (369, 219), 2775: (192, 108), 2776: (422, 255), 2784: (488, 220), 2789: (364, 401), 2813: (402, 237), 2814: (465, 208), 2815: (482, 236), 2816: (390, 230), 2822: (339, 203), 2824: (335, 131), 2828: (479, 223), 2832: (380, 191), 2833: (154, 109), 2834: (420, 245), 2835: (420, 228), 2855: (314, 375), 2880: (233, 365), 2881: (439, 202), 2903: (426, 196), 2908: (205, 99), 2912: (486, 137), 2918: (464, 199), 2940: (462, 200), 2975: (347, 164), 2997: (330, 291), 2998: (413, 174), 3000: (435, 444), 3001: (321, 231), 3005: (382, 276), 3007: (294, 372), 3016: (315, 371), 3019: (437, 201), 3026: (145, 106), 3039: (311, 323), 3074: (107, 86), 3080: (475, 218), 3085: (469, 199), 3089: (339, 455), 3093: (129, 97), 3094: (320, 378), 3100: (367, 401), 3117: (598, 233), 3122: (271, 368), 3126: (308, 381), 3146: (322, 375), 3147: (468, 154), 3149: (417, 248), 3152: (305, 377), 3175: (271, 368), 3208: (289, 157), 3231: (298, 380), 3233: (319, 347), 3235: (373, 365), 3236: (530, 204), 3237: (401, 225), 3239: (487, 171), 3240: (243, 136), 3242: (360, 411), 3248: (251, 136), 3249: (294, 373), 3250: (460, 138), 3258: (307, 375), 3263: (280, 371), 3268: (398, 415), 3270: (131, 105), 3278: (297, 379), 3281: (474, 172), 3288: (307, 178), 3311: (439, 138), 3323: (283, 147), 3324: (271, 166), 3325: (267, 143), 3326: (313, 172), 3333: (414, 251), 3431: (337, 189), 3432: (220, 182), 3433: (555, 192), 3435: (474, 168), 3437: (291, 382), 3442: (309, 382), 3443: (435, 150), 3444: (567, 238), 3453: (510, 167), 3454: (516, 183), 3469: (359, 283), 3472: (374, 218), 3473: (424, 161), 3476: (125, 95), 3487: (304, 379), 3488: (208, 177), 3489: (197, 142), 3496: (559, 220), 3513: (472, 136), 3536: (463, 198), 3540: (408, 218), 3543: (555, 192), 3770: (354, 469), 3799: (387, 166), 3805: (476, 169), 3811: (105, 93), 3822: (547, 168), 3831: (339, 191), 3857: (544, 154), 3864: (357, 349), 3872: (265, 383), 3951: (599, 229), 3952: (564, 170), 4002: (408, 146), 4003: (324, 375), 4029: (501, 164), 4030: (296, 380), 4033: (527, 192), 4035: (379, 218), 4044: (411, 236), 4045: (316, 374), 4046: (419, 430), 4065: (455, 208), 4090: (321, 156), 4114: (433, 461), 4127: (359, 175), 4157: (409, 247), 4158: (445, 448), 4159: (444, 350), 4238: (301, 379), 4392: (382, 466), 4421: (481, 272), 4422: (434, 461), 4476: (355, 340), 4480: (382, 466), 5177: (573, 250), 5265: (311, 323), 5352: (441, 444), 5358: (473, 231), 5372: (443, 352), 5427: (229, 113), 5428: (392, 466), 5429: (239, 177), 5444: (237, 126), 5512: (421, 247), 5596: (294, 382), 5597: (282, 372), 5598: (274, 372), 5660: (559, 161), 5690: (313, 379), 5753: (474, 230), 5754: (224, 117), 5778: (422, 452), 5780: (557, 168), 5781: (308, 381), 5803: (421, 288), 5805: (450, 454), 5826: (429, 466), 5890: (192, 107), 5895: (413, 222), 5899: (504, 182), 5900: (420, 498), 5908: (314, 322), 5974: (402, 142), 5986: (472, 214), 6011: (408, 290), 6032: (280, 370), 6035: (459, 462), 6049: (282, 372), 6064: (423, 363), 6075: (306, 382), 6093: (385, 411), 6094: (578, 226), 6126: (453, 313), 6127: (430, 455), 6329: (408, 170), 6336: (342, 165), 6362: (294, 372), 6363: (382, 390), 6364: (310, 323), 6380: (421, 384), 6409: (327, 128), 6410: (315, 376), 6424: (546, 168), 6425: (297, 381), 6427: (388, 166), 6431: (428, 447), 6433: (298, 381), 6441: (209, 192), 6442: (317, 343), 6452: (339, 265), 6472: (230, 112), 6481: (381, 374), 6482: (196, 106), 6489: (226, 114), 6490: (392, 239), 6492: (402, 200), 6493: (424, 382), 6494: (325, 145), 6498: (337, 132), 6504: (146, 102), 6516: (373, 372), 6527: (289, 141), 6566: (335, 148), 6567: (459, 467), 6571: (426, 261), 6573: (374, 390), 6575: (366, 219), 6576: (366, 232), 6577: (453, 139), 6596: (311, 322), 6597: (313, 322), 6601: (230, 112), 6605: (460, 468), 6609: (452, 178), 6610: (311, 379), 6613: (299, 141), 6614: (387, 137), 6616: (457, 167), 6618: (399, 139), 6620: (342, 233), 6629: (404, 285), 6633: (319, 265), 6634: (401, 288), 6637: (184, 107), 6638: (378, 374), 6640: (377, 401), 6641: (312, 167), 6646: (458, 461), 6648: (459, 140), 6656: (226, 108), 6663: (596, 227), 6665: (368, 400), 6667: (437, 452), 6669: (325, 234), 6671: (356, 346), 6672: (428, 258), 6673: (438, 154), 6675: (406, 153), 6676: (325, 329), 6677: (554, 192), 6678: (552, 192), 6682: (418, 275), 6721: (435, 445), 6723: (407, 248), 6726: (424, 275), 6727: (360, 175), 6729: (363, 176), 6732: (414, 137), 6748: (338, 169), 6780: (446, 167), 6781: (257, 151), 6785: (280, 155), 6786: (486, 291), 6789: (446, 446), 6794: (276, 386), 6795: (427, 258), 6796: (488, 138), 6798: (510, 289), 6800: (428, 258), 6801: (363, 175), 6807: (300, 397), 6815: (360, 176), 6817: (285, 388), 6826: (152, 109), 6827: (422, 256), 6828: (318, 270), 6830: (360, 330), 6834: (278, 170), 6835: (206, 126), 6838: (377, 401), 6839: (279, 364), 6842: (416, 291), 6845: (282, 380), 6846: (392, 181), 6848: (217, 114), 6856: (367, 306), 6862: (452, 260), 6865: (442, 346), 6867: (553, 158), 6869: (401, 465), 6871: (486, 178), 6872: (434, 289), 6873: (439, 166), 6874: (425, 301), 6875: (364, 447), 6880: (202, 146), 6882: (264, 163), 6891: (361, 330), 6892: (423, 258), 6917: (488, 257), 6928: (306, 381), 6932: (465, 262), 6936: (444, 299), 6938: (433, 279), 6949: (358, 174), 6958: (359, 174), 6961: (374, 326), 6962: (306, 381), 6964: (313, 378), 6966: (484, 137), 6967: (425, 301), 6975: (413, 260), 6979: (454, 150), 6981: (355, 173), 6982: (312, 324), 6990: (350, 278), 6991: (468, 281), 6994: (347, 161), 6998: (567, 231), 6999: (412, 260), 7001: (379, 327), 7017: (445, 300), 7019: (236, 400), 7025: (543, 194), 7033: (465, 284), 7043: (542, 195), 7044: (409, 465), 7049: (508, 278), 7050: (429, 175), 7051: (428, 180), 7060: (476, 168), 7061: (313, 167), 7085: (382, 338), 7088: (280, 155), 7090: (392, 163), 7092: (552, 191), 7093: (460, 310), 7116: (401, 149), 7118: (457, 195), 7119: (482, 289), 7120: (240, 133), 7136: (442, 350), 7149: (435, 144), 7151: (231, 114), 7152: (539, 197), 7159: (180, 120), 7163: (538, 195), 7166: (321, 347), 7171: (417, 170), 7173: (449, 174), 7174: (192, 114), 7176: (424, 383), 7177: (416, 170), 7184: (192, 114), 7190: (538, 197), 7194: (405, 169), 7199: (375, 401), 7200: (365, 304), 7202: (455, 468), 7213: (318, 380), 7229: (239, 126), 7230: (384, 376), 7234: (279, 138), 7239: (435, 437), 7240: (226, 115), 7241: (292, 380), 7243: (374, 388), 7245: (205, 126), 7247: (366, 250), 7248: (280, 357), 7255: (367, 450), 7257: (331, 155), 7258: (427, 174), 7259: (292, 166), 7262: (231, 112), 7274: (329, 162), 7275: (440, 173), 7276: (198, 127), 7277: (541, 195), 7285: (442, 445), 7286: (289, 349), 7287: (443, 443), 7290: (312, 324), 7291: (367, 220), 7295: (146, 102), 7312: (497, 288), 7313: (438, 286), 7314: (312, 381), 7315: (376, 273), 7317: (301, 379), 7338: (443, 351), 7344: (461, 170), 7349: (519, 191), 7351: (367, 400), 7356: (305, 382), 7359: (284, 144), 7368: (430, 137), 7369: (348, 172), 7378: (307, 153), 7387: (124, 97), 7390: (273, 386), 7393: (385, 391), 7395: (363, 174), 7396: (266, 142), 7399: (378, 375), 7401: (416, 438), 7402: (291, 381), 7408: (359, 175), 7409: (348, 151), 7410: (324, 376), 7411: (522, 199), 7412: (274, 371), 7439: (323, 156), 7440: (321, 347), 7445: (368, 178), 7447: (138, 107), 7451: (334, 147), 7452: (514, 195), 7464: (331, 158), 7468: (354, 173), 7471: (368, 178), 7473: (259, 141), 7478: (373, 372), 7487: (229, 112), 7490: (399, 279), 7495: (226, 109), 7498: (337, 130), 7517: (362, 177), 7519: (292, 146), 7520: (225, 110), 7521: (514, 195), 7527: (336, 265), 7528: (325, 272), 7533: (290, 365), 7534: (554, 208), 7537: (228, 119), 7547: (384, 390), 7548: (442, 445), 7550: (219, 110), 7553: (521, 199), 7554: (444, 453), 7556: (380, 406), 7557: (535, 188), 7558: (280, 370), 7560: (230, 114), 7562: (230, 114), 7566: (340, 169), 7568: (356, 313), 7569: (489, 187), 7579: (123, 91), 7580: (312, 171), 7581: (529, 206), 7585: (291, 380), 7593: (220, 109), 7595: (228, 112), 7603: (538, 195), 7608: (275, 138), 7633: (360, 386), 7634: (386, 390), 7635: (266, 387), 7636: (465, 468), 7640: (346, 164), 7642: (378, 141), 7644: (201, 138), 7648: (282, 372), 7651: (271, 144), 7660: (262, 444), 7661: (149, 103), 7662: (450, 335), 7664: (184, 105), 7670: (440, 328), 7674: (275, 370), 7693: (263, 446), 7694: (425, 343), 7700: (270, 375), 7702: (465, 198), 7719: (311, 322), 7720: (267, 341), 7721: (399, 283), 7723: (312, 320), 7748: (206, 144), 7756: (343, 273), 7757: (119, 84), 7758: (299, 341), 7759: (406, 191), 7762: (326, 382), 7763: (177, 126), 7766: (403, 296), 7767: (515, 194), 7776: (510, 289), 7830: (224, 117), 7832: (315, 380), 7833: (355, 340), 7834: (396, 455), 7836: (281, 382), 7837: (311, 323), 7843: (487, 291), 7844: (417, 293), 7845: (354, 341), 7865: (400, 140), 7885: (357, 343), 7901: (355, 341), 7902: (437, 340), 7904: (286, 389), 7905: (501, 306), 7906: (193, 105), 7907: (317, 266), 7910: (226, 116), 7927: (380, 295), 7928: (309, 151), 7930: (303, 379), 7932: (400, 161), 8066: (456, 272), 8090: (309, 381), 8099: (471, 196), 8137: (444, 350), 8175: (483, 308), 8215: (257, 150), 8229: (344, 282), 8253: (191, 108), 8266: (274, 179), 8267: (318, 172), 8281: (117, 95), 8411: (373, 275), 8419: (289, 381), 8420: (307, 379), 8453: (229, 113), 8464: (421, 444), 8505: (272, 370), 8551: (321, 348), 8558: (400, 393), 8587: (385, 390), 8591: (328, 384), 8612: (307, 381), 8708: (391, 462), 8713: (442, 346), 8801: (230, 194), 8914: (440, 346), 9069: (370, 416), 9070: (421, 318), 9153: (323, 156), 9171: (445, 347), 9173: (433, 338), 9191: (231, 194), 9350: (407, 326), 9366: (433, 326), 9367: (436, 447), 9368: (293, 381), 9369: (274, 178), 9478: (439, 342), 9479: (433, 339), 9480: (434, 335), 9530: (426, 288), 9531: (272, 177), 9569: (311, 400), 9648: (265, 164), 9763: (419, 264), 9764: (445, 345), 9765: (443, 346), 9815: (407, 460), 9816: (308, 240), 9866: (422, 255), 9867: (393, 286), 9870: (367, 226), 9871: (415, 271), 9902: (399, 165), 9903: (270, 169), 9960: (302, 368), 9989: (435, 343), 10003: (330, 266), 10005: (270, 373), 10048: (277, 389), 10063: (266, 164), 10064: (384, 345), 10082: (273, 366), 10086: (424, 444), 10150: (265, 172), 10168: (270, 261), 10169: (311, 381), 10190: (325, 370), 10206: (372, 269), 10222: (235, 174), 10254: (364, 375), 10280: (273, 372), 10361: (300, 378), 10366: (277, 261), 10367: (375, 279), 10425: (378, 280), 10426: (321, 264), 10438: (440, 326), 10440: (432, 339), 10497: (277, 184), 10509: (407, 326), 10511: (276, 364), 10512: (270, 386), 10523: (441, 344), 10528: (444, 327), 10532: (432, 450), 10533: (272, 173), 10548: (444, 329), 10585: (287, 180), 10589: (330, 396), 10595: (269, 174), 10620: (277, 359), 10630: (237, 196), 10631: (295, 199), 10648: (357, 407), 10649: (307, 387), 10692: (354, 290), 10693: (432, 341), 10702: (305, 385), 10704: (450, 454), 10791: (339, 256), 10792: (453, 324), 10793: (385, 466), 10794: (313, 327), 10835: (436, 445), 10837: (442, 345), 10838: (301, 378), 10840: (390, 463), 10841: (313, 258), 10842: (281, 175), 10843: (207, 179), 10844: (364, 274), 10890: (244, 178), 10929: (223, 174), 10973: (269, 373), 10975: (279, 385), 10976: (440, 439), 11040: (454, 452), 11042: (400, 393), 11166: (423, 466), 11174: (274, 387), 11175: (393, 459), 11176: (372, 462), 11287: (252, 353), 11401: (283, 372), 11438: (311, 322), 11441: (343, 395), 11465: (283, 183), 11466: (406, 465), 11494: (287, 367), 11522: (379, 405), 11558: (317, 379), 11573: (442, 444), 11591: (285, 182), 11628: (347, 192), 11630: (408, 425), 11656: (252, 169), 11657: (379, 280), 11685: (445, 442), 11687: (443, 445), 11811: (258, 176), 11865: (439, 349), 11895: (272, 387), 11987: (282, 374), 12013: (315, 199), 12029: (378, 406), 12035: (333, 389), 12091: (468, 460), 12092: (373, 411), 12314: (311, 382), 12368: (319, 182), 12371: (442, 445), 12451: (437, 452), 12487: (444, 327), 12495: (375, 391), 12642: (216, 194), 12646: (371, 285), 12647: (349, 254), 12761: (337, 381), 12764: (250, 169), 12772: (436, 344), 12826: (317, 370), 12847: (362, 277), 12860: (340, 203), 12886: (442, 345), 12887: (385, 369), 12917: (320, 270), 12918: (269, 373), 12919: (373, 372), 12920: (272, 372), 13012: (430, 431), 13052: (440, 354), 13057: (324, 376), 13308: (367, 344), 13309: (283, 370), 13310: (397, 374), 13311: (286, 382), 13388: (417, 169), 13389: (379, 186), 13390: (451, 195), 13498: (443, 445), 13499: (436, 356), 13570: (291, 364), 13607: (323, 376), 13641: (441, 328), 13670: (391, 464), 13767: (345, 283), 13768: (337, 276), 13769: (235, 183), 13773: (411, 270), 13811: (430, 455), 13817: (334, 266), 13840: (273, 366), 14316: (390, 464), 14394: (316, 382), 14452: (256, 176), 14453: (273, 177), 14467: (312, 382), 14531: (297, 385), 14577: (288, 175), 14649: (310, 378), 14650: (432, 450), 14657: (315, 381), 14672: (443, 343), 14673: (435, 333), 14674: (446, 326), 14757: (338, 204), 14859: (374, 400), 14861: (306, 389), 14923: (320, 269), 14924: (328, 289), 15051: (468, 461), 15091: (378, 417), 15359: (443, 345), 15603: (240, 252), 15721: (289, 365), 15765: (166, 178), 15830: (375, 401), 15846: (354, 313), 15850: (324, 306), 15851: (330, 306), 15852: (446, 345), 16214: (222, 173), 16216: (313, 382), 16267: (216, 176), 16298: (359, 399), 16364: (380, 293), 16474: (322, 258), 16480: (314, 265), 16483: (372, 284), 16570: (303, 378), 16597: (357, 384), 16715: (354, 314), 16717: (363, 274), 16769: (320, 266), 16770: (331, 282), 16920: (358, 399), 16921: (315, 267), 17035: (265, 182), 17062: (358, 384), 17166: (344, 287), 17333: (285, 388), 17465: (265, 383), 17878: (368, 400), 18019: (452, 276), 18128: (395, 285), 18161: (358, 278), 18249: (320, 264), 18873: (323, 277), 18989: (371, 277), 18990: (417, 288), 22335: (374, 254), 23083: (316, 251), 26771: (299, 167), 26774: (397, 182), 26775: (468, 172), 26776: (420, 265), 26777: (468, 159), 26780: (296, 168), 26781: (323, 165), 26782: (359, 174), 26783: (309, 170), 26784: (322, 156), 26785: (434, 199), 26786: (361, 167), 26787: (226, 115), 26788: (289, 160), 26790: (399, 160), 26791: (291, 160), 26792: (371, 273), 26793: (326, 258), 26794: (323, 165), 26795: (417, 169), 26796: (370, 271), 26797: (369, 271), 26799: (406, 199), 26800: (422, 162), 26801: (451, 189), 26802: (483, 179), 26804: (359, 175), 26805: (507, 163), 26806: (350, 275), 26808: (314, 168), 26809: (384, 273), 26810: (444, 177), 26811: (360, 283), 26812: (349, 275), 26813: (319, 158), 26816: (510, 181), 26817: (394, 181), 26818: (300, 154), 26822: (303, 154), 26823: (277, 149), 26824: (282, 156), 26826: (360, 176), 26827: (377, 186), 26828: (276, 148), 26829: (298, 166), 26830: (277, 174), 26831: (366, 271), 26832: (324, 261), 26833: (327, 269), 26835: (242, 136), 26836: (332, 265), 26837: (375, 275), 26838: (258, 176), 26839: (330, 272), 26840: (365, 275), 26841: (430, 190), 26844: (362, 274), 26845: (430, 165), 26846: (432, 160), 26848: (379, 186), 26851: (365, 213), 26852: (425, 249), 26853: (361, 177), 26854: (445, 167), 26855: (327, 268), 26856: (343, 216), 26858: (447, 177), 26860: (359, 174), 26861: (568, 236), 26862: (449, 180), 26863: (423, 182), 26865: (434, 175), 26867: (331, 160), 26868: (450, 163), 26869: (367, 277), 26870: (367, 276), 26871: (441, 183), 26872: (372, 199), 26873: (341, 268), 26874: (283, 160), 26875: (324, 272), 26876: (435, 163), 26878: (410, 220), 26879: (375, 274), 26880: (437, 174), 26881: (450, 168), 26882: (441, 188), 26884: (447, 159), 26885: (439, 184), 26886: (360, 177), 26887: (448, 215), 26889: (320, 264), 26890: (421, 165), 26892: (359, 175), 26893: (358, 174), 26894: (385, 278), 26895: (446, 186), 26896: (320, 163), 26897: (351, 149), 26898: (385, 187), 26900: (333, 167), 26901: (373, 275), 26903: (311, 170), 26904: (377, 272), 26905: (331, 272), 26906: (328, 274), 26907: (275, 177), 26908: (287, 174), 26909: (329, 265), 26910: (334, 269), 26911: (472, 190), 26912: (308, 186), 26913: (367, 281), 26914: (279, 178), 26917: (244, 182), 26918: (319, 182), 26919: (340, 272), 26920: (331, 273), 26921: (378, 281), 26922: (311, 165), 26923: (308, 183), 26924: (453, 171), 26925: (362, 216), 26927: (279, 156), 26928: (457, 159), 26929: (430, 156), 26930: (347, 207), 26932: (340, 189), 26933: (417, 169), 26934: (379, 218), 26935: (324, 266), 26937: (336, 133), 26938: (406, 169), 26940: (503, 199), 26941: (309, 166), 26942: (316, 170), 26943: (362, 163), 26944: (527, 192), 26947: (448, 175), 26949: (340, 168), 26951: (330, 160), 26952: (361, 273), 26953: (439, 137), 26956: (266, 173), 26957: (332, 268), 26958: (399, 281), 26959: (428, 180), 26960: (359, 269), 26963: (344, 269), 26964: (379, 144), 26965: (423, 271), 26966: (311, 200), 26967: (454, 167), 26968: (345, 278), 26969: (342, 271), 26970: (360, 285), 26972: (474, 168), 26973: (425, 161), 26974: (442, 176), 26975: (443, 178), 26977: (464, 199), 26979: (359, 175), 26980: (457, 160), 26981: (316, 161), 26982: (361, 202), 26985: (279, 156), 26986: (473, 190), 26987: (458, 195), 26990: (362, 177), 26991: (290, 157), 26992: (196, 106), 26993: (436, 279), 26994: (536, 192), 26995: (317, 182), 26996: (443, 178), 26997: (554, 195), 26998: (399, 150), 26999: (453, 167), 27000: (448, 162), 27002: (275, 133), 27003: (573, 226), 27004: (353, 149), 27005: (469, 193), 27006: (438, 192), 27007: (388, 136), 27008: (457, 163), 27009: (358, 179), 27011: (287, 162), 27012: (361, 176), 27015: (225, 110), 27016: (356, 176), 27017: (347, 167), 27018: (334, 162), 27020: (451, 173), 27022: (346, 168), 27023: (362, 176), 27024: (330, 157), 27025: (264, 145), 27027: (188, 107), 27028: (450, 186), 27029: (327, 268), 27030: (313, 172), 27031: (329, 271), 27032: (373, 275), 27033: (340, 187), 27034: (299, 163), 27035: (478, 171), 27036: (378, 186), 27037: (429, 189), 27041: (439, 169), 27042: (438, 195), 27043: (382, 282), 27045: (311, 169), 27046: (464, 170), 27047: (460, 197), 27048: (453, 196), 27049: (477, 170), 27050: (314, 168), 27051: (538, 197), 27052: (447, 159), 27053: (424, 164), 27054: (425, 168), 27055: (328, 162), 27056: (455, 139), 27057: (447, 175), 27058: (446, 214), 27060: (340, 168), 27061: (286, 162), 27062: (340, 271), 27063: (315, 165), 27064: (437, 172), 27065: (286, 162), 27066: (293, 166), 27067: (355, 275)}, 'nonhrv': {2607: (119, 123), 2626: (165, 164), 2631: (172, 111), 2657: (113, 107), 2660: (86, 120), 2729: (125, 109), 2760: (161, 187), 2766: (191, 175), 2770: (156, 122), 2775: (215, 85), 2776: (138, 134), 2784: (116, 122), 2789: (158, 182), 2813: (145, 128), 2814: (124, 118), 2815: (118, 127), 2816: (149, 125), 2822: (166, 116), 2824: (167, 92), 2828: (119, 123), 2832: (152, 112), 2833: (228, 85), 2834: (139, 130), 2835: (139, 125), 2855: (174, 174), 2880: (201, 170), 2881: (133, 116), 2903: (137, 114), 2908: (211, 82), 2912: (117, 94), 2918: (124, 115), 2940: (125, 115), 2975: (163, 103), 2997: (169, 146), 2998: (141, 107), 3000: (134, 197), 3001: (172, 126), 3005: (152, 141), 3007: (181, 173), 3016: (174, 172), 3019: (133, 116), 3026: (231, 84), 3039: (175, 156), 3074: (243, 77), 3080: (121, 121), 3085: (123, 115), 3089: (166, 200), 3093: (236, 81), 3094: (172, 175), 3100: (157, 182), 3117: (80, 126), 3122: (189, 171), 3126: (176, 176), 3146: (172, 174), 3147: (123, 100), 3149: (140, 131), 3152: (177, 174), 3175: (189, 171), 3208: (183, 101), 3231: (180, 175), 3233: (173, 164), 3235: (155, 170), 3236: (102, 117), 3237: (145, 124), 3239: (117, 106), 3240: (198, 94), 3242: (159, 186), 3248: (195, 94), 3249: (181, 173), 3250: (126, 95), 3258: (177, 174), 3263: (186, 172), 3268: (146, 187), 3270: (235, 84), 3278: (180, 175), 3281: (121, 106), 3288: (177, 108), 3311: (133, 95), 3323: (185, 98), 3324: (189, 104), 3325: (190, 96), 3326: (175, 106), 3333: (141, 132), 3431: (167, 112), 3432: (206, 109), 3433: (94, 113), 3435: (121, 105), 3437: (182, 176), 3442: (176, 176), 3443: (134, 99), 3444: (90, 128), 3453: (109, 104), 3454: (107, 110), 3469: (159, 143), 3472: (154, 121), 3473: (138, 102), 3476: (237, 80), 3487: (178, 175), 3488: (210, 108), 3489: (213, 96), 3496: (93, 122), 3513: (122, 94), 3536: (125, 115), 3540: (143, 121), 3543: (94, 113), 3770: (161, 205), 3799: (150, 104), 3805: (120, 105), 3811: (244, 80), 3822: (97, 105), 3831: (166, 112), 3857: (98, 100), 3864: (160, 165), 3872: (191, 176), 3951: (79, 125), 3952: (91, 105), 4002: (143, 97), 4003: (171, 174), 4029: (112, 103), 4030: (180, 175), 4033: (103, 113), 4035: (153, 121), 4044: (142, 127), 4045: (174, 173), 4046: (139, 192), 4065: (127, 118), 4090: (172, 101), 4114: (135, 202), 4127: (159, 107), 4157: (143, 131), 4158: (131, 198), 4159: (131, 165), 4238: (179, 175), 4392: (152, 204), 4421: (119, 139), 4422: (134, 202), 4476: (161, 162), 4480: (152, 204), 5177: (88, 132), 5265: (175, 156), 5352: (132, 197), 5358: (121, 126), 5372: (131, 166), 5427: (203, 86), 5428: (148, 204), 5429: (199, 108), 5444: (200, 91), 5512: (139, 131), 5596: (181, 176), 5597: (185, 173), 5598: (188, 173), 5660: (93, 102), 5690: (175, 175), 5753: (121, 125), 5754: (204, 88), 5778: (138, 199), 5780: (93, 105), 5781: (176, 176), 5803: (139, 145), 5805: (129, 200), 5826: (136, 204), 5890: (215, 84), 5895: (141, 123), 5899: (111, 109), 5900: (139, 215), 5908: (174, 156), 5974: (145, 96), 5986: (122, 120), 6011: (143, 145), 6032: (186, 172), 6035: (126, 203), 6049: (185, 173), 6064: (138, 170), 6075: (177, 176), 6093: (151, 186), 6094: (86, 124), 6126: (128, 153), 6127: (136, 200), 6329: (143, 105), 6336: (165, 104), 6362: (181, 173), 6363: (152, 179), 6364: (176, 156), 6380: (139, 177), 6409: (170, 91), 6410: (174, 174), 6424: (97, 105), 6425: (180, 176), 6427: (150, 104), 6431: (136, 198), 6433: (180, 176), 6441: (209, 113), 6442: (173, 163), 6452: (166, 137), 6472: (202, 86), 6481: (152, 173), 6482: (214, 84), 6489: (204, 87), 6490: (148, 128), 6492: (145, 115), 6493: (138, 176), 6494: (171, 97), 6498: (167, 93), 6504: (230, 83), 6516: (155, 173), 6527: (183, 96), 6566: (167, 98), 6567: (126, 204), 6571: (137, 136), 6573: (154, 179), 6575: (157, 122), 6576: (157, 126), 6577: (128, 95), 6596: (175, 156), 6597: (175, 156), 6601: (202, 86), 6605: (126, 205), 6609: (128, 108), 6610: (175, 175), 6613: (179, 96), 6614: (150, 94), 6616: (127, 104), 6618: (146, 95), 6620: (165, 126), 6629: (144, 144), 6633: (173, 137), 6634: (145, 145), 6637: (218, 84), 6638: (153, 173), 6640: (153, 182), 6641: (175, 104), 6646: (126, 202), 6648: (126, 95), 6656: (204, 85), 6663: (80, 124), 6665: (156, 182), 6667: (133, 199), 6669: (171, 127), 6671: (160, 164), 6672: (136, 135), 6673: (133, 100), 6675: (144, 100), 6676: (171, 158), 6677: (94, 113), 6678: (95, 113), 6682: (140, 140), 6721: (134, 197), 6723: (143, 131), 6726: (138, 140), 6727: (159, 107), 6729: (158, 107), 6732: (141, 94), 6748: (166, 105), 6780: (130, 104), 6781: (193, 99), 6785: (186, 100), 6786: (117, 146), 6789: (130, 197), 6794: (187, 177), 6795: (137, 135), 6796: (116, 95), 6798: (109, 145), 6800: (136, 135), 6801: (158, 107), 6807: (179, 181), 6815: (159, 107), 6817: (184, 178), 6826: (228, 85), 6827: (138, 134), 6828: (173, 139), 6830: (159, 159), 6834: (186, 105), 6835: (210, 91), 6838: (153, 182), 6839: (186, 170), 6842: (140, 146), 6845: (185, 175), 6846: (148, 109), 6848: (207, 87), 6856: (157, 151), 6862: (128, 135), 6865: (132, 164), 6867: (95, 101), 6869: (145, 204), 6871: (117, 108), 6872: (134, 145), 6873: (133, 104), 6874: (137, 149), 6875: (158, 198), 6880: (212, 97), 6882: (191, 103), 6891: (159, 159), 6892: (138, 135), 6917: (116, 134), 6928: (177, 176), 6932: (124, 136), 6936: (131, 148), 6938: (135, 142), 6949: (160, 107), 6958: (159, 107), 6961: (154, 157), 6962: (177, 176), 6964: (175, 175), 6966: (118, 94), 6967: (137, 149), 6975: (141, 135), 6979: (128, 99), 6981: (161, 106), 6982: (175, 157), 6990: (162, 141), 6991: (123, 142), 6994: (163, 102), 6998: (90, 126), 6999: (142, 135), 7001: (153, 158), 7017: (131, 149), 7019: (200, 182), 7025: (98, 113), 7033: (124, 143), 7043: (98, 114), 7044: (143, 204), 7049: (110, 141), 7050: (136, 107), 7051: (136, 109), 7060: (120, 105), 7061: (175, 104), 7085: (152, 161), 7088: (186, 100), 7090: (148, 103), 7092: (95, 112), 7093: (126, 152), 7116: (145, 98), 7118: (127, 114), 7119: (118, 145), 7120: (199, 93), 7136: (132, 165), 7149: (134, 97), 7151: (202, 87), 7152: (99, 114), 7159: (219, 89), 7163: (100, 114), 7166: (172, 164), 7171: (140, 105), 7173: (129, 107), 7174: (215, 87), 7176: (138, 176), 7177: (140, 105), 7184: (215, 87), 7190: (100, 114), 7194: (144, 105), 7199: (154, 182), 7200: (157, 150), 7202: (127, 205), 7213: (173, 175), 7229: (199, 91), 7230: (151, 174), 7234: (186, 95), 7239: (134, 194), 7240: (204, 87), 7241: (182, 175), 7243: (154, 178), 7245: (211, 91), 7247: (157, 132), 7248: (186, 168), 7255: (157, 199), 7257: (169, 100), 7258: (137, 107), 7259: (182, 104), 7262: (202, 86), 7274: (169, 103), 7275: (132, 106), 7276: (213, 91), 7277: (99, 114), 7285: (132, 197), 7286: (183, 165), 7287: (131, 196), 7290: (175, 157), 7291: (157, 122), 7295: (230, 83), 7312: (113, 145), 7313: (133, 144), 7314: (175, 176), 7315: (154, 140), 7317: (179, 175), 7338: (131, 166), 7344: (125, 105), 7349: (106, 112), 7351: (157, 182), 7356: (177, 176), 7359: (184, 97), 7368: (136, 94), 7369: (163, 106), 7378: (177, 100), 7387: (238, 81), 7390: (188, 177), 7393: (151, 179), 7395: (158, 107), 7396: (190, 96), 7399: (153, 174), 7401: (140, 195), 7402: (182, 176), 7408: (159, 107), 7409: (163, 99), 7410: (171, 174), 7411: (105, 115), 7412: (188, 172), 7439: (171, 101), 7440: (172, 164), 7445: (156, 108), 7447: (233, 84), 7451: (168, 98), 7452: (108, 114), 7464: (169, 101), 7468: (161, 106), 7471: (156, 108), 7473: (193, 96), 7478: (155, 173), 7487: (203, 86), 7490: (146, 142), 7495: (204, 85), 7498: (167, 92), 7517: (158, 108), 7519: (182, 97), 7520: (204, 85), 7521: (108, 114), 7527: (167, 137), 7528: (171, 139), 7533: (182, 170), 7534: (94, 118), 7537: (203, 88), 7547: (151, 179), 7548: (132, 197), 7550: (206, 85), 7553: (105, 115), 7554: (131, 200), 7556: (152, 184), 7557: (101, 111), 7558: (186, 172), 7560: (202, 87), 7562: (202, 87), 7566: (166, 105), 7568: (160, 153), 7569: (116, 111), 7579: (238, 79), 7580: (175, 106), 7581: (103, 117), 7585: (182, 175), 7593: (206, 85), 7595: (203, 86), 7603: (100, 114), 7608: (187, 95), 7633: (159, 177), 7634: (150, 179), 7635: (190, 178), 7636: (124, 205), 7640: (164, 103), 7642: (153, 96), 7644: (212, 95), 7648: (185, 173), 7651: (189, 97), 7660: (192, 197), 7661: (229, 83), 7662: (129, 160), 7664: (218, 84), 7670: (132, 158), 7674: (187, 172), 7693: (191, 197), 7694: (137, 163), 7700: (189, 174), 7702: (124, 115), 7719: (175, 156), 7720: (190, 162), 7721: (146, 143), 7723: (175, 155), 7748: (210, 97), 7756: (165, 140), 7757: (239, 77), 7758: (179, 162), 7759: (144, 112), 7762: (170, 176), 7763: (220, 91), 7766: (145, 147), 7767: (107, 113), 7776: (109, 145), 7830: (204, 88), 7832: (174, 175), 7833: (161, 162), 7834: (147, 200), 7836: (185, 176), 7837: (175, 156), 7843: (117, 146), 7844: (140, 146), 7845: (161, 162), 7865: (146, 95), 7885: (160, 163), 7901: (161, 162), 7902: (133, 162), 7904: (184, 178), 7905: (112, 151), 7906: (215, 84), 7907: (173, 137), 7910: (204, 87), 7927: (152, 147), 7928: (176, 99), 7930: (178, 175), 7932: (146, 102), 8066: (127, 139), 8090: (176, 176), 8099: (122, 114), 8137: (131, 165), 8175: (118, 151), 8215: (193, 99), 8229: (164, 143), 8253: (215, 85), 8266: (188, 108), 8267: (173, 106), 8281: (240, 80), 8411: (155, 140), 8419: (183, 176), 8420: (177, 175), 8453: (203, 86), 8464: (139, 197), 8505: (188, 172), 8551: (172, 165), 8558: (146, 180), 8587: (151, 179), 8591: (170, 177), 8612: (177, 176), 8708: (149, 203), 8713: (132, 164), 8801: (202, 113), 8914: (132, 164), 9069: (156, 187), 9070: (139, 155), 9153: (171, 101), 9171: (131, 164), 9173: (135, 161), 9191: (202, 113), 9350: (143, 157), 9366: (135, 157), 9367: (134, 198), 9368: (181, 176), 9369: (188, 108), 9478: (133, 163), 9479: (135, 162), 9480: (134, 160), 9530: (137, 145), 9531: (188, 108), 9569: (175, 182), 9648: (191, 103), 9763: (139, 137), 9764: (131, 164), 9765: (131, 164), 9815: (143, 202), 9816: (176, 129), 9866: (138, 134), 9867: (148, 144), 9870: (157, 124), 9871: (141, 139), 9902: (146, 104), 9903: (189, 105), 9960: (178, 171), 9989: (134, 163), 10003: (169, 137), 10005: (189, 173), 10048: (187, 178), 10063: (190, 103), 10064: (151, 164), 10082: (188, 171), 10086: (138, 197), 10150: (191, 106), 10168: (189, 136), 10169: (175, 176), 10190: (171, 172), 10206: (155, 138), 10222: (201, 107), 10254: (158, 174), 10280: (188, 173), 10361: (179, 175), 10366: (187, 136), 10367: (154, 142), 10425: (153, 142), 10426: (172, 137), 10438: (132, 157), 10440: (135, 162), 10497: (187, 110), 10509: (143, 157), 10511: (187, 170), 10512: (189, 177), 10523: (132, 163), 10528: (131, 158), 10532: (135, 199), 10533: (188, 106), 10548: (131, 158), 10585: (183, 109), 10589: (169, 181), 10595: (189, 107), 10620: (187, 168), 10630: (200, 114), 10631: (181, 115), 10648: (160, 184), 10649: (177, 178), 10692: (161, 145), 10693: (135, 162), 10702: (177, 177), 10704: (129, 200), 10791: (166, 134), 10792: (128, 157), 10793: (151, 204), 10794: (175, 158), 10835: (134, 197), 10837: (132, 164), 10838: (179, 175), 10840: (149, 203), 10841: (175, 135), 10842: (185, 107), 10843: (210, 108), 10844: (158, 140), 10890: (198, 108), 10929: (205, 107), 10973: (189, 173), 10975: (186, 177), 10976: (132, 195), 11040: (128, 199), 11042: (146, 180), 11166: (138, 204), 11174: (188, 178), 11175: (148, 202), 11176: (155, 203), 11287: (195, 166), 11401: (185, 173), 11438: (175, 156), 11441: (165, 180), 11465: (185, 110), 11466: (144, 204), 11494: (183, 171), 11522: (153, 184), 11558: (173, 175), 11573: (132, 197), 11591: (184, 109), 11628: (163, 113), 11630: (143, 190), 11656: (195, 105), 11657: (153, 142), 11685: (131, 196), 11687: (131, 197), 11811: (193, 107), 11865: (133, 165), 11895: (188, 178), 11987: (185, 173), 12013: (174, 115), 12029: (153, 184), 12035: (168, 178), 12091: (123, 202), 12092: (155, 186), 12314: (175, 176), 12368: (173, 109), 12371: (132, 197), 12451: (133, 199), 12487: (131, 158), 12495: (154, 179), 12642: (207, 113), 12646: (155, 144), 12647: (163, 133), 12761: (167, 176), 12764: (196, 105), 12772: (134, 163), 12826: (173, 172), 12847: (158, 141), 12860: (166, 116), 12886: (132, 164), 12887: (151, 172), 12917: (172, 139), 12918: (189, 173), 12919: (155, 173), 12920: (188, 173), 13012: (136, 192), 13052: (132, 167), 13057: (171, 174), 13308: (157, 163), 13309: (185, 172), 13310: (147, 173), 13311: (184, 176), 13388: (140, 105), 13389: (153, 111), 13390: (129, 114), 13498: (131, 197), 13499: (134, 167), 13570: (182, 170), 13607: (171, 174), 13641: (132, 158), 13670: (149, 203), 13767: (164, 143), 13768: (167, 141), 13769: (201, 110), 13773: (142, 139), 13811: (136, 200), 13817: (168, 137), 13840: (188, 171), 14316: (149, 203), 14394: (174, 176), 14452: (194, 107), 14453: (188, 108), 14467: (175, 176), 14531: (180, 177), 14577: (183, 107), 14649: (176, 175), 14650: (135, 199), 14657: (174, 176), 14672: (131, 163), 14673: (134, 160), 14674: (130, 157), 14757: (166, 117), 14859: (154, 182), 14861: (177, 178), 14923: (172, 138), 14924: (170, 145), 15051: (123, 202), 15091: (153, 188), 15359: (131, 164), 15603: (199, 133), 15721: (183, 170), 15765: (224, 108), 15830: (154, 182), 15846: (161, 153), 15850: (171, 151), 15851: (169, 151), 15852: (130, 164), 16214: (205, 106), 16216: (175, 176), 16267: (207, 107), 16298: (159, 182), 16364: (152, 146), 16474: (172, 135), 16480: (174, 137), 16483: (155, 143), 16570: (178, 175), 16597: (160, 177), 16715: (161, 153), 16717: (158, 140), 16769: (172, 137), 16770: (169, 143), 16920: (160, 182), 16921: (174, 138), 17035: (191, 109), 17062: (160, 177), 17166: (164, 144), 17333: (184, 178), 17465: (191, 176), 17878: (156, 182), 18019: (128, 141), 18128: (147, 144), 18161: (160, 141), 18249: (172, 137), 18873: (171, 141), 18989: (155, 141), 18990: (140, 145), 22335: (154, 133), 23083: (174, 132), 26771: (179, 104), 26774: (147, 109), 26775: (123, 106), 26776: (139, 137), 26777: (123, 102), 26780: (180, 105), 26781: (171, 104), 26782: (159, 107), 26783: (176, 105), 26784: (172, 101), 26785: (134, 115), 26786: (159, 104), 26787: (204, 87), 26788: (183, 102), 26790: (146, 102), 26791: (182, 102), 26792: (155, 140), 26793: (170, 135), 26794: (171, 104), 26795: (140, 105), 26796: (156, 139), 26797: (156, 139), 26799: (144, 115), 26800: (138, 103), 26801: (129, 112), 26802: (118, 108), 26804: (159, 107), 26805: (110, 103), 26806: (162, 140), 26808: (174, 105), 26809: (151, 140), 26810: (131, 108), 26811: (159, 143), 26812: (163, 140), 26813: (173, 101), 26816: (109, 109), 26817: (148, 109), 26818: (179, 100), 26822: (178, 100), 26823: (187, 98), 26824: (185, 101), 26826: (159, 107), 26827: (153, 111), 26828: (187, 98), 26829: (180, 104), 26830: (187, 107), 26831: (157, 139), 26832: (171, 136), 26833: (170, 138), 26835: (198, 94), 26836: (168, 137), 26837: (154, 140), 26838: (193, 107), 26839: (169, 139), 26840: (157, 140), 26841: (136, 112), 26844: (158, 140), 26845: (136, 104), 26846: (135, 102), 26848: (153, 111), 26851: (157, 120), 26852: (137, 132), 26853: (159, 108), 26854: (131, 104), 26855: (170, 138), 26856: (165, 121), 26858: (130, 108), 26860: (159, 107), 26861: (90, 127), 26862: (129, 109), 26863: (138, 109), 26865: (134, 107), 26867: (169, 102), 26868: (129, 103), 26869: (157, 141), 26870: (157, 141), 26871: (132, 110), 26872: (155, 115), 26873: (165, 138), 26874: (185, 102), 26875: (171, 139), 26876: (134, 103), 26878: (142, 122), 26879: (154, 140), 26880: (133, 107), 26881: (129, 105), 26882: (132, 111), 26884: (130, 102), 26885: (133, 110), 26886: (159, 108), 26887: (130, 120), 26889: (172, 137), 26890: (139, 104), 26892: (159, 107), 26893: (160, 107), 26894: (151, 141), 26895: (130, 111), 26896: (172, 103), 26897: (162, 98), 26898: (151, 111), 26900: (168, 104), 26901: (155, 140), 26903: (175, 105), 26904: (153, 139), 26905: (169, 139), 26906: (170, 140), 26907: (187, 108), 26908: (183, 107), 26909: (169, 137), 26910: (168, 138), 26911: (122, 112), 26912: (176, 111), 26913: (157, 142), 26914: (186, 108), 26917: (198, 109), 26918: (173, 109), 26919: (166, 139), 26920: (169, 140), 26921: (153, 142), 26922: (175, 104), 26923: (176, 110), 26924: (128, 106), 26925: (158, 121), 26927: (186, 101), 26928: (127, 102), 26929: (136, 101), 26930: (163, 118), 26932: (166, 112), 26933: (140, 105), 26934: (153, 121), 26935: (171, 137), 26937: (167, 93), 26938: (144, 105), 26940: (111, 115), 26941: (176, 104), 26942: (174, 105), 26943: (158, 103), 26944: (103, 113), 26947: (130, 107), 26949: (166, 105), 26951: (169, 102), 26952: (159, 140), 26953: (133, 94), 26956: (190, 106), 26957: (168, 138), 26958: (146, 142), 26959: (136, 109), 26960: (159, 138), 26963: (164, 138), 26964: (153, 97), 26965: (138, 139), 26966: (175, 115), 26967: (128, 104), 26968: (164, 141), 26969: (165, 139), 26970: (159, 144), 26972: (121, 105), 26973: (137, 102), 26974: (132, 107), 26975: (131, 108), 26977: (124, 115), 26979: (159, 107), 26980: (127, 102), 26981: (174, 102), 26982: (159, 116), 26985: (186, 101), 26986: (121, 112), 26987: (126, 114), 26990: (158, 108), 26991: (182, 101), 26992: (214, 84), 26993: (134, 142), 26994: (100, 113), 26995: (173, 109), 26996: (131, 108), 26997: (94, 114), 26998: (146, 99), 26999: (128, 104), 27000: (130, 103), 27002: (187, 93), 27003: (88, 124), 27004: (161, 98), 27005: (123, 113), 27006: (133, 113), 27007: (150, 94), 27008: (127, 103), 27009: (160, 108), 27011: (183, 103), 27012: (159, 107), 27015: (204, 85), 27016: (160, 107), 27017: (163, 104), 27018: (168, 103), 27020: (129, 106), 27022: (164, 105), 27023: (158, 107), 27024: (169, 101), 27025: (191, 97), 27027: (216, 84), 27028: (129, 111), 27029: (170, 138), 27030: (175, 106), 27031: (169, 139), 27032: (155, 140), 27033: (166, 111), 27034: (179, 103), 27035: (120, 106), 27036: (153, 111), 27037: (136, 112), 27041: (133, 105), 27042: (133, 114), 27043: (152, 143), 27045: (175, 105), 27046: (124, 105), 27047: (126, 114), 27048: (128, 114), 27049: (120, 105), 27050: (174, 105), 27051: (100, 114), 27052: (130, 102), 27053: (138, 103), 27054: (137, 105), 27055: (170, 103), 27056: (127, 95), 27057: (130, 107), 27058: (130, 120), 27060: (166, 105), 27061: (184, 103), 27062: (166, 139), 27063: (174, 104), 27064: (133, 106), 27065: (184, 103), 27066: (181, 104), 27067: (161, 140)}, 'weather': {2607: (174, 119), 2626: (125, 160), 2631: (136, 109), 2657: (182, 104), 2660: (200, 116), 2729: (172, 106), 2760: (120, 184), 2766: (98, 173), 2770: (145, 118), 2775: (110, 87), 2776: (156, 129), 2784: (176, 118), 2789: (124, 179), 2813: (152, 124), 2814: (171, 114), 2815: (173, 123), 2816: (150, 122), 2822: (139, 114), 2824: (144, 92), 2828: (174, 119), 2832: (151, 110), 2833: (100, 87), 2834: (156, 126), 2835: (158, 121), 2855: (113, 171), 2880: (91, 168), 2881: (165, 113), 2903: (162, 111), 2908: (114, 84), 2912: (181, 93), 2918: (172, 112), 2940: (171, 112), 2975: (144, 102), 2997: (128, 142), 2998: (160, 104), 3000: (139, 194), 3001: (132, 123), 3005: (144, 136), 3007: (108, 170), 3016: (114, 169), 3019: (164, 112), 3026: (99, 87), 3039: (119, 153), 3074: (91, 81), 3080: (173, 117), 3085: (173, 112), 3089: (109, 200), 3093: (95, 84), 3094: (114, 172), 3100: (125, 179), 3117: (204, 121), 3122: (102, 169), 3126: (111, 173), 3146: (115, 171), 3147: (176, 98), 3149: (155, 127), 3152: (110, 171), 3175: (102, 169), 3208: (130, 100), 3231: (108, 173), 3233: (118, 161), 3235: (131, 167), 3236: (188, 112), 3237: (153, 120), 3239: (179, 103), 3240: (121, 94), 3242: (122, 183), 3248: (123, 94), 3249: (108, 170), 3250: (175, 93), 3258: (111, 171), 3263: (104, 170), 3268: (132, 184), 3270: (95, 87), 3278: (108, 172), 3281: (176, 103), 3288: (133, 106), 3311: (170, 93), 3323: (130, 97), 3324: (125, 103), 3325: (126, 96), 3326: (135, 105), 3333: (154, 128), 3431: (140, 110), 3432: (110, 109), 3433: (195, 109), 3435: (176, 102), 3437: (105, 173), 3442: (111, 173), 3443: (168, 97), 3444: (195, 123), 3453: (185, 102), 3454: (186, 106), 3469: (136, 139), 3472: (147, 118), 3473: (164, 100), 3476: (95, 84), 3487: (110, 172), 3488: (108, 107), 3489: (108, 97), 3496: (194, 117), 3513: (178, 92), 3536: (171, 111), 3540: (156, 118), 3543: (195, 109), 3770: (111, 205), 3799: (154, 102), 3805: (177, 102), 3811: (90, 83), 3822: (195, 102), 3831: (140, 110), 3857: (195, 97), 3864: (129, 161), 3872: (98, 174), 3951: (204, 120), 3952: (199, 102), 4002: (161, 96), 4003: (116, 171), 4029: (183, 101), 4030: (107, 173), 4033: (188, 109), 4035: (148, 118), 4044: (155, 124), 4045: (114, 170), 4046: (136, 189), 4065: (168, 114), 4090: (139, 100), 4114: (136, 201), 4127: (147, 105), 4157: (153, 127), 4158: (142, 196), 4159: (153, 160), 4238: (109, 172), 4392: (120, 203), 4421: (170, 134), 4422: (137, 201), 4476: (129, 158), 4480: (120, 203), 5177: (196, 127), 5265: (119, 153), 5352: (141, 194), 5358: (171, 121), 5372: (152, 161), 5427: (119, 88), 5428: (123, 203), 5429: (116, 107), 5444: (120, 92), 5512: (157, 127), 5596: (106, 173), 5597: (104, 170), 5598: (102, 170), 5660: (198, 100), 5690: (112, 172), 5753: (172, 121), 5754: (118, 89), 5778: (134, 198), 5780: (197, 101), 5781: (111, 173), 5803: (153, 140), 5805: (142, 198), 5826: (135, 203), 5890: (110, 87), 5895: (156, 119), 5899: (183, 106), 5900: (127, 215), 5908: (120, 152), 5974: (160, 95), 5986: (172, 116), 6011: (149, 141), 6032: (104, 169), 6035: (144, 201), 6049: (104, 170), 6064: (146, 165), 6075: (110, 173), 6093: (129, 183), 6094: (199, 119), 6126: (159, 148), 6127: (136, 198), 6329: (160, 103), 6336: (143, 102), 6362: (108, 170), 6363: (131, 175), 6364: (119, 153), 6380: (143, 172), 6409: (142, 91), 6410: (113, 171), 6424: (194, 102), 6425: (107, 173), 6427: (155, 102), 6431: (137, 196), 6433: (108, 173), 6441: (106, 112), 6442: (118, 160), 6452: (133, 133), 6472: (120, 88), 6481: (132, 170), 6482: (112, 86), 6489: (118, 88), 6490: (150, 125), 6492: (156, 112), 6493: (144, 172), 6494: (140, 96), 6498: (145, 92), 6504: (99, 85), 6516: (131, 169), 6527: (132, 96), 6566: (143, 97), 6567: (144, 203), 6571: (156, 131), 6573: (128, 175), 6575: (144, 119), 6576: (144, 123), 6577: (173, 94), 6596: (119, 152), 6597: (120, 152), 6601: (120, 88), 6605: (144, 203), 6609: (170, 105), 6610: (112, 172), 6613: (134, 96), 6614: (157, 94), 6616: (172, 102), 6618: (160, 94), 6620: (137, 123), 6629: (148, 139), 6633: (128, 133), 6634: (147, 140), 6637: (108, 87), 6638: (132, 170), 6640: (128, 179), 6641: (135, 103), 6646: (144, 200), 6648: (174, 94), 6656: (119, 87), 6663: (203, 119), 6665: (125, 179), 6667: (139, 197), 6669: (132, 124), 6671: (129, 160), 6672: (157, 130), 6673: (168, 98), 6675: (160, 98), 6676: (122, 155), 6677: (195, 109), 6678: (194, 109), 6682: (153, 136), 6721: (139, 195), 6723: (153, 127), 6726: (155, 136), 6727: (147, 105), 6729: (148, 105), 6732: (164, 93), 6748: (142, 104), 6780: (169, 102), 6781: (123, 99), 6785: (128, 100), 6786: (170, 140), 6789: (142, 195), 6794: (101, 175), 6795: (157, 130), 6796: (182, 93), 6798: (177, 140), 6800: (157, 130), 6801: (148, 105), 6807: (106, 179), 6815: (147, 105), 6817: (103, 176), 6826: (100, 87), 6827: (156, 130), 6828: (127, 135), 6830: (132, 155), 6834: (126, 104), 6835: (112, 92), 6838: (128, 179), 6839: (104, 167), 6842: (151, 141), 6845: (103, 173), 6846: (155, 107), 6848: (116, 88), 6856: (136, 146), 6862: (164, 131), 6865: (153, 159), 6867: (197, 99), 6869: (126, 203), 6871: (179, 105), 6872: (156, 140), 6873: (168, 102), 6874: (152, 144), 6875: (118, 196), 6880: (109, 98), 6882: (123, 102), 6891: (132, 155), 6892: (156, 130), 6917: (173, 130), 6928: (110, 173), 6932: (167, 131), 6936: (158, 144), 6938: (157, 137), 6949: (147, 105), 6958: (147, 105), 6961: (136, 153), 6962: (110, 173), 6964: (112, 172), 6966: (181, 93), 6967: (153, 144), 6975: (153, 131), 6979: (173, 97), 6981: (146, 105), 6982: (119, 153), 6990: (135, 137), 6991: (166, 137), 6994: (145, 101), 6998: (196, 121), 6999: (153, 131), 7001: (137, 153), 7017: (158, 144), 7019: (87, 181), 7025: (192, 110), 7033: (165, 138), 7043: (192, 110), 7044: (129, 203), 7049: (177, 136), 7050: (164, 104), 7051: (164, 106), 7060: (177, 102), 7061: (136, 103), 7085: (137, 157), 7088: (128, 100), 7090: (156, 101), 7092: (194, 109), 7093: (161, 147), 7116: (159, 97), 7118: (170, 110), 7119: (169, 140), 7120: (120, 94), 7136: (152, 161), 7149: (168, 95), 7151: (120, 88), 7152: (191, 110), 7159: (106, 90), 7163: (191, 110), 7166: (119, 161), 7171: (162, 103), 7173: (169, 104), 7174: (110, 89), 7176: (144, 172), 7177: (162, 103), 7184: (110, 89), 7190: (190, 110), 7194: (159, 103), 7199: (127, 179), 7200: (136, 146), 7202: (142, 203), 7213: (113, 172), 7229: (120, 92), 7230: (133, 170), 7234: (129, 95), 7239: (140, 192), 7240: (118, 88), 7241: (106, 173), 7243: (129, 175), 7245: (112, 92), 7247: (142, 128), 7248: (106, 165), 7255: (118, 197), 7257: (141, 99), 7258: (164, 104), 7259: (130, 103), 7262: (120, 88), 7274: (140, 102), 7275: (167, 104), 7276: (110, 92), 7277: (191, 110), 7285: (141, 195), 7286: (109, 162), 7287: (142, 194), 7290: (119, 153), 7291: (145, 119), 7295: (99, 85), 7312: (173, 140), 7313: (157, 139), 7314: (112, 173), 7315: (142, 136), 7317: (109, 172), 7338: (153, 161), 7344: (173, 103), 7349: (186, 109), 7351: (125, 179), 7356: (110, 173), 7359: (130, 96), 7368: (168, 93), 7369: (144, 104), 7378: (135, 99), 7387: (94, 84), 7390: (100, 175), 7393: (132, 175), 7395: (148, 105), 7396: (126, 96), 7399: (132, 170), 7401: (135, 192), 7402: (106, 173), 7408: (147, 105), 7409: (146, 98), 7410: (116, 171), 7411: (186, 111), 7412: (102, 170), 7439: (139, 100), 7440: (119, 161), 7445: (149, 106), 7447: (97, 87), 7451: (143, 97), 7452: (185, 110), 7464: (141, 100), 7468: (146, 105), 7471: (149, 106), 7473: (124, 96), 7478: (130, 169), 7487: (119, 88), 7490: (148, 137), 7495: (119, 87), 7498: (145, 92), 7517: (147, 106), 7519: (132, 97), 7520: (119, 87), 7521: (185, 110), 7527: (132, 133), 7528: (128, 136), 7533: (108, 167), 7534: (194, 114), 7537: (118, 90), 7547: (131, 175), 7548: (141, 195), 7550: (117, 87), 7553: (186, 111), 7554: (141, 197), 7556: (128, 181), 7557: (190, 108), 7558: (104, 169), 7560: (119, 88), 7562: (119, 88), 7566: (142, 103), 7568: (132, 149), 7569: (179, 108), 7579: (95, 82), 7580: (135, 104), 7581: (188, 113), 7585: (106, 173), 7593: (117, 87), 7595: (119, 88), 7603: (191, 110), 7608: (128, 95), 7633: (125, 174), 7634: (132, 175), 7635: (97, 176), 7636: (145, 203), 7640: (144, 102), 7642: (154, 95), 7644: (110, 95), 7648: (104, 170), 7651: (127, 96), 7660: (87, 196), 7661: (100, 86), 7662: (156, 155), 7664: (108, 86), 7670: (154, 153), 7674: (103, 169), 7693: (87, 197), 7694: (148, 158), 7700: (100, 171), 7702: (172, 111), 7719: (119, 152), 7720: (104, 159), 7721: (147, 139), 7723: (120, 152), 7748: (110, 97), 7756: (133, 136), 7757: (94, 81), 7758: (113, 159), 7759: (157, 110), 7762: (116, 173), 7763: (105, 92), 7766: (147, 143), 7767: (185, 110), 7776: (177, 140), 7830: (118, 89), 7832: (113, 172), 7833: (129, 158), 7834: (126, 199), 7836: (103, 174), 7837: (119, 153), 7843: (170, 140), 7844: (151, 142), 7845: (129, 159), 7865: (160, 94), 7885: (129, 159), 7901: (129, 159), 7902: (152, 157), 7904: (103, 176), 7905: (173, 145), 7906: (111, 86), 7907: (127, 134), 7910: (118, 89), 7927: (141, 143), 7928: (136, 98), 7930: (109, 172), 7932: (158, 101), 8066: (164, 135), 8090: (111, 173), 8099: (173, 111), 8137: (153, 160), 8175: (168, 146), 8215: (123, 99), 8229: (133, 139), 8253: (110, 87), 8266: (124, 107), 8267: (136, 104), 8281: (92, 84), 8411: (141, 136), 8419: (105, 173), 8420: (111, 172), 8453: (119, 88), 8464: (135, 194), 8505: (102, 170), 8551: (119, 161), 8558: (136, 176), 8587: (132, 175), 8591: (116, 174), 8612: (110, 173), 8708: (124, 202), 8713: (153, 159), 8801: (112, 112), 8914: (152, 159), 9069: (124, 185), 9070: (150, 150), 9153: (139, 100), 9171: (153, 160), 9173: (151, 156), 9191: (112, 112), 9350: (145, 153), 9366: (152, 152), 9367: (139, 195), 9368: (106, 173), 9369: (125, 107), 9478: (152, 158), 9479: (151, 157), 9480: (152, 156), 9530: (154, 140), 9531: (124, 106), 9569: (109, 180), 9648: (124, 103), 9763: (155, 132), 9764: (154, 159), 9765: (153, 159), 9815: (129, 201), 9816: (127, 126), 9866: (156, 129), 9867: (145, 140), 9870: (144, 121), 9871: (153, 135), 9902: (158, 102), 9903: (124, 104), 9960: (111, 168), 9989: (151, 158), 10003: (131, 134), 10005: (101, 171), 10048: (100, 176), 10063: (124, 103), 10064: (137, 160), 10082: (102, 168), 10086: (136, 194), 10150: (123, 105), 10168: (115, 133), 10169: (112, 173), 10190: (117, 169), 10206: (141, 134), 10222: (115, 106), 10254: (128, 170), 10280: (102, 170), 10361: (109, 172), 10366: (117, 133), 10367: (141, 137), 10425: (142, 138), 10426: (128, 133), 10438: (154, 152), 10440: (151, 157), 10497: (125, 108), 10509: (145, 153), 10511: (104, 167), 10512: (99, 175), 10523: (153, 158), 10528: (155, 153), 10532: (138, 197), 10533: (124, 105), 10548: (155, 153), 10585: (128, 107), 10589: (115, 178), 10595: (124, 106), 10620: (105, 165), 10630: (113, 113), 10631: (128, 113), 10648: (121, 181), 10649: (109, 175), 10692: (134, 141), 10693: (151, 158), 10702: (109, 174), 10704: (143, 198), 10791: (134, 131), 10792: (158, 152), 10793: (121, 203), 10794: (119, 154), 10835: (140, 195), 10837: (153, 159), 10838: (109, 172), 10840: (123, 202), 10841: (127, 131), 10842: (127, 106), 10843: (107, 108), 10844: (139, 136), 10890: (117, 107), 10929: (112, 106), 10973: (100, 171), 10975: (102, 175), 10976: (142, 192), 11040: (144, 197), 11042: (136, 176), 11166: (133, 203), 11174: (100, 175), 11175: (125, 200), 11176: (118, 202), 11287: (98, 164), 11401: (104, 170), 11438: (119, 152), 11441: (119, 177), 11465: (126, 108), 11466: (128, 203), 11494: (107, 168), 11522: (128, 180), 11558: (113, 172), 11573: (141, 194), 11591: (127, 108), 11628: (142, 110), 11630: (134, 188), 11656: (120, 104), 11657: (142, 138), 11685: (143, 194), 11687: (141, 195), 11811: (121, 106), 11865: (152, 160), 11895: (99, 175), 11987: (104, 171), 12013: (133, 113), 12029: (128, 181), 12035: (117, 176), 12091: (147, 200), 12092: (125, 183), 12314: (111, 173), 12368: (136, 108), 12371: (141, 195), 12451: (139, 197), 12487: (155, 153), 12495: (129, 176), 12642: (108, 112), 12646: (140, 140), 12647: (137, 130), 12761: (119, 172), 12764: (119, 104), 12772: (151, 159), 12826: (115, 169), 12847: (138, 137), 12860: (139, 114), 12886: (153, 159), 12887: (134, 168), 12917: (127, 135), 12918: (100, 171), 12919: (131, 169), 12920: (101, 170), 13012: (140, 189), 13052: (151, 162), 13057: (116, 171), 13308: (132, 159), 13309: (105, 169), 13310: (137, 169), 13311: (104, 174), 13388: (162, 103), 13389: (151, 108), 13390: (169, 110), 13498: (141, 195), 13499: (150, 163), 13570: (108, 167), 13607: (116, 171), 13641: (154, 153), 13670: (123, 202), 13767: (133, 139), 13768: (131, 137), 13769: (114, 109), 13773: (152, 134), 13811: (136, 199), 13817: (132, 134), 13840: (102, 168), 14316: (123, 203), 14394: (113, 173), 14452: (120, 106), 14453: (124, 106), 14467: (112, 173), 14531: (107, 174), 14577: (128, 106), 14649: (112, 172), 14650: (138, 197), 14657: (112, 173), 14672: (153, 158), 14673: (152, 155), 14674: (156, 152), 14757: (139, 114), 14859: (127, 179), 14861: (109, 176), 14923: (128, 135), 14924: (128, 141), 15051: (147, 200), 15091: (126, 185), 15359: (153, 159), 15603: (108, 130), 15721: (107, 168), 15765: (96, 108), 15830: (127, 179), 15846: (132, 149), 15850: (125, 147), 15851: (126, 147), 15852: (154, 159), 16214: (112, 106), 16216: (112, 173), 16267: (110, 107), 16298: (123, 179), 16364: (141, 142), 16474: (129, 131), 16480: (126, 134), 16483: (140, 139), 16570: (110, 172), 16597: (124, 173), 16715: (132, 149), 16717: (138, 136), 16769: (128, 134), 16770: (129, 139), 16920: (123, 179), 16921: (126, 134), 17035: (122, 108), 17062: (125, 173), 17166: (132, 140), 17333: (103, 176), 17465: (98, 174), 17878: (125, 179), 18019: (162, 136), 18128: (146, 139), 18161: (137, 137), 18249: (128, 133), 18873: (127, 137), 18989: (140, 137), 18990: (152, 140), 22335: (143, 130), 23083: (128, 129), 26771: (132, 103), 26774: (156, 107), 26775: (174, 103), 26776: (155, 132), 26777: (175, 100), 26780: (131, 104), 26781: (138, 102), 26782: (147, 105), 26783: (134, 104), 26784: (139, 100), 26785: (164, 112), 26786: (148, 103), 26787: (118, 88), 26788: (130, 101), 26790: (158, 100), 26791: (131, 101), 26792: (141, 136), 26793: (130, 131), 26794: (138, 102), 26795: (162, 103), 26796: (141, 135), 26797: (140, 135), 26799: (157, 112), 26800: (164, 101), 26801: (169, 108), 26802: (178, 105), 26804: (147, 105), 26805: (185, 100), 26806: (135, 136), 26808: (136, 104), 26809: (144, 135), 26810: (168, 105), 26811: (137, 139), 26812: (135, 136), 26813: (138, 100), 26816: (184, 106), 26817: (155, 107), 26818: (133, 99), 26822: (134, 99), 26823: (128, 98), 26824: (129, 100), 26826: (147, 105), 26827: (150, 108), 26828: (128, 98), 26829: (132, 103), 26830: (126, 106), 26831: (140, 135), 26832: (129, 132), 26833: (129, 135), 26835: (120, 95), 26836: (131, 133), 26837: (142, 136), 26838: (121, 106), 26839: (130, 136), 26840: (139, 136), 26841: (164, 109), 26844: (138, 136), 26845: (165, 102), 26846: (166, 100), 26848: (151, 108), 26851: (145, 117), 26852: (157, 127), 26853: (147, 106), 26854: (169, 102), 26855: (129, 134), 26856: (139, 118), 26858: (169, 105), 26860: (147, 105), 26861: (196, 122), 26862: (169, 106), 26863: (162, 107), 26865: (166, 104), 26867: (141, 101), 26868: (171, 101), 26869: (139, 137), 26870: (139, 137), 26871: (167, 107), 26872: (148, 112), 26873: (133, 134), 26874: (129, 101), 26875: (128, 136), 26876: (167, 101), 26878: (156, 118), 26879: (142, 136), 26880: (166, 104), 26881: (170, 102), 26882: (167, 108), 26884: (170, 100), 26885: (166, 107), 26886: (147, 106), 26887: (166, 116), 26889: (128, 133), 26890: (163, 102), 26892: (147, 105), 26893: (147, 105), 26894: (144, 137), 26895: (168, 108), 26896: (138, 102), 26897: (147, 97), 26898: (152, 109), 26900: (141, 103), 26901: (141, 136), 26903: (135, 104), 26904: (142, 135), 26905: (130, 136), 26906: (129, 136), 26907: (125, 107), 26908: (128, 106), 26909: (130, 134), 26910: (131, 135), 26911: (174, 109), 26912: (132, 109), 26913: (139, 138), 26914: (126, 107), 26917: (116, 108), 26918: (136, 108), 26919: (133, 136), 26920: (130, 136), 26921: (142, 138), 26922: (135, 103), 26923: (133, 108), 26924: (171, 103), 26925: (144, 118), 26927: (128, 100), 26928: (173, 100), 26929: (166, 99), 26930: (141, 115), 26932: (141, 110), 26933: (162, 103), 26934: (148, 118), 26935: (129, 134), 26937: (144, 93), 26938: (159, 103), 26940: (182, 111), 26941: (135, 103), 26942: (136, 104), 26943: (148, 101), 26944: (188, 109), 26947: (169, 104), 26949: (142, 103), 26951: (140, 101), 26952: (138, 136), 26953: (170, 93), 26956: (123, 105), 26957: (131, 134), 26958: (148, 138), 26959: (164, 106), 26960: (138, 134), 26963: (134, 135), 26964: (154, 96), 26965: (155, 135), 26966: (132, 113), 26967: (171, 102), 26968: (133, 137), 26969: (133, 135), 26970: (136, 140), 26972: (176, 102), 26973: (164, 100), 26974: (168, 105), 26975: (168, 105), 26977: (172, 112), 26979: (147, 105), 26980: (173, 100), 26981: (137, 101), 26982: (145, 113), 26985: (128, 100), 26986: (174, 109), 26987: (170, 110), 26990: (147, 106), 26991: (131, 100), 26992: (111, 86), 26993: (158, 137), 26994: (190, 109), 26995: (135, 108), 26996: (168, 105), 26997: (195, 110), 26998: (159, 97), 26999: (171, 102), 27000: (170, 100), 27002: (129, 93), 27003: (198, 119), 27004: (147, 97), 27005: (173, 110), 27006: (165, 110), 27007: (157, 93), 27008: (172, 101), 27009: (146, 106), 27011: (129, 102), 27012: (147, 105), 27015: (119, 87), 27016: (146, 105), 27017: (144, 103), 27018: (141, 101), 27020: (170, 104), 27022: (144, 103), 27023: (147, 105), 27024: (141, 100), 27025: (125, 97), 27027: (109, 87), 27028: (169, 108), 27029: (129, 134), 27030: (135, 105), 27031: (130, 135), 27032: (141, 136), 27033: (141, 109), 27034: (132, 102), 27035: (177, 103), 27036: (151, 108), 27037: (163, 109), 27041: (167, 103), 27042: (165, 111), 27043: (143, 139), 27045: (135, 104), 27046: (174, 103), 27047: (171, 111), 27048: (169, 111), 27049: (177, 103), 27050: (136, 103), 27051: (190, 111), 27052: (170, 100), 27053: (164, 101), 27054: (164, 102), 27055: (140, 102), 27056: (174, 94), 27057: (169, 104), 27058: (166, 116), 27060: (142, 103), 27061: (129, 102), 27062: (133, 135), 27063: (136, 103), 27064: (167, 104), 27065: (129, 102), 27066: (130, 103), 27067: (136, 136)}, 'aerosols': {2607: (138, 145), 2626: (108, 120), 2631: (115, 151), 2657: (143, 155), 2660: (154, 147), 2729: (137, 153), 2760: (104, 104), 2766: (91, 111), 2770: (120, 145), 2775: (99, 165), 2776: (127, 139), 2784: (140, 146), 2789: (107, 107), 2813: (125, 142), 2814: (136, 148), 2815: (138, 143), 2816: (123, 143), 2822: (116, 148), 2824: (120, 162), 2828: (138, 145), 2832: (124, 151), 2833: (92, 165), 2834: (127, 141), 2835: (128, 144), 2855: (100, 113), 2880: (86, 114), 2881: (133, 149), 2903: (131, 150), 2908: (101, 167), 2912: (143, 161), 2918: (137, 150), 2940: (136, 149), 2975: (120, 156), 2997: (109, 131), 2998: (130, 154), 3000: (117, 98), 3001: (112, 143), 3005: (119, 134), 3007: (97, 113), 3016: (101, 114), 3019: (132, 149), 3026: (91, 165), 3039: (104, 124), 3074: (86, 169), 3080: (138, 146), 3085: (138, 150), 3089: (98, 95), 3093: (89, 167), 3094: (101, 112), 3100: (108, 107), 3117: (157, 144), 3122: (93, 114), 3126: (99, 111), 3146: (102, 113), 3147: (139, 158), 3149: (126, 140), 3152: (99, 112), 3175: (93, 114), 3208: (111, 157), 3231: (97, 112), 3233: (103, 119), 3235: (111, 115), 3236: (147, 149), 3237: (125, 144), 3239: (142, 155), 3240: (105, 160), 3242: (105, 105), 3248: (106, 160), 3249: (97, 113), 3250: (139, 161), 3258: (99, 113), 3263: (94, 113), 3268: (112, 104), 3270: (89, 165), 3278: (97, 112), 3281: (139, 155), 3288: (113, 153), 3311: (135, 161), 3323: (110, 159), 3324: (108, 155), 3325: (108, 159), 3326: (114, 154), 3333: (126, 139), 3431: (117, 151), 3432: (98, 152), 3433: (152, 151), 3435: (140, 156), 3437: (95, 111), 3442: (99, 111), 3443: (134, 159), 3444: (152, 143), 3453: (145, 156), 3454: (146, 153), 3469: (115, 133), 3472: (121, 146), 3473: (132, 157), 3476: (89, 167), 3487: (98, 112), 3488: (97, 153), 3489: (97, 159), 3496: (151, 146), 3513: (141, 162), 3536: (137, 150), 3540: (127, 146), 3543: (151, 151), 3770: (99, 91), 3799: (126, 156), 3805: (140, 155), 3811: (86, 167), 3822: (151, 156), 3831: (117, 151), 3857: (151, 159), 3864: (110, 119), 3872: (90, 111), 3951: (157, 144), 3952: (154, 156), 4002: (130, 159), 4003: (102, 113), 4029: (144, 156), 4030: (96, 111), 4033: (147, 151), 4035: (122, 146), 4044: (126, 142), 4045: (101, 113), 4046: (115, 101), 4065: (135, 148), 4090: (116, 157), 4114: (115, 94), 4127: (121, 154), 4157: (125, 140), 4158: (118, 97), 4159: (125, 119), 4238: (98, 112), 4392: (105, 92), 4421: (136, 136), 4422: (115, 94), 4476: (110, 121), 4480: (105, 92), 5177: (152, 140), 5265: (104, 124), 5352: (118, 98), 5358: (137, 144), 5372: (125, 119), 5427: (104, 165), 5428: (106, 92), 5429: (102, 153), 5444: (105, 162), 5512: (127, 140), 5596: (96, 111), 5597: (95, 113), 5598: (93, 113), 5660: (153, 157), 5690: (100, 112), 5753: (137, 144), 5754: (103, 164), 5778: (114, 96), 5780: (153, 156), 5781: (99, 111), 5803: (125, 132), 5805: (119, 96), 5826: (114, 93), 5890: (99, 165), 5895: (127, 145), 5899: (144, 153), 5900: (109, 85), 5908: (104, 124), 5974: (130, 160), 5986: (137, 147), 6011: (123, 131), 6032: (95, 114), 6035: (120, 94), 6049: (95, 113), 6064: (120, 116), 6075: (98, 111), 6093: (110, 105), 6094: (154, 145), 6126: (129, 127), 6127: (115, 95), 6329: (129, 155), 6336: (119, 156), 6362: (97, 113), 6363: (111, 110), 6364: (104, 124), 6380: (119, 112), 6409: (119, 162), 6410: (100, 113), 6424: (151, 156), 6425: (97, 111), 6427: (126, 156), 6431: (115, 97), 6433: (97, 111), 6441: (96, 150), 6442: (103, 120), 6452: (113, 136), 6472: (104, 165), 6481: (112, 114), 6482: (99, 166), 6489: (103, 164), 6490: (123, 142), 6492: (127, 149), 6493: (119, 112), 6494: (117, 159), 6498: (120, 162), 6504: (91, 166), 6516: (111, 114), 6527: (112, 160), 6566: (119, 159), 6567: (119, 93), 6571: (127, 137), 6573: (110, 110), 6575: (120, 145), 6576: (119, 143), 6577: (138, 161), 6596: (104, 124), 6597: (104, 124), 6601: (104, 165), 6605: (119, 93), 6609: (136, 154), 6610: (99, 112), 6613: (113, 160), 6614: (127, 161), 6616: (137, 156), 6618: (129, 161), 6620: (115, 143), 6629: (122, 132), 6633: (109, 136), 6634: (122, 132), 6637: (97, 165), 6638: (112, 113), 6640: (109, 107), 6641: (114, 155), 6646: (120, 94), 6648: (139, 161), 6656: (104, 165), 6663: (157, 145), 6665: (108, 108), 6667: (116, 96), 6669: (112, 142), 6671: (110, 119), 6672: (128, 138), 6673: (135, 158), 6675: (130, 158), 6676: (106, 123), 6677: (151, 151), 6678: (151, 151), 6682: (125, 134), 6721: (116, 98), 6723: (125, 140), 6726: (126, 135), 6727: (121, 154), 6729: (122, 154), 6732: (132, 161), 6748: (118, 155), 6780: (135, 156), 6781: (106, 158), 6785: (110, 157), 6786: (136, 132), 6789: (118, 98), 6794: (92, 110), 6795: (128, 138), 6796: (143, 161), 6798: (140, 132), 6800: (128, 138), 6801: (122, 154), 6807: (96, 108), 6815: (121, 154), 6817: (94, 110), 6826: (92, 165), 6827: (127, 138), 6828: (109, 135), 6830: (112, 123), 6834: (109, 154), 6835: (100, 162), 6838: (109, 107), 6839: (95, 115), 6842: (124, 131), 6845: (94, 111), 6846: (126, 153), 6848: (102, 164), 6856: (115, 128), 6862: (132, 138), 6865: (125, 120), 6867: (152, 158), 6869: (108, 93), 6871: (141, 154), 6872: (127, 132), 6873: (134, 156), 6874: (125, 129), 6875: (103, 97), 6880: (98, 158), 6882: (107, 155), 6891: (112, 123), 6892: (127, 138), 6917: (138, 139), 6928: (98, 111), 6932: (134, 137), 6936: (128, 130), 6938: (128, 134), 6949: (121, 154), 6958: (121, 154), 6961: (115, 124), 6962: (98, 111), 6964: (100, 112), 6966: (142, 161), 6967: (125, 129), 6975: (125, 137), 6979: (137, 159), 6981: (121, 154), 6982: (104, 124), 6990: (114, 134), 6991: (133, 134), 6994: (120, 156), 6998: (152, 144), 6999: (125, 137), 7001: (115, 124), 7017: (128, 130), 7019: (84, 106), 7025: (149, 151), 7033: (133, 133), 7043: (149, 151), 7044: (110, 93), 7049: (140, 134), 7050: (132, 154), 7051: (132, 153), 7060: (140, 156), 7061: (114, 155), 7085: (115, 121), 7088: (110, 157), 7090: (127, 156), 7092: (151, 152), 7093: (130, 128), 7116: (129, 159), 7118: (136, 150), 7119: (135, 132), 7120: (105, 161), 7136: (125, 119), 7149: (135, 160), 7151: (104, 164), 7152: (149, 150), 7159: (96, 163), 7163: (149, 151), 7166: (104, 119), 7171: (131, 155), 7173: (135, 154), 7174: (98, 164), 7176: (119, 112), 7177: (130, 155), 7184: (98, 164), 7190: (148, 150), 7194: (129, 155), 7199: (109, 107), 7200: (114, 128), 7202: (118, 92), 7213: (100, 112), 7229: (105, 162), 7230: (113, 113), 7234: (110, 160), 7239: (117, 100), 7240: (103, 164), 7241: (96, 111), 7243: (110, 110), 7245: (99, 162), 7247: (118, 139), 7248: (95, 116), 7255: (103, 96), 7257: (118, 157), 7258: (132, 154), 7259: (111, 155), 7262: (104, 165), 7274: (117, 156), 7275: (134, 154), 7276: (98, 162), 7277: (149, 151), 7285: (118, 98), 7286: (98, 118), 7287: (118, 98), 7290: (104, 124), 7291: (120, 145), 7295: (92, 166), 7312: (138, 132), 7313: (128, 132), 7314: (99, 111), 7315: (118, 135), 7317: (98, 112), 7338: (125, 119), 7344: (138, 155), 7349: (146, 152), 7351: (108, 108), 7356: (98, 111), 7359: (111, 159), 7368: (134, 161), 7369: (120, 154), 7378: (114, 158), 7387: (88, 167), 7390: (92, 110), 7393: (112, 110), 7395: (122, 154), 7396: (108, 159), 7399: (112, 113), 7401: (114, 99), 7402: (95, 111), 7408: (121, 154), 7409: (121, 158), 7410: (102, 113), 7411: (146, 150), 7412: (93, 113), 7439: (116, 157), 7440: (104, 119), 7445: (122, 153), 7447: (90, 165), 7451: (119, 159), 7452: (145, 151), 7464: (118, 157), 7468: (120, 154), 7471: (122, 153), 7473: (107, 160), 7478: (111, 114), 7487: (104, 165), 7490: (122, 134), 7495: (104, 165), 7498: (120, 162), 7517: (122, 154), 7519: (112, 159), 7520: (104, 165), 7521: (145, 151), 7527: (112, 136), 7528: (110, 135), 7533: (97, 115), 7534: (151, 148), 7537: (104, 163), 7547: (112, 110), 7548: (118, 98), 7550: (103, 165), 7553: (146, 150), 7554: (118, 96), 7556: (110, 106), 7557: (148, 152), 7558: (95, 114), 7560: (104, 164), 7562: (104, 164), 7566: (119, 155), 7568: (112, 126), 7569: (141, 152), 7579: (89, 168), 7580: (114, 154), 7581: (147, 149), 7585: (96, 111), 7593: (103, 165), 7595: (104, 165), 7603: (149, 151), 7608: (110, 160), 7633: (108, 111), 7634: (112, 110), 7635: (90, 110), 7636: (120, 93), 7640: (120, 156), 7642: (126, 160), 7644: (98, 160), 7648: (95, 113), 7651: (109, 159), 7660: (84, 97), 7661: (92, 166), 7662: (127, 122), 7664: (97, 166), 7670: (126, 124), 7674: (94, 114), 7693: (84, 96), 7694: (122, 121), 7700: (92, 112), 7702: (137, 150), 7719: (104, 124), 7720: (95, 120), 7721: (122, 133), 7723: (104, 125), 7748: (99, 159), 7756: (113, 135), 7757: (88, 169), 7758: (100, 120), 7759: (128, 151), 7762: (102, 111), 7763: (95, 162), 7766: (121, 130), 7767: (145, 151), 7776: (140, 132), 7830: (103, 164), 7832: (100, 112), 7833: (110, 121), 7834: (108, 95), 7836: (94, 111), 7837: (104, 124), 7843: (136, 132), 7844: (124, 131), 7845: (110, 120), 7865: (129, 160), 7885: (110, 120), 7901: (110, 120), 7902: (125, 121), 7904: (94, 109), 7905: (137, 129), 7906: (99, 166), 7907: (109, 136), 7910: (103, 164), 7927: (118, 130), 7928: (115, 158), 7930: (98, 112), 7932: (128, 157), 8066: (132, 135), 8090: (99, 111), 8099: (138, 150), 8137: (125, 119), 8175: (134, 128), 8215: (106, 158), 8229: (112, 133), 8253: (98, 165), 8266: (107, 152), 8267: (115, 154), 8281: (87, 167), 8411: (118, 134), 8419: (95, 111), 8420: (99, 112), 8453: (104, 165), 8464: (114, 98), 8505: (93, 114), 8551: (104, 119), 8558: (114, 109), 8587: (112, 110), 8591: (102, 111), 8612: (99, 111), 8708: (107, 94), 8713: (125, 120), 8801: (99, 149), 8914: (125, 120), 9069: (107, 104), 9070: (123, 126), 9153: (116, 157), 9171: (125, 120), 9173: (124, 122), 9191: (99, 149), 9350: (120, 124), 9366: (125, 124), 9367: (117, 97), 9368: (96, 111), 9369: (107, 153), 9478: (125, 121), 9479: (124, 121), 9480: (124, 122), 9530: (126, 132), 9531: (107, 153), 9569: (98, 107), 9648: (107, 155), 9763: (126, 137), 9764: (126, 120), 9765: (125, 120), 9815: (110, 94), 9816: (109, 141), 9866: (127, 139), 9867: (120, 132), 9870: (120, 144), 9871: (125, 135), 9902: (128, 156), 9903: (107, 154), 9960: (99, 114), 9989: (124, 120), 10003: (111, 136), 10005: (92, 113), 10048: (92, 109), 10063: (107, 155), 10064: (115, 120), 10082: (94, 114), 10086: (115, 98), 10150: (106, 154), 10168: (101, 137), 10169: (99, 111), 10190: (103, 114), 10206: (118, 136), 10222: (101, 153), 10254: (109, 113), 10280: (93, 113), 10361: (97, 112), 10366: (103, 137), 10367: (118, 134), 10425: (118, 133), 10426: (110, 136), 10438: (126, 124), 10440: (124, 121), 10497: (107, 152), 10509: (120, 124), 10511: (94, 115), 10512: (91, 110), 10523: (125, 120), 10528: (126, 124), 10532: (115, 97), 10533: (107, 154), 10548: (126, 124), 10585: (109, 152), 10589: (101, 108), 10595: (107, 154), 10620: (95, 116), 10630: (100, 149), 10631: (110, 149), 10648: (105, 106), 10649: (98, 110), 10692: (114, 131), 10693: (124, 121), 10702: (98, 111), 10704: (119, 96), 10791: (113, 138), 10792: (128, 125), 10793: (105, 92), 10794: (104, 123), 10835: (117, 98), 10837: (125, 120), 10838: (98, 112), 10840: (106, 93), 10841: (109, 137), 10842: (109, 153), 10843: (96, 152), 10844: (116, 134), 10890: (102, 152), 10929: (99, 153), 10973: (92, 113), 10975: (93, 110), 10976: (118, 99), 11040: (120, 96), 11042: (114, 109), 11166: (113, 93), 11174: (92, 110), 11175: (107, 94), 11176: (103, 93), 11287: (91, 117), 11401: (95, 113), 11438: (104, 124), 11441: (104, 109), 11465: (109, 152), 11466: (109, 93), 11494: (96, 114), 11522: (110, 107), 11558: (100, 112), 11573: (118, 98), 11591: (109, 152), 11628: (118, 151), 11630: (113, 102), 11656: (104, 154), 11657: (118, 133), 11685: (119, 99), 11687: (118, 98), 11811: (105, 153), 11865: (124, 119), 11895: (92, 110), 11987: (95, 113), 12013: (113, 149), 12029: (109, 106), 12035: (103, 110), 12091: (121, 94), 12092: (108, 105), 12314: (99, 111), 12368: (114, 152), 12371: (118, 98), 12451: (116, 96), 12487: (126, 124), 12495: (110, 110), 12642: (97, 149), 12646: (117, 132), 12647: (115, 138), 12761: (104, 112), 12764: (104, 154), 12772: (124, 120), 12826: (101, 114), 12847: (116, 134), 12860: (116, 148), 12886: (125, 120), 12887: (113, 115), 12917: (109, 135), 12918: (92, 113), 12919: (111, 114), 12920: (93, 113), 13012: (117, 101), 13052: (124, 118), 13057: (102, 113), 13308: (112, 120), 13309: (95, 114), 13310: (115, 114), 13311: (95, 111), 13388: (131, 155), 13389: (124, 152), 13390: (135, 150), 13498: (118, 98), 13499: (123, 118), 13570: (97, 115), 13607: (102, 113), 13641: (126, 124), 13670: (107, 93), 13767: (112, 132), 13768: (112, 134), 13769: (101, 151), 13773: (124, 135), 13811: (115, 95), 13817: (112, 136), 13840: (94, 114), 14316: (106, 93), 14394: (100, 111), 14452: (105, 153), 14453: (107, 153), 14467: (99, 111), 14531: (96, 110), 14577: (110, 153), 14649: (99, 112), 14650: (116, 97), 14657: (100, 111), 14672: (125, 121), 14673: (125, 123), 14674: (127, 124), 14757: (116, 148), 14859: (109, 108), 14861: (98, 110), 14923: (109, 135), 14924: (109, 131), 15051: (121, 94), 15091: (108, 104), 15359: (125, 120), 15603: (97, 138), 15721: (97, 115), 15765: (90, 152), 15830: (109, 107), 15846: (112, 126), 15850: (107, 128), 15851: (108, 128), 15852: (126, 120), 16214: (99, 153), 16216: (99, 111), 16267: (98, 153), 16298: (106, 108), 16364: (118, 131), 16474: (110, 137), 16480: (108, 136), 16483: (117, 132), 16570: (98, 112), 16597: (107, 111), 16715: (112, 126), 16717: (116, 134), 16769: (109, 136), 16770: (110, 133), 16920: (106, 108), 16921: (108, 135), 17035: (106, 152), 17062: (107, 111), 17166: (112, 132), 17333: (94, 110), 17465: (91, 111), 17878: (108, 108), 18019: (131, 135), 18128: (121, 132), 18161: (115, 134), 18249: (110, 136), 18873: (109, 134), 18989: (117, 134), 18990: (124, 132), 22335: (119, 139), 23083: (110, 139), 26771: (112, 155), 26774: (127, 153), 26775: (139, 155), 26776: (126, 137), 26777: (139, 157), 26780: (111, 155), 26781: (116, 155), 26782: (121, 154), 26783: (113, 154), 26784: (116, 157), 26785: (132, 150), 26786: (122, 155), 26787: (103, 164), 26788: (111, 156), 26790: (128, 157), 26791: (111, 156), 26792: (118, 135), 26793: (111, 137), 26794: (116, 155), 26795: (131, 155), 26796: (118, 135), 26797: (117, 135), 26799: (127, 149), 26800: (132, 156), 26801: (135, 152), 26802: (141, 154), 26804: (121, 154), 26805: (145, 157), 26806: (114, 134), 26808: (114, 155), 26809: (120, 135), 26810: (135, 154), 26811: (115, 133), 26812: (114, 134), 26813: (116, 157), 26816: (145, 153), 26817: (126, 153), 26818: (113, 157), 26822: (113, 157), 26823: (110, 158), 26824: (110, 157), 26826: (121, 154), 26827: (124, 152), 26828: (109, 159), 26829: (112, 155), 26830: (108, 154), 26831: (117, 135), 26832: (110, 137), 26833: (110, 135), 26835: (105, 160), 26836: (111, 136), 26837: (118, 134), 26838: (105, 153), 26839: (111, 135), 26840: (116, 134), 26841: (132, 151), 26844: (116, 134), 26845: (133, 156), 26846: (133, 157), 26848: (124, 152), 26851: (120, 147), 26852: (128, 140), 26853: (121, 154), 26854: (135, 156), 26855: (110, 135), 26856: (116, 146), 26858: (135, 154), 26860: (121, 154), 26861: (152, 143), 26862: (135, 153), 26863: (131, 153), 26865: (133, 154), 26867: (118, 156), 26868: (136, 157), 26869: (117, 134), 26870: (117, 134), 26871: (134, 153), 26872: (122, 149), 26873: (113, 136), 26874: (110, 156), 26875: (110, 135), 26876: (134, 156), 26878: (127, 145), 26879: (118, 135), 26880: (134, 154), 26881: (136, 155), 26882: (134, 152), 26884: (136, 157), 26885: (134, 152), 26886: (121, 154), 26887: (133, 147), 26889: (110, 136), 26890: (131, 156), 26892: (121, 154), 26893: (121, 154), 26894: (120, 134), 26895: (135, 152), 26896: (116, 156), 26897: (121, 159), 26898: (125, 152), 26900: (118, 155), 26901: (118, 134), 26903: (114, 154), 26904: (119, 135), 26905: (111, 135), 26906: (110, 134), 26907: (108, 153), 26908: (110, 154), 26909: (111, 136), 26910: (111, 135), 26911: (138, 151), 26912: (112, 151), 26913: (116, 133), 26914: (108, 153), 26917: (102, 152), 26918: (114, 152), 26919: (112, 135), 26920: (111, 134), 26921: (118, 133), 26922: (114, 155), 26923: (113, 152), 26924: (136, 155), 26925: (119, 146), 26927: (110, 157), 26928: (137, 157), 26929: (133, 158), 26930: (118, 148), 26932: (117, 151), 26933: (131, 155), 26934: (122, 146), 26935: (110, 136), 26937: (120, 161), 26938: (129, 155), 26940: (143, 150), 26941: (114, 155), 26942: (115, 155), 26943: (122, 156), 26944: (147, 151), 26947: (135, 154), 26949: (118, 155), 26951: (117, 156), 26952: (116, 135), 26953: (136, 161), 26956: (106, 154), 26957: (111, 135), 26958: (122, 133), 26959: (132, 153), 26960: (116, 135), 26963: (113, 135), 26964: (126, 160), 26965: (126, 135), 26966: (112, 149), 26967: (137, 156), 26968: (113, 134), 26969: (113, 135), 26970: (115, 132), 26972: (140, 156), 26973: (132, 157), 26974: (134, 154), 26975: (134, 154), 26977: (137, 150), 26979: (121, 154), 26980: (137, 157), 26981: (115, 156), 26982: (120, 149), 26985: (110, 157), 26986: (139, 151), 26987: (136, 151), 26990: (122, 154), 26991: (111, 157), 26992: (99, 166), 26993: (128, 134), 26994: (148, 151), 26995: (114, 152), 26996: (134, 154), 26997: (151, 151), 26998: (129, 159), 26999: (136, 156), 27000: (136, 157), 27002: (110, 161), 27003: (153, 145), 27004: (122, 159), 27005: (138, 151), 27006: (133, 151), 27007: (128, 161), 27008: (137, 156), 27009: (121, 153), 27011: (110, 156), 27012: (121, 154), 27015: (104, 165), 27016: (121, 154), 27017: (120, 155), 27018: (118, 156), 27020: (136, 155), 27022: (119, 155), 27023: (121, 154), 27024: (118, 157), 27025: (108, 159), 27027: (98, 165), 27028: (135, 152), 27029: (110, 135), 27030: (114, 154), 27031: (110, 135), 27032: (118, 134), 27033: (117, 151), 27034: (112, 156), 27035: (140, 155), 27036: (124, 152), 27037: (132, 151), 27041: (134, 155), 27042: (133, 150), 27043: (119, 133), 27045: (114, 155), 27046: (138, 155), 27047: (136, 150), 27048: (135, 150), 27049: (140, 155), 27050: (114, 155), 27051: (148, 150), 27052: (136, 157), 27053: (132, 156), 27054: (132, 155), 27055: (117, 156), 27056: (138, 161), 27057: (135, 154), 27058: (133, 147), 27060: (118, 155), 27061: (110, 156), 27062: (112, 135), 27063: (115, 155), 27064: (134, 155), 27065: (110, 156), 27066: (111, 155), 27067: (115, 134)}}\n"
     ]
    }
   ],
   "source": [
    "print(site_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For generating the angle of solar incidence, you need to run up to the normalisation step to get the correct angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 0          52.44\n",
      "1          54.99\n",
      "2          51.81\n",
      "3          51.49\n",
      "4          51.61\n",
      "           ...  \n",
      "6039341    53.59\n",
      "6039342    53.55\n",
      "6039343    53.76\n",
      "6039344    53.10\n",
      "6039345    53.06\n",
      "Name: latitude_rounded, Length: 6039346, dtype: float64, 'longitude': 0         -0.12\n",
      "1         -3.18\n",
      "2         -2.50\n",
      "3          0.36\n",
      "4         -0.24\n",
      "           ... \n",
      "6039341   -3.04\n",
      "6039342   -2.23\n",
      "6039343   -1.52\n",
      "6039344   -2.04\n",
      "6039345   -2.98\n",
      "Name: longitude_rounded, Length: 6039346, dtype: float64}\n",
      "                           apparent_zenith      zenith  apparent_elevation  \\\n",
      "timestamp                                                                    \n",
      "2020-07-01 00:00:00+00:00       104.469109  104.469109          -14.469109   \n",
      "2020-07-01 00:00:00+00:00       101.844074  101.844074          -11.844074   \n",
      "2020-07-01 00:00:00+00:00       105.043198  105.043198          -15.043198   \n",
      "2020-07-01 00:00:00+00:00       105.423181  105.423181          -15.423181   \n",
      "2020-07-01 00:00:00+00:00       105.297536  105.297536          -15.297536   \n",
      "...                                    ...         ...                 ...   \n",
      "2020-07-31 23:55:00+00:00       108.283641  108.283641          -18.283641   \n",
      "2020-07-31 23:55:00+00:00       108.369427  108.369427          -18.369427   \n",
      "2020-07-31 23:55:00+00:00       108.194781  108.194781          -18.194781   \n",
      "2020-07-31 23:55:00+00:00       108.827588  108.827588          -18.827588   \n",
      "2020-07-31 23:55:00+00:00       108.814521  108.814521          -18.814521   \n",
      "\n",
      "                           elevation     azimuth  equation_of_time  \n",
      "timestamp                                                           \n",
      "2020-07-01 00:00:00+00:00 -14.469109  358.963037         -3.882701  \n",
      "2020-07-01 00:00:00+00:00 -11.844074  356.098345         -3.882701  \n",
      "2020-07-01 00:00:00+00:00 -15.043198  356.693428         -3.882701  \n",
      "2020-07-01 00:00:00+00:00 -15.423181  359.416459         -3.882701  \n",
      "2020-07-01 00:00:00+00:00 -15.297536  358.844604         -3.882701  \n",
      "...                              ...         ...               ...  \n",
      "2020-07-31 23:55:00+00:00 -18.283641  354.111254         -6.346891  \n",
      "2020-07-31 23:55:00+00:00 -18.369427  354.920280         -6.346891  \n",
      "2020-07-31 23:55:00+00:00 -18.194781  355.636396         -6.346891  \n",
      "2020-07-31 23:55:00+00:00 -18.827588  355.097534         -6.346891  \n",
      "2020-07-31 23:55:00+00:00 -18.814521  354.153158         -6.346891  \n",
      "\n",
      "[6039346 rows x 6 columns]\n",
      "              ss_id         power  latitude_rounded  longitude_rounded  \\\n",
      "count  6.039346e+06  6.039346e+06      6.039346e+06       6.039346e+06   \n",
      "mean   8.030236e+03  1.370859e-01      5.366749e+01      -2.471362e+00   \n",
      "std    3.642435e+03  1.986836e-01      2.194247e+00       1.559668e+00   \n",
      "min    2.607000e+03  0.000000e+00      5.004000e+01      -5.590000e+00   \n",
      "25%    6.441000e+03  0.000000e+00      5.158000e+01      -3.620000e+00   \n",
      "50%    7.314000e+03  3.958714e-02      5.358000e+01      -2.680000e+00   \n",
      "75%    1.008200e+04  2.041766e-01      5.568000e+01      -1.450000e+00   \n",
      "max    2.308300e+04  1.000000e+00      5.845000e+01       1.750000e+00   \n",
      "\n",
      "        orientation          tilt  solar_azimuth  solar_zenith  \n",
      "count  6.039346e+06  6.039346e+06   6.039346e+06  6.039346e+06  \n",
      "mean   1.835168e+02  3.400187e+01   1.800956e+02  7.132172e+01  \n",
      "std    4.915530e+01  6.961903e+00   1.103672e+02  2.512651e+01  \n",
      "min    0.000000e+00  4.000000e+00   3.109212e-04  2.699068e+01  \n",
      "25%    1.600000e+02  3.000000e+01   7.729593e+01  4.730235e+01  \n",
      "50%    1.800000e+02  3.500000e+01   1.802273e+02  7.320459e+01  \n",
      "75%    2.200000e+02  3.800000e+01   2.829485e+02  9.564447e+01  \n",
      "max    3.450000e+02  6.500000e+01   3.600000e+02  1.117663e+02  \n",
      "                        timestamp  ss_id  power  latitude_rounded  \\\n",
      "0       2020-07-01 00:00:00+00:00   2607    0.0             52.44   \n",
      "1       2020-07-01 00:00:00+00:00   2626    0.0             54.99   \n",
      "2       2020-07-01 00:00:00+00:00   2631    0.0             51.81   \n",
      "3       2020-07-01 00:00:00+00:00   2657    0.0             51.49   \n",
      "4       2020-07-01 00:00:00+00:00   2729    0.0             51.61   \n",
      "...                           ...    ...    ...               ...   \n",
      "6039341 2020-07-31 23:55:00+00:00  18873    0.0             53.59   \n",
      "6039342 2020-07-31 23:55:00+00:00  18989    0.0             53.55   \n",
      "6039343 2020-07-31 23:55:00+00:00  18990    0.0             53.76   \n",
      "6039344 2020-07-31 23:55:00+00:00  22335    0.0             53.10   \n",
      "6039345 2020-07-31 23:55:00+00:00  23083    0.0             53.06   \n",
      "\n",
      "         longitude_rounded  orientation  tilt  solar_azimuth  solar_zenith  \\\n",
      "0                    -0.12        200.0  35.0     358.963037    104.469109   \n",
      "1                    -3.18        270.0  22.0     356.098345    101.844074   \n",
      "2                    -2.50        130.0  30.0     356.693428    105.043198   \n",
      "3                     0.36        185.0  47.0     359.416459    105.423181   \n",
      "4                    -0.24        180.0  45.0     358.844604    105.297536   \n",
      "...                    ...          ...   ...            ...           ...   \n",
      "6039341              -3.04        290.0  35.0     354.111254    108.283641   \n",
      "6039342              -2.23        207.0  35.0     354.920280    108.369427   \n",
      "6039343              -1.52        180.0  34.0     355.636396    108.194781   \n",
      "6039344              -2.04        140.0  21.0     355.097534    108.827588   \n",
      "6039345              -2.98        120.0  31.0     354.153158    108.814521   \n",
      "\n",
      "         solar_azimuth_radians  solar_zenith_radians  panel_tilt_radians  \\\n",
      "0                     6.265087              1.823330            0.610865   \n",
      "1                     6.215089              1.777514            0.383972   \n",
      "2                     6.225475              1.833350            0.523599   \n",
      "3                     6.273001              1.839982            0.820305   \n",
      "4                     6.263020              1.837789            0.785398   \n",
      "...                        ...                   ...                 ...   \n",
      "6039341               6.180407              1.889906            0.610865   \n",
      "6039342               6.194527              1.891403            0.610865   \n",
      "6039343               6.207026              1.888355            0.593412   \n",
      "6039344               6.197621              1.899400            0.366519   \n",
      "6039345               6.181139              1.899172            0.541052   \n",
      "\n",
      "         panel_orientation_radians  \n",
      "0                         3.490659  \n",
      "1                         4.712389  \n",
      "2                         2.268928  \n",
      "3                         3.228859  \n",
      "4                         3.141593  \n",
      "...                            ...  \n",
      "6039341                   5.061455  \n",
      "6039342                   3.612832  \n",
      "6039343                   3.141593  \n",
      "6039344                   2.443461  \n",
      "6039345                   2.094395  \n",
      "\n",
      "[6039346 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#this block finds the long and lat for the sites where it is available \n",
    "pv_ss_id = pv.index.get_level_values(\"ss_id\").unique()\n",
    "pv_ss_time = pv.index.get_level_values(\"timestamp\")\n",
    "pv_meta = pd.read_csv(\"data/pv/metadata.csv\")\n",
    "pv_meta_ssid = pv_meta.ss_id\n",
    "shared_id = [x for x in pv_ss_id if x in pv_meta_ssid]\n",
    "pv_shared = pv_meta[pv_meta[\"ss_id\"].isin(shared_id)] #shared location data, only 698 of 908 sites in the pv file have long and lat for plotting\n",
    "\n",
    "#Merges \n",
    "pv_shared_merge = pv_shared[[\"ss_id\", \"latitude_rounded\", \"longitude_rounded\", \"orientation\", \"tilt\"]]\n",
    "pv_reset = pv.reset_index()\n",
    "\n",
    "pv_az = pd.merge(pv_reset, pv_shared_merge[['ss_id', 'latitude_rounded', 'longitude_rounded', \"orientation\", \"tilt\"]], on='ss_id', how='left').dropna().set_index([\"timestamp\", \"ss_id\"])\n",
    "pv_az_pre = pv_az.reset_index()\n",
    "\n",
    "#Calculates the azimuth\n",
    "location_data = {\n",
    "    'latitude': pv_az_pre['latitude_rounded'],  # extract all sites latitude and longitude into Loc data\n",
    "    'longitude': pv_az_pre['longitude_rounded'],\n",
    "}\n",
    "print(location_data)\n",
    "# Ensure UTC timezone for PVlib to work\n",
    "pv_az_pre['timestamp'] = pd.to_datetime(pv_az_pre['timestamp'], utc=True)#.dt.tz_convert(tz)\n",
    "location = pvlib.location.Location(location_data['latitude'], location_data['longitude']) #tz default UTC, altitude default =0\n",
    "solar_position = location.get_solarposition(pv_az_pre['timestamp']) #Based on location and time to calculate Solar Position\n",
    "print(solar_position)\n",
    "# Assign the azimuth values directly\n",
    "pv_az_pre['solar_azimuth'] = solar_position['azimuth'].values\n",
    "pv_az_pre['solar_zenith'] = solar_position['zenith'].values\n",
    "print(pv_az_pre.describe())\n",
    "\n",
    "########### Converting to radians for calculating the Solar Incidental angle ##############\n",
    "pv_az_pre['solar_azimuth_radians'] = np.radians(pv_az_pre['solar_azimuth'])\n",
    "pv_az_pre['solar_zenith_radians'] = np.radians(pv_az_pre['solar_zenith'])\n",
    "pv_az_pre[\"panel_tilt_radians\"] = np.radians(pv_az_pre['tilt'])\n",
    "pv_az_pre[\"panel_orientation_radians\"] = np.radians(pv_az_pre['orientation'])\n",
    "\n",
    "print(pv_az_pre)\n",
    "\n",
    "cos_theta_inc = (\n",
    "    np.cos(pv_az_pre['solar_zenith_radians']) * np.cos(pv_az_pre['panel_tilt_radians']))+(np.sin(pv_az_pre['solar_zenith_radians']) * np.sin(pv_az_pre['panel_tilt_radians']) * np.cos((pv_az_pre['solar_azimuth_radians']-pv_az_pre[\"panel_orientation_radians\"])))\n",
    "pv_az_pre['angle_of_incidence_radians'] = np.arccos(cos_theta_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_inc = pv_az_pre.drop(columns={'latitude_rounded', 'longitude_rounded',\n",
    "       'orientation', 'tilt', 'solar_azimuth',\n",
    "       'solar_azimuth_radians', 'solar_zenith', 'solar_zenith_radians',\n",
    "       'panel_tilt_radians',\n",
    "       'panel_orientation_radians'}).sort_values([ \"timestamp\"]).set_index([\"timestamp\", \"ss_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_inc_chk = pv_inc.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting final outputs for the pv_inc and formatting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pv_inc.reset_index()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "pv_inc = df.set_index([\"timestamp\", \"ss_id\"])\n",
    "\n",
    "pv_inc.index = pv_inc.index.set_levels([pv_inc.index.levels[0].tz_localize(None), pv_inc.index.levels[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomralising power and angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_norm = pv_inc\n",
    "\n",
    "power_min = pv_norm['power'].min()\n",
    "power_range = pv_norm['power'].max() - power_min\n",
    "pv_norm['power_normalized'] = (pv_norm['power'] - power_min) / power_range\n",
    "\n",
    "# Normalize the 'angle_of_incidence_radians' column\n",
    "angle_min = pv_norm['angle_of_incidence_radians'].min()\n",
    "angle_range = pv_norm['angle_of_incidence_radians'].max() - angle_min\n",
    "pv_norm['angle_of_incidence_radians_normalized'] = (pv_norm['angle_of_incidence_radians'] - angle_min) / angle_range\n",
    "\n",
    "pv_inc =pv_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['power', 'angle_of_incidence_radians', 'power_normalized',\n",
      "       'angle_of_incidence_radians_normalized'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pv_inc.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           power  angle_of_incidence_radians  \\\n",
      "timestamp           ss_id                                      \n",
      "2020-07-01 00:00:00 2607     0.0                    2.378988   \n",
      "                    7905     0.0                    1.363505   \n",
      "                    7906     0.0                    2.206425   \n",
      "                    7907     0.0                    2.275762   \n",
      "                    7910     0.0                    2.273475   \n",
      "...                          ...                         ...   \n",
      "2020-07-31 23:55:00 6781     0.0                    2.296199   \n",
      "                    6785     0.0                    2.450600   \n",
      "                    6786     0.0                    2.370474   \n",
      "                    6726     0.0                    2.194937   \n",
      "                    23083    0.0                    2.167528   \n",
      "\n",
      "                           power_normalized  \\\n",
      "timestamp           ss_id                     \n",
      "2020-07-01 00:00:00 2607                0.0   \n",
      "                    7905                0.0   \n",
      "                    7906                0.0   \n",
      "                    7907                0.0   \n",
      "                    7910                0.0   \n",
      "...                                     ...   \n",
      "2020-07-31 23:55:00 6781                0.0   \n",
      "                    6785                0.0   \n",
      "                    6786                0.0   \n",
      "                    6726                0.0   \n",
      "                    23083               0.0   \n",
      "\n",
      "                           angle_of_incidence_radians_normalized  \n",
      "timestamp           ss_id                                         \n",
      "2020-07-01 00:00:00 2607                                0.778431  \n",
      "                    7905                                0.446107  \n",
      "                    7906                                0.721959  \n",
      "                    7907                                0.744650  \n",
      "                    7910                                0.743901  \n",
      "...                                                          ...  \n",
      "2020-07-31 23:55:00 6781                                0.751338  \n",
      "                    6785                                0.801867  \n",
      "                    6786                                0.775645  \n",
      "                    6726                                0.718199  \n",
      "                    23083                               0.709230  \n",
      "\n",
      "[6039346 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#pv_inc = pv_inc.set_index([\"timestamp\", \"ss_id\"])\n",
    "print(pv_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For a deep learning model like a ResNet, standardization is commonly used because it generally makes training less sensitive to the scale of features and their values. Neural networks often perform better with standardized inputs, especially in the deeper layers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function extracts the area around each individual site using the PV dicts pixel based (as in the location of sites are determined by their pixel in the image) location and then extracts an area around each site. These areas are combined, based on their timestamp,\n",
    "#with the HRV data that then has its satellite imagery data extracted. This implies that the model is using subsets of the satellite imagery to train the model to make predictions for each site rather than using the whole image and then \"learning\" where the sites are.\n",
    "\n",
    "class ChallengeDataset_inc(IterableDataset):#This function sets up the data so that it can be iterated through by the CNN\n",
    "    def __init__(self, pv_inc, hrv, site_locations, start_date = \"2020-7-1\", end_date = \"2020-7-30\", sites=None):#The \"self\" augmentation here is used to use create a shared class between the different data types that are then iterable based on their shared timestamp\n",
    "        self.pv_inc = pv_inc\n",
    "        self.hrv = hrv\n",
    "        self._site_locations = site_locations\n",
    "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())#This gets the individual site ids which are stored as the dict's keys\n",
    "        self.start_date = list(map(int, start_date.split(\"-\")))\n",
    "        self.end_date= list(map(int, end_date.split(\"-\")))\n",
    "\n",
    "    def _get_image_times(self):#This function starts at the minimum date in the set and iterates up to the highest date, this is done as the data set is large and due to the nature of the parquette and xarray\n",
    "        min_date = datetime(self.start_date[0], self.start_date[1], self.start_date[2])\n",
    "        max_date = datetime(self.end_date[0], self.end_date[1], self.end_date[2])\n",
    "        #max and min need to be changed if we use more than one month of data\n",
    "        start_time = time(8)\n",
    "        end_time = time(17)\n",
    "\n",
    "        date = min_date#starts at the first timestamp\n",
    "        while date <= max_date: #iterates through up to the max\n",
    "            current_time = datetime.combine(date, start_time)\n",
    "            while current_time.time() < end_time:\n",
    "                if current_time:\n",
    "                    yield current_time\n",
    "\n",
    "                current_time += timedelta(minutes=60)\n",
    "\n",
    "            date += timedelta(days=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for time in self._get_image_times():\n",
    "            time_ids = pd.date_range(start=time + timedelta(hours=1),\n",
    "                                     end=time + timedelta(hours=5) - timedelta(minutes=5),\n",
    "                                     freq='5min')\n",
    "            time_ids = time_ids.strftime('%Y-%m-%dT%H:%M:%S').tolist()\n",
    "            #time_ids = time\n",
    "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))#gets the time and then uses this to select the corresponding time from the pv set  \n",
    "            #print(\"Available columns in pv_inc:\", self.pv_inc.columns)\n",
    "           \n",
    "            pv_features = self.pv_inc.xs(first_hour, drop_level=False)[[\"power\", \"angle_of_incidence_radians\"]]\n",
    "\n",
    "        # Fetching PV targets for the future time span\n",
    "            pv_targets = self.pv_inc.xs(\n",
    "                slice(\n",
    "                    str(time + timedelta(hours=1)),  # Start 1 hour after the first_hour\n",
    "                    str(time + timedelta(hours=4, minutes=55)),  # Up to almost 5 hours later\n",
    "                ),\n",
    "                drop_level=False,\n",
    "            )[\"power\"]\n",
    "            #print(\"First hour slice:\", first_hour)\n",
    "            #print(\"Sample data from pv_inc:\", self.pv_inc.xs(first_hour, drop_level=False).head())\n",
    "            #gets the hrv satellite image that is associated with the first hour timestamp setting it up as an input feature\n",
    "            hrv_data = self.hrv['data'].sel(time=first_hour).to_numpy()\n",
    "\n",
    "            for site in self._sites:\n",
    "                site_id = site\n",
    "\n",
    "                try:\n",
    "                    #print(site)\n",
    "                    # Get solar PV features and targets, the site_targets is used to find the models loss\n",
    "                    site_features = pv_features.xs(site, level=1).to_numpy()#.squeeze(-1)#gets the pixel based location of the pv site and then uses this to make predictions based on the individual sites\n",
    "                    \n",
    "                    site_targets = pv_targets.xs(site, level=1).to_numpy()#.squeeze(-1)\n",
    "                    assert site_features.shape == (12,2) and site_targets.shape == (48,)#compresses the data from N dimensions to 12 and 48 respectively\n",
    "                  \n",
    "                    # Get a 128x128 HRV crop centred on the site over the previous hour\n",
    "                    x, y = self._site_locations[\"hrv\"][site]#gets the location of the site based on the pv sites pixel level location\n",
    "                    hrv_features = hrv_data[:, y - 3 : y + 3, x - 3 : x + 3, 0]\n",
    "                    assert hrv_features.shape == (12, 6, 6)#crops the image to be be 128x128 around the site\n",
    "                    #asset is used to force the dimensions of the extracted site level image to be the same\n",
    "                    # How might you adapt this for the non-HRV, weather and aerosol data?\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                yield  time_ids, site_id, site_features, hrv_features, site_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #Change this to change the number of layers that you are using, \n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    #This section creates a sequence of layers that perform the networks convolution which are applied iteratively in the Resnet_light block\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding), #Feature extraction\n",
    "        nn.BatchNorm2d(out_channels), #Noramlises the outputs from the convolution layers\n",
    "        nn.ReLU(inplace=True)#Applies the activation function\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1 \n",
    "    #Applies the convolution established in the previous layer twice giving the F(x) portion of the resnet model\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x): #Keeps the x portion of the resnet \n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None: #Downsamples the model if needed to match the dimensions of outputs if the identity output does not match the F(x) portion of the output\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity #Creates the F(x) + x that is then passed to the relu activation function between the resnet conv layers\n",
    "        return F.relu(out, inplace=False)  #Relu applied to combined results, \n",
    "\n",
    "class ResNet_light_inc(nn.Module):\n",
    "    #This class stacks the multiple basic blocks set up in the previous functions\n",
    "    def __init__(self, block, layers):\n",
    "        #I Think we can reduce the number of layers here as the model is applied four convolutions to generate F(x), the resnet paper uses two.\n",
    "        super(ResNet_light_inc, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = conv_block(12, 12, kernel_size=2, stride=1, padding=3)#Applies the initial convolution \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)#Runs maxpool convolution\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))#Forces the consistency of output sizes to be 1x1 \n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 48)  #takes the flatterened output of the conv layers for the 12 hourly time instances and then hands them to 48 different class outputs\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):#Used to stack the multiple layers of the resnet model\n",
    "        downsample = None#This checks to make sure that the stride applied matches between input tensor and the output tensor, I am not completely sure if this changes the dimensions of the output tensor\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:#Expands the number of outputs compared to the inputs, for the BasicBlock typically no expansion is needed. This is still needed for the model to run. \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]#This section creates a series of blocks for the layer\n",
    "        self.in_channels = out_channels * block.expansion #Ensures that after the blocks have been defined the next layer gets the correct number of input channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))#\n",
    "        return nn.Sequential(*layers)#Stretches the dims of the resnet to match the layers defined above\n",
    "        #Need to clarify exactly what expansion is doing.\n",
    "    def forward(self, pv_inc, hrv ):#Defines how the model passes the outputs through the network\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)#Passes the HRV data through the initial block defined earlier\n",
    "        x = self.maxpool(x)#Downsamples using maxpooling\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)#Applies the layers defined above, \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Shape after avgpool:\", x.shape)\n",
    "        x = torch.flatten(x, 1)# Applies the flattering to the second dimension as the first dimension is the batch size\n",
    "        pv_inc = torch.flatten(pv_inc, start_dim=1)#take in at the fully connected layer\n",
    "        #print(f\"Sshape of x = {x.shape} shape of pv = {pv.shape}\")\n",
    "        #x = torch.concat((x, pv), dim=-1)\n",
    "        #print(\"Shape after avgpool and flatten:\", x.shape)\n",
    "\n",
    "        #Take in the pc_inc and the pv power as seperate features and run the model again\n",
    "        \n",
    "        #pv = pv.view(pv.size(0), -1)\n",
    "        #Checks to make sure that the pv tensor dimensions match the HRV tensor dimensions\n",
    "        if pv_inc.dim() > 2:\n",
    "            pv_inc = torch.flatten(pv_inc, start_dim=1)\n",
    "        #print(\"Adjusted PV shape:\", pv.shape)\n",
    "\n",
    "        combined = torch.cat((x, pv_inc), dim=1)#Combines the pv and hrv data along the feature dimension\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 48).to(combined.device)\n",
    "            #Above runs a check to make sure that the number of input features is correct\n",
    "        out = self.fc(combined) #takes the combined output of the pv and hrv and passes them to the fully connected layer defined above\n",
    "        return out\n",
    "model_light_res_inc = ResNet_light_inc(BasicBlock, layers).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop1(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop1, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 48)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Shape after avgpool:\", x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(f\"Sshape of x = {x.shape} shape of pv = {pv.shape}\")\n",
    "        #x = torch.concat((x, pv), dim=-1)\n",
    "        #print(\"Shape after avgpool and flatten:\", x.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        #pv = pv.view(pv.size(0), -1)\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(\"Adjusted PV shape:\", pv.shape)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 48).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "model_light_deep_res_crop1 = ResNet_light_deep_crop1(BasicBlock, layers).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 x 1, 3 x 3, 6 x 6 one hour resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop1_60m(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop1_60m, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Shape after avgpool:\", x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(f\"Sshape of x = {x.shape} shape of pv = {pv.shape}\")\n",
    "        #x = torch.concat((x, pv), dim=-1)\n",
    "        #print(\"Shape after avgpool and flatten:\", x.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        #pv = pv.view(pv.size(0), -1)\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(\"Adjusted PV shape:\", pv.shape)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "model_light_deep_res_crop1_60m = ResNet_light_deep_crop1_60m(BasicBlock, layers).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 x 16 one hour under development, parameters need tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet_light_deep_crop16_60m(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet_light_deep_crop16_60m, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Shape after avgpool:\", x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(f\"Sshape of x = {x.shape} shape of pv = {pv.shape}\")\n",
    "        #x = torch.concat((x, pv), dim=-1)\n",
    "        #print(\"Shape after avgpool and flatten:\", x.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        #pv = pv.view(pv.size(0), -1)\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "        #print(\"Adjusted PV shape:\", pv.shape)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "model_light_deep_res_crop16_60m = ResNet_light_deep_crop16_60m(BasicBlock, layers).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_light_deep_res_crop1 = ResNet_light_deep_crop1(BasicBlock, layers)\n",
    "print(model_light_deep_res_crop1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce51VGYWXoLX"
   },
   "source": [
    "## Training models\n",
    "This generates weights for the model that we can then use for validation. The weights are then saved as the model submission meaning that each time we generate weights we can then save the weights along with the associated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidence check\n",
    "One hour for small CNN\n",
    "Criterion measures (faster convergence, faster training)\n",
    "1x1, 6x6, 128\n",
    "1 hour incidence, set up for Ovais\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE criterion and validation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "def model_validation(model, criterion, validation_dataloader):\n",
    "    model.eval() # This is used to set the model to evaluation mode\n",
    "    with torch.no_grad(): # This is used to stop the model from storing gradients\n",
    "        losses = []\n",
    "        for pv_features, hrv_features, pv_targets in validation_dataloader:\n",
    "            pv_features, hrv_features, pv_targets = pv_features.to(device, dtype=torch.float), hrv_features.to(device, dtype=torch.float), pv_targets.to(device, dtype=torch.float)\n",
    "            predictions = model(pv_features, hrv_features)\n",
    "            loss = criterion(predictions, pv_targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    model.train() # This is used to set the model back to training mode\n",
    "    \n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def model_validation_indv(model, criterion, validation_dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    individual_losses = []  # List to store each individual loss\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for pv_features, hrv_features, pv_targets in validation_dataloader:\n",
    "            pv_features, hrv_features, pv_targets = pv_features.to(device, dtype=torch.float), hrv_features.to(device, dtype=torch.float), pv_targets.to(device, dtype=torch.float)\n",
    "\n",
    "            predictions = model(pv_features, hrv_features)  # Get model predictions\n",
    "            \n",
    "            # Calculate loss for each individual in the batch\n",
    "            individual_batch_losses = criterion(predictions, pv_targets, reduction='none')  # This should return a tensor of losses for each item in the batch\n",
    "            \n",
    "            individual_losses.extend(individual_batch_losses.tolist())  # Convert tensor to list and append to the list of losses\n",
    "            \n",
    "    model.train()  # Set the model back to training mode\n",
    "    return individual_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 x 1 training for one hour change the dataloader and the model loaded as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 x 1 \n",
    "BATCH_SIZE = 32#This controls the number of sites that predictions are made for I think\n",
    "#6x6 for 4 hours prediction: #ChallengeDataset_inc #model_light_deep_res_crop1 model_light_res_inc\n",
    "#the number of sites per batch\n",
    "\n",
    "#load the data based on the previously defined functions above, the above functions can be altered to change how the data is ingested\n",
    "train_dataset = ChallengeDataset_inc(pv_inc, hrv, site_locations=site_locations,\n",
    "                                 start_date=\"2020-7-1\", end_date=\"2020-7-02\")  # controls which data is loaded in\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "validation_dataset = ChallengeDataset_inc(pv_inc, hrv, site_locations=site_locations,\n",
    "                                      start_date=\"2020-7-31\", end_date=\"2020-7-31\") \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=True)  # controls which data is loaded in\n",
    "model = model_light_res_inc\n",
    "criterion = RMSELoss()#nn.L1Loss()#Here we are defining the test stat as MAE\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation loop. Includes unpacking for id and time to get the batch dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 200: Training Loss: 0.3548417825251818\n",
      "     Validation Loss: 0.7532454559875307\n",
      "\n",
      "Epoch 1 Completed: Training Loss: 0.28961656534970365\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_validation_losses = []\n",
    "timestamps = []\n",
    "site_ids = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Training loop\n",
    "    for i, (site_id, time_id, pv_features, hrv_features, pv_targets) in enumerate(train_dataloader):\n",
    "        if epoch == 0:\n",
    "            timestamps.append(time_id)\n",
    "           \n",
    "\n",
    "        site_ids.append(site_id)\n",
    "        optimiser.zero_grad()\n",
    "        pv_features, hrv_features, pv_targets = \\\n",
    "            pv_features.to(device, dtype=torch.float), \\\n",
    "            hrv_features.to(device, dtype=torch.float), \\\n",
    "            pv_targets.to(device, dtype=torch.float)\n",
    "\n",
    "        predictions = model(pv_features, hrv_features)\n",
    "        loss = criterion(predictions, pv_targets)\n",
    "        loss.backward()\n",
    "        #print(f\"{loss}\")\n",
    "        optimiser.step()\n",
    "        \n",
    "        size = pv_targets.size(0)\n",
    "        running_loss += float(loss) * size\n",
    "        count += size\n",
    "        #print(f\"{count}\")\n",
    "        #print(f\"{running_loss}\")\n",
    "        #print(f\"{size}\")\n",
    "        if i % 200 == 199:\n",
    "            current_loss = running_loss / count\n",
    "            training_losses.append(current_loss)\n",
    "            #print(f\"training loss: {training_losses} current loss: {current_loss}\")\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}: Training Loss: {current_loss}\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            validation_loss = 0\n",
    "            val_count = 0\n",
    "            for val_site, val_time, val_pv_features, val_hrv_features, val_pv_targets in validation_dataloader:\n",
    "                val_pv_features, val_hrv_features, val_pv_targets = \\\n",
    "                    val_pv_features.to(device, dtype=torch.float), \\\n",
    "                    val_hrv_features.to(device, dtype=torch.float), \\\n",
    "                    val_pv_targets.to(device, dtype=torch.float)\n",
    "                val_predictions = model(val_pv_features, val_hrv_features)\n",
    "                val_loss = criterion(val_predictions, val_pv_targets)\n",
    "\n",
    "                val_size = val_pv_targets.size(0)\n",
    "                validation_loss += float(val_loss) * val_size\n",
    "                val_count += val_size\n",
    "\n",
    "            validation_loss /= val_count\n",
    "            validation_losses.append(validation_loss)\n",
    "            print(f\"     Validation Loss: {validation_loss}\\n\")\n",
    "\n",
    "    epoch_train_loss = running_loss / count\n",
    "    epoch_train_losses.append(epoch_train_loss)  \n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Completed: Training Loss: {epoch_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1, 3800: 0.15208584108733034 for fourty layers deep resnet 1x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original training loop, no validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date      time         0         1         2         3         4  \\\n",
      "0     2020-07-31  09:00:00  0.207626  0.063968  0.098740  0.178361  0.193836   \n",
      "1     2020-07-31  09:00:00  0.187472  0.060985  0.049691  0.183546  0.197033   \n",
      "2     2020-07-31  09:00:00  0.280266  0.216783  0.176330  0.173897  0.132876   \n",
      "3     2020-07-31  09:00:00  0.219739  0.068374  0.110114  0.174961  0.194873   \n",
      "4     2020-07-31  09:00:00  0.302651  0.300680  0.197734  0.179709  0.094973   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "5832  2020-07-31  17:00:00  0.285369  0.262564  0.165901  0.172984  0.113175   \n",
      "5833  2020-07-31  17:00:00  0.262466  0.230401  0.153412  0.170675  0.109556   \n",
      "5834  2020-07-31  17:00:00  0.227072  0.117302  0.116183  0.164554  0.158304   \n",
      "5835  2020-07-31  17:00:00  0.189948  0.114223  0.088413  0.173214  0.143572   \n",
      "5836  2020-07-31  17:00:00  0.176337  0.083704  0.050183  0.171725  0.174519   \n",
      "\n",
      "             5         6         7  ...  target_38     target_39  \\\n",
      "0     0.226111  0.245629  0.194199  ...   0.712396  7.124694e-01   \n",
      "1     0.270547  0.326439  0.208495  ...   0.560133  4.005767e-01   \n",
      "2     0.088835  0.067457  0.142536  ...   0.508743  5.137950e-01   \n",
      "3     0.220233  0.222204  0.185665  ...   0.720942  7.256490e-01   \n",
      "4     0.034213  0.006079  0.124994  ...   0.519444  5.267670e-01   \n",
      "...        ...       ...       ...  ...        ...           ...   \n",
      "5832  0.082511  0.075234  0.122514  ...   0.000566  5.198940e-06   \n",
      "5833  0.117473  0.137313  0.137759  ...   0.000000  0.000000e+00   \n",
      "5834  0.190531  0.227558  0.149248  ...   0.000000  0.000000e+00   \n",
      "5835  0.217245  0.291261  0.181109  ...   0.000000  0.000000e+00   \n",
      "5836  0.273049  0.358797  0.196803  ...   0.000000  7.513968e-10   \n",
      "\n",
      "         target_40  target_41  target_42  target_43  target_44  target_45  \\\n",
      "0     7.142531e-01   0.714176   0.713449   0.713535   0.710110   0.710600   \n",
      "1     3.677333e-01   0.364587   0.552543   0.670490   0.366627   0.413677   \n",
      "2     5.127630e-01   0.511155   0.512907   0.515502   0.514665   0.515163   \n",
      "3     7.243350e-01   0.718437   0.716337   0.715839   0.711489   0.710793   \n",
      "4     5.327490e-01   0.537933   0.544143   0.546270   0.551580   0.559188   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "5832  1.027116e-07   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "5833  0.000000e+00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "5834  0.000000e+00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "5835  0.000000e+00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "5836  1.272792e-11   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "      target_46  target_47  \n",
      "0      0.707498   0.703086  \n",
      "1      0.480950   0.731530  \n",
      "2      0.520326   0.519882  \n",
      "3      0.712563   0.713277  \n",
      "4      0.564801   0.567999  \n",
      "...         ...        ...  \n",
      "5832   0.000000   0.000000  \n",
      "5833   0.000000   0.000000  \n",
      "5834   0.000000   0.000000  \n",
      "5835   0.000000   0.000000  \n",
      "5836   0.000000   0.000000  \n",
      "\n",
      "[5837 rows x 98 columns]\n"
     ]
    }
   ],
   "source": [
    "def _eval_visual(dataloader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    predictions_list = []\n",
    "    timestamps_list = []\n",
    "    pv_targets_list = []  # List to store pv_targets for each batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(dataloader):\n",
    "            hrv_features = hrv_features.to(device, dtype=torch.float)\n",
    "            pv_features = pv_features.to(device, dtype=torch.float)\n",
    "            pv_targets = pv_targets.to(device, dtype=torch.float)\n",
    "            \n",
    "            batch_predictions = model(pv_features, hrv_features)\n",
    "            batch_predictions = batch_predictions.cpu().numpy()\n",
    "            batch_pv_targets = pv_targets.cpu().numpy()  # Convert pv_targets to numpy array\n",
    "\n",
    "            # Timestamp processing as before\n",
    "            if isinstance(time_ids[0], tuple) or isinstance(time_ids[0], list):\n",
    "                single_timestamp = time_ids[0][0]\n",
    "            else:\n",
    "                single_timestamp = time_ids[0]\n",
    "            if isinstance(single_timestamp, datetime):\n",
    "                timestamp = single_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            else:\n",
    "                timestamp = str(single_timestamp)\n",
    "            \n",
    "            # Append each batch's data to the lists\n",
    "            predictions_list.append(batch_predictions)\n",
    "            pv_targets_list.append(batch_pv_targets)  # Append pv_targets to its list\n",
    "            batch_timestamps = [timestamp] * batch_predictions.shape[0]\n",
    "            timestamps_list.extend(batch_timestamps)\n",
    "\n",
    "    # Concatenate all collected arrays into single numpy arrays\n",
    "    predictions = np.concatenate(predictions_list, axis=0)\n",
    "    pv_targets = np.concatenate(pv_targets_list, axis=0)  # Concatenate all pv_targets\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    pv_targets_df = pd.DataFrame(pv_targets, columns=[f'target_{i}' for i in range(pv_targets.shape[1])])\n",
    "    timestamps_df = pd.DataFrame(timestamps_list, columns=['timestamp'])\n",
    "\n",
    "    # Combine timestamps, predictions, and targets by using index alignment\n",
    "    final_df = pd.concat([timestamps_df, predictions_df, pv_targets_df], axis=1)\n",
    "    # final_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "prediction_df = _eval_visual(validation_dataloader, model, device)\n",
    "prediction_df[['date', 'time']] = prediction_df['timestamp'].str.split('T', expand=True)\n",
    "timestamp_index = prediction_df.columns.get_loc('timestamp')\n",
    "prediction_df.insert(timestamp_index, 'date', prediction_df.pop('date'))\n",
    "prediction_df.insert(timestamp_index + 1, 'time', prediction_df.pop('time'))\n",
    "prediction_df.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.154493\n",
      "1       0.052626\n",
      "2       0.312657\n",
      "3       0.179147\n",
      "4       0.346952\n",
      "          ...   \n",
      "5832    0.283251\n",
      "5833    0.221292\n",
      "5834    0.163495\n",
      "5835    0.089232\n",
      "5836    0.019058\n",
      "Name: 11, Length: 5837, dtype: float32 0       0.577792\n",
      "1       0.636043\n",
      "2       0.379299\n",
      "3       0.619134\n",
      "4       0.198560\n",
      "          ...   \n",
      "5832    0.050025\n",
      "5833    0.030107\n",
      "5834    0.068036\n",
      "5835    0.036139\n",
      "5836    0.058710\n",
      "Name: target_11, Length: 5837, dtype: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>target_39</th>\n",
       "      <th>target_40</th>\n",
       "      <th>target_41</th>\n",
       "      <th>target_42</th>\n",
       "      <th>target_43</th>\n",
       "      <th>target_44</th>\n",
       "      <th>target_45</th>\n",
       "      <th>target_46</th>\n",
       "      <th>target_47</th>\n",
       "      <th>RMSE_1 hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>0.207626</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>0.098740</td>\n",
       "      <td>0.178361</td>\n",
       "      <td>0.193836</td>\n",
       "      <td>0.226111</td>\n",
       "      <td>0.245629</td>\n",
       "      <td>0.194199</td>\n",
       "      <td>...</td>\n",
       "      <td>7.124694e-01</td>\n",
       "      <td>7.142531e-01</td>\n",
       "      <td>0.714176</td>\n",
       "      <td>0.713449</td>\n",
       "      <td>0.713535</td>\n",
       "      <td>0.710110</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.707498</td>\n",
       "      <td>0.703086</td>\n",
       "      <td>0.423299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>0.187472</td>\n",
       "      <td>0.060985</td>\n",
       "      <td>0.049691</td>\n",
       "      <td>0.183546</td>\n",
       "      <td>0.197033</td>\n",
       "      <td>0.270547</td>\n",
       "      <td>0.326439</td>\n",
       "      <td>0.208495</td>\n",
       "      <td>...</td>\n",
       "      <td>4.005767e-01</td>\n",
       "      <td>3.677333e-01</td>\n",
       "      <td>0.364587</td>\n",
       "      <td>0.552543</td>\n",
       "      <td>0.670490</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>0.413677</td>\n",
       "      <td>0.480950</td>\n",
       "      <td>0.731530</td>\n",
       "      <td>0.583417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>0.280266</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.176330</td>\n",
       "      <td>0.173897</td>\n",
       "      <td>0.132876</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>0.067457</td>\n",
       "      <td>0.142536</td>\n",
       "      <td>...</td>\n",
       "      <td>5.137950e-01</td>\n",
       "      <td>5.127630e-01</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.512907</td>\n",
       "      <td>0.515502</td>\n",
       "      <td>0.514665</td>\n",
       "      <td>0.515163</td>\n",
       "      <td>0.520326</td>\n",
       "      <td>0.519882</td>\n",
       "      <td>0.066642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>0.219739</td>\n",
       "      <td>0.068374</td>\n",
       "      <td>0.110114</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>0.194873</td>\n",
       "      <td>0.220233</td>\n",
       "      <td>0.222204</td>\n",
       "      <td>0.185665</td>\n",
       "      <td>...</td>\n",
       "      <td>7.256490e-01</td>\n",
       "      <td>7.243350e-01</td>\n",
       "      <td>0.718437</td>\n",
       "      <td>0.716337</td>\n",
       "      <td>0.715839</td>\n",
       "      <td>0.711489</td>\n",
       "      <td>0.710793</td>\n",
       "      <td>0.712563</td>\n",
       "      <td>0.713277</td>\n",
       "      <td>0.439987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>0.302651</td>\n",
       "      <td>0.300680</td>\n",
       "      <td>0.197734</td>\n",
       "      <td>0.179709</td>\n",
       "      <td>0.094973</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.124994</td>\n",
       "      <td>...</td>\n",
       "      <td>5.267670e-01</td>\n",
       "      <td>5.327490e-01</td>\n",
       "      <td>0.537933</td>\n",
       "      <td>0.544143</td>\n",
       "      <td>0.546270</td>\n",
       "      <td>0.551580</td>\n",
       "      <td>0.559188</td>\n",
       "      <td>0.564801</td>\n",
       "      <td>0.567999</td>\n",
       "      <td>0.148392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0.285369</td>\n",
       "      <td>0.262564</td>\n",
       "      <td>0.165901</td>\n",
       "      <td>0.172984</td>\n",
       "      <td>0.113175</td>\n",
       "      <td>0.082511</td>\n",
       "      <td>0.075234</td>\n",
       "      <td>0.122514</td>\n",
       "      <td>...</td>\n",
       "      <td>5.198940e-06</td>\n",
       "      <td>1.027116e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0.262466</td>\n",
       "      <td>0.230401</td>\n",
       "      <td>0.153412</td>\n",
       "      <td>0.170675</td>\n",
       "      <td>0.109556</td>\n",
       "      <td>0.117473</td>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.137759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0.227072</td>\n",
       "      <td>0.117302</td>\n",
       "      <td>0.116183</td>\n",
       "      <td>0.164554</td>\n",
       "      <td>0.158304</td>\n",
       "      <td>0.190531</td>\n",
       "      <td>0.227558</td>\n",
       "      <td>0.149248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0.189948</td>\n",
       "      <td>0.114223</td>\n",
       "      <td>0.088413</td>\n",
       "      <td>0.173214</td>\n",
       "      <td>0.143572</td>\n",
       "      <td>0.217245</td>\n",
       "      <td>0.291261</td>\n",
       "      <td>0.181109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0.176337</td>\n",
       "      <td>0.083704</td>\n",
       "      <td>0.050183</td>\n",
       "      <td>0.171725</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>0.273049</td>\n",
       "      <td>0.358797</td>\n",
       "      <td>0.196803</td>\n",
       "      <td>...</td>\n",
       "      <td>7.513968e-10</td>\n",
       "      <td>1.272792e-11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows  99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      time         0         1         2         3         4  \\\n",
       "0     2020-07-31  09:00:00  0.207626  0.063968  0.098740  0.178361  0.193836   \n",
       "1     2020-07-31  09:00:00  0.187472  0.060985  0.049691  0.183546  0.197033   \n",
       "2     2020-07-31  09:00:00  0.280266  0.216783  0.176330  0.173897  0.132876   \n",
       "3     2020-07-31  09:00:00  0.219739  0.068374  0.110114  0.174961  0.194873   \n",
       "4     2020-07-31  09:00:00  0.302651  0.300680  0.197734  0.179709  0.094973   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "5832  2020-07-31  17:00:00  0.285369  0.262564  0.165901  0.172984  0.113175   \n",
       "5833  2020-07-31  17:00:00  0.262466  0.230401  0.153412  0.170675  0.109556   \n",
       "5834  2020-07-31  17:00:00  0.227072  0.117302  0.116183  0.164554  0.158304   \n",
       "5835  2020-07-31  17:00:00  0.189948  0.114223  0.088413  0.173214  0.143572   \n",
       "5836  2020-07-31  17:00:00  0.176337  0.083704  0.050183  0.171725  0.174519   \n",
       "\n",
       "             5         6         7  ...     target_39     target_40  \\\n",
       "0     0.226111  0.245629  0.194199  ...  7.124694e-01  7.142531e-01   \n",
       "1     0.270547  0.326439  0.208495  ...  4.005767e-01  3.677333e-01   \n",
       "2     0.088835  0.067457  0.142536  ...  5.137950e-01  5.127630e-01   \n",
       "3     0.220233  0.222204  0.185665  ...  7.256490e-01  7.243350e-01   \n",
       "4     0.034213  0.006079  0.124994  ...  5.267670e-01  5.327490e-01   \n",
       "...        ...       ...       ...  ...           ...           ...   \n",
       "5832  0.082511  0.075234  0.122514  ...  5.198940e-06  1.027116e-07   \n",
       "5833  0.117473  0.137313  0.137759  ...  0.000000e+00  0.000000e+00   \n",
       "5834  0.190531  0.227558  0.149248  ...  0.000000e+00  0.000000e+00   \n",
       "5835  0.217245  0.291261  0.181109  ...  0.000000e+00  0.000000e+00   \n",
       "5836  0.273049  0.358797  0.196803  ...  7.513968e-10  1.272792e-11   \n",
       "\n",
       "      target_41  target_42  target_43  target_44  target_45  target_46  \\\n",
       "0      0.714176   0.713449   0.713535   0.710110   0.710600   0.707498   \n",
       "1      0.364587   0.552543   0.670490   0.366627   0.413677   0.480950   \n",
       "2      0.511155   0.512907   0.515502   0.514665   0.515163   0.520326   \n",
       "3      0.718437   0.716337   0.715839   0.711489   0.710793   0.712563   \n",
       "4      0.537933   0.544143   0.546270   0.551580   0.559188   0.564801   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "5832   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5833   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5834   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5835   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5836   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "      target_47  RMSE_1 hr  \n",
       "0      0.703086   0.423299  \n",
       "1      0.731530   0.583417  \n",
       "2      0.519882   0.066642  \n",
       "3      0.713277   0.439987  \n",
       "4      0.567999   0.148392  \n",
       "...         ...        ...  \n",
       "5832   0.000000   0.233226  \n",
       "5833   0.000000   0.191185  \n",
       "5834   0.000000   0.095460  \n",
       "5835   0.000000   0.053093  \n",
       "5836   0.000000   0.039652  \n",
       "\n",
       "[5837 rows x 99 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_RMSE = prediction_df[11]\n",
    "targets_RMSE = prediction_df['target_11']\n",
    "mse = (predictions_RMSE - targets_RMSE) ** 2\n",
    "rmse = np.sqrt(mse)\n",
    "prediction_df['RMSE_1 hr'] = rmse\n",
    "\n",
    "print(prediction_df[11],prediction_df['target_11'])\n",
    "prediction_df\n",
    "\n",
    "#prediction_df.to_csv(\"RMSE Visualisation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"RMSE Visualisation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in validation dataloader: 5837\n"
     ]
    }
   ],
   "source": [
    "def count_samples_in_dataloader(dataloader):\n",
    "    total_samples = 0\n",
    "    for batch in dataloader:\n",
    "        time_ids, site_id, site_features, hrv_features, site_targets = batch\n",
    "        # Assuming site_features or hrv_features are tensors and we are interested in counting samples in them\n",
    "        # Adjust the indexing if the actual tensor with the batch size as the first dimension is in a different position\n",
    "        total_samples += site_features.shape[0]\n",
    "    return total_samples\n",
    "\n",
    "# Count samples in the validation dataloader\n",
    "num_samples = count_samples_in_dataloader(validation_dataloader)\n",
    "print(\"Total number of samples in validation dataloader:\", num_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
