{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import json\n",
    "from dataset import ChallengeDataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocf_blosc2 import Blosc2\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime, time, timedelta\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selects the gpu if available (when running on Google Colab) otherwise on the local cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block downloads the data from huggingface. This is only required if working on Google Colab OR data has not yet been downloaded locally. Expect this cell to run for up to 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download site locations (indices.json) and data (pv and satellite-hrv)\n",
    "\n",
    "# if not os.path.exists(\"submission\"):\n",
    "#      os.makedirs(\"submission\", exist_ok=True)\n",
    "#      #Installing locally means you do not need to rerun this each time you restart the notebook\n",
    "#      !curl -L https://raw.githubusercontent.com/climatehackai/getting-started-2023/main/indices.json --output indices.json\n",
    "\n",
    "# if not os.path.exists(\"data\"):\n",
    "#     os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/pv/2021\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "#     os.makedirs(\"data/satellite-hrv/2021\", exist_ok=True)\n",
    "\n",
    "#     !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "\n",
    "#      # Download data for June, July, August 2020 and 2021\n",
    "#     for summer_months in range (6,9):\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/{summer_months}.parquet --output data/pv/2020/{summer_months}.parquet\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2021/{summer_months}.parquet --output data/pv/2021/{summer_months}.parquet     \n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/{summer_months}.zarr.zip --output data/satellite-hrv/2020/{summer_months}.zarr.zip\n",
    "#           !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2021/{summer_months}.zarr.zip --output data/satellite-hrv/2021/{summer_months}.zarr.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load pv, hrv and indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pv data into a single dataframe\n",
    "pv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        pv.append(pd.read_parquet(f\"data/pv/{year}/{month}.parquet\").drop(\"generation_wh\", axis=1))  \n",
    "pv = pd.concat(pv)\n",
    "pv.index = pv.index.set_levels([pv.index.levels[0].tz_localize(None), pv.index.levels[1]])\n",
    " \n",
    "# The parquet data here is similar to a dataframe. The \"power\" is the column with the other data types being indexes. The data is shaped with each timestamp being its own \n",
    "# subframe with the sites having their corresponding power (% of site capacity).  \n",
    "hrv = []\n",
    "for year in [2020, 2021]:\n",
    "    for month in [6, 7, 8]:\n",
    "        hrv.append(xr.open_dataset(f\"data/satellite-hrv/{year}/{month}.zarr.zip\", engine=\"zarr\", chunks=\"auto\"))\n",
    "\n",
    "hrv = xr.concat(hrv, dim=\"time\")\n",
    "\n",
    "# Images are stored as vectors. The vectors are stored as an array of vectors. The arrays have a timestamp. Since there is only one channel (hrv)\n",
    "# the array is a 1D set of vectors with the dimension being time. Read this to help you understand how this is being stored \n",
    "# https://tutorial.xarray.dev/fundamentals/01_datastructures.html\n",
    "with open(\"indices.json\") as f:\n",
    "    site_locations = {\n",
    "        data_source: {\n",
    "            int(site): (int(location[0]), int(location[1]))\n",
    "            for site, location in locations.items() #if site == '2607'#added this to run only 1 site location to understand how it works\n",
    "        }\n",
    "        for data_source, locations in json.load(f).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train, validation and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1 # forecast horizon in hours\n",
    "crop_size = 1 # number of pixels of hrv image to crop around each site location\n",
    "BATCH_SIZE = 32 # number of samples per batch\n",
    "\n",
    "train_start_date = \"2020-06-01\"\n",
    "train_end_date = \"2020-07-01\"\n",
    "validation_start_date = \"2020-07-08\"\n",
    "validation_end_date = \"2020-07-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ChallengeDataset\n",
    "\n",
    "# train \n",
    "train_dataset = ChallengeDataset(pv, hrv, site_locations, start_date=train_start_date, end_date=train_end_date)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory = True)\n",
    "\n",
    "# validation\n",
    "validation_dataset = ChallengeDataset(pv, hrv, site_locations=site_locations, start_date=validation_start_date, end_date=validation_end_date) \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "# # test\n",
    "# test_dataset = ChallengeDataset(pv, hrv, site_locations,\n",
    "#                         start_date=\"2021-07-01\", end_date=\"2020-08-31\",\n",
    "#                         crop_size = crop_size, horizon = horizon)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 4, 4, 4] #For a deeper resnet with 16 total conv layers\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, out_channels, stride=stride)\n",
    "        self.conv2 = conv_block(out_channels, out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 12 #reduce the stride\n",
    "        self.initial = nn.Identity()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 12, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 48, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 96, layers[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        # Adjust this linear layer based on the concatenated size of HRV and PV features\n",
    "        self.fc = nn.Linear(96  + 12, 12)  \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pv, hrv ):\n",
    "        #print(\"Initial HRV shape:\", hrv.shape)  \n",
    "        #print(\"Initial PV shape:\", pv.shape) \n",
    "        #print(f\"{pv[0]}\")\n",
    "        x = self.initial(hrv)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(\"Shape after initial conv and maxpool:\", x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"Shape after ResNet_light blocks:\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "\n",
    "        if pv.dim() > 2:\n",
    "            pv = torch.flatten(pv, start_dim=1)\n",
    "\n",
    "        combined = torch.cat((x, pv), dim=1)\n",
    "\n",
    "        if self.fc.in_features != combined.shape[1]:\n",
    "            self.fc = nn.Linear(combined.shape[1], 12).to(combined.device)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(model, criterion, validation_dataloader):\n",
    "    model.eval() # This is used to set the model to evaluation mode\n",
    "    with torch.no_grad(): # This is used to stop the model from storing gradients\n",
    "        losses = []\n",
    "        for time_ids, site_id, pv_features, hrv_features, pv_targets in validation_dataloader:\n",
    "            pv_features, hrv_features, pv_targets = pv_features.to(device, dtype=torch.float), hrv_features.to(device, dtype=torch.float), pv_targets.to(device, dtype=torch.float)\n",
    "            predictions = model(pv_features, hrv_features)\n",
    "            loss = criterion(predictions, pv_targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    model.train() # This is used to set the model back to training mode\n",
    "    \n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 200: 0.131649367576465\n",
      "     Training Loss: 0.131649367576465\n",
      "Epoch 1, 400: 0.07859283935744316\n",
      "     Training Loss: 0.07859283935744316\n",
      "Epoch 1, 600: 0.0573826653463766\n",
      "     Training Loss: 0.0573826653463766\n",
      "Epoch 1, 800: 0.04693743366195122\n",
      "     Training Loss: 0.04693743366195122\n",
      "Epoch 1, 1000: 0.042656528956023974\n",
      "     Training Loss: 0.042656528956023974\n",
      "Epoch 1, 1200: 0.04388317736156751\n",
      "     Training Loss: 0.04388317736156751\n",
      "Epoch 1, 1400: 0.042152924912682334\n",
      "     Training Loss: 0.042152924912682334\n",
      "Epoch 1, 1600: 0.03998660125609604\n",
      "     Training Loss: 0.03998660125609604\n",
      "Epoch 1, 1800: 0.037808177487070986\n",
      "     Training Loss: 0.037808177487070986\n",
      "Epoch 1, 2000: 0.03673040943511296\n",
      "     Training Loss: 0.03673040943511296\n",
      "Epoch 1, 2200: 0.035283012432839975\n",
      "     Training Loss: 0.035283012432839975\n",
      "Epoch 1, 2400: 0.03315730240336658\n",
      "     Training Loss: 0.03315730240336658\n",
      "Epoch 1, 2600: 0.032297039485263385\n",
      "     Training Loss: 0.032297039485263385\n",
      "Epoch 1, 2800: 0.031258429354701155\n",
      "     Training Loss: 0.031258429354701155\n",
      "Epoch 1, 3000: 0.030271089536196085\n",
      "     Training Loss: 0.030271089536196085\n",
      "Epoch 1, 3200: 0.02992483823773\n",
      "     Training Loss: 0.02992483823773\n",
      "Epoch 1, 3400: 0.029990788407380665\n",
      "     Training Loss: 0.029990788407380665\n",
      "Epoch 1, 3600: 0.029783189240406502\n",
      "     Training Loss: 0.029783189240406502\n",
      "Epoch 1, 3800: 0.02958502195440049\n",
      "     Training Loss: 0.02958502195440049\n",
      "Epoch 1, 4000: 0.02923446026527381\n",
      "     Training Loss: 0.02923446026527381\n",
      "Epoch 1, 4200: 0.028800403731140595\n",
      "     Training Loss: 0.028800403731140595\n",
      "Epoch 1, 4400: 0.027958519055881134\n",
      "     Training Loss: 0.027958519055881134\n",
      "Epoch 1, 4600: 0.027789922910653884\n",
      "     Training Loss: 0.027789922910653884\n",
      "Epoch 1, 4800: 0.02810102046327908\n",
      "     Training Loss: 0.02810102046327908\n",
      "Epoch 1, 5000: 0.028213110372575467\n",
      "     Training Loss: 0.028213110372575467\n",
      "Epoch 1, 5200: 0.028175440159718092\n",
      "     Training Loss: 0.028175440159718092\n",
      "Epoch 1, 5400: 0.027692246757786185\n",
      "     Training Loss: 0.027692246757786185\n",
      "Epoch 1, 5600: 0.027132913941532024\n",
      "     Training Loss: 0.027132913941532024\n",
      "Epoch 1, 5800: 0.026588685039183976\n",
      "     Training Loss: 0.026588685039183976\n",
      "Epoch 1, 6000: 0.02598629278812829\n",
      "     Training Loss: 0.02598629278812829\n",
      "Epoch 1, 6200: 0.025731851005083656\n",
      "     Training Loss: 0.025731851005083656\n",
      "Epoch 1, 6400: 0.025820875921008336\n",
      "     Training Loss: 0.025820875921008336\n",
      "Epoch 1, 6600: 0.025891754306159618\n",
      "     Training Loss: 0.025891754306159618\n",
      "Epoch 1, 6800: 0.025863682938087388\n",
      "     Training Loss: 0.025863682938087388\n",
      "Epoch 1, 7000: 0.025642250636617455\n",
      "     Training Loss: 0.025642250636617455\n",
      "Epoch 1, 7200: 0.02529769701940596\n",
      "     Training Loss: 0.02529769701940596\n",
      "Epoch 1, 7400: 0.02518765254725696\n",
      "     Training Loss: 0.02518765254725696\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "EPOCHS = 1\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_validation_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0 ##sets the starting loss at zero\n",
    "    count = 0 #is used to keep track of the number of batches passed through the training model\n",
    "  \n",
    "    for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(train_dataloader): \n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(\n",
    "            pv_features.to(device, dtype=torch.float),\n",
    "            hrv_features.to(device, dtype=torch.float),\n",
    "        )\n",
    "        loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item() * pv_targets.size(0)\n",
    "        count += pv_targets.size(0)\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            \n",
    "            batch_loss = running_loss / count\n",
    "            training_losses.append(batch_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, {i + 1}: {batch_loss}\")\n",
    "            \n",
    "            # print(f\"     Training Loss: {batch_loss}\")\n",
    "\n",
    "            # validation_loss = model_validation(model, criterion, validation_dataloader)\n",
    "            # validation_losses.append(validation_loss)\n",
    "            # print(f\"     Validation Loss: {validation_loss}\\n\")\n",
    "            \n",
    "    \n",
    "    epoch_train_loss = running_loss / count\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # epoch_validation_loss = model_validation(model, criterion, validation_dataloader)\n",
    "    # epoch_validation_losses.append(epoch_validation_loss)\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}, Training Loss: {epoch_train_loss}\")\n",
    "    # print(f\"Epoch {epoch + 1}, Validation Loss: {epoch_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ResNet MSE\"\n",
    "os.makedirs(f\"models/experiments/MSEvMAE/{model_name}\", exist_ok=True)\n",
    "\n",
    "# Save the variables used to make the dataset to a text file\n",
    "with open(f\"models/experiments/MSEvMAE/{model_name}/data_summary.txt\", \"w\") as f:\n",
    "    f.write(\"BATCH_SIZE = \"+ str(BATCH_SIZE)+ \"\\n\"+ \n",
    "            \"train_start_date = \"+ train_start_date+ \"\\n\"+\n",
    "            \"train_end_date = \"+ train_end_date+ \"\\n\"+\n",
    "            \"validation_start_date = \"+ validation_start_date+ \"\\n\"+\n",
    "            \"validation_end_date = \"+ validation_end_date)\n",
    "    \n",
    "# Save the trained model for future predictions\n",
    "torch.save(model.state_dict(), f\"models/experiments/MSEvMAE/{model_name}/trained_model.pt\")\n",
    "\n",
    "# Create a DataFrame from the training_losses and validation_losses lists\n",
    "df = pd.DataFrame({'Training Losses': training_losses})\n",
    "# df = pd.DataFrame({'Training Losses': batch_losses, 'Validation Losses': validation_losses})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(f'models/experiments/MSEvMAE/{model_name}/losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (initial): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=108, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ResNet MSE\"\n",
    "model = ResNet(BasicBlock, layers).to(device)\n",
    "model.load_state_dict(torch.load(f\"models/experiments/MSEvMAE/week 1/{model_name}/trained_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_visual(dataloader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    predictions_list = []\n",
    "    timestamps_list = []\n",
    "    pv_targets_list = []  # List to store pv_targets for each batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (time_ids, site_id, pv_features, hrv_features, pv_targets) in enumerate(dataloader):\n",
    "            hrv_features = hrv_features.to(device, dtype=torch.float)\n",
    "            pv_features = pv_features.to(device, dtype=torch.float)\n",
    "            pv_targets = pv_targets.to(device, dtype=torch.float)\n",
    "            \n",
    "            batch_predictions = model(pv_features, hrv_features)\n",
    "            batch_predictions = batch_predictions.cpu().numpy()\n",
    "            batch_pv_targets = pv_targets.cpu().numpy()  # Convert pv_targets to numpy array\n",
    "\n",
    "            # Timestamp processing as before\n",
    "            if isinstance(time_ids[0], tuple) or isinstance(time_ids[0], list):\n",
    "                single_timestamp = time_ids[0][0]\n",
    "            else:\n",
    "                single_timestamp = time_ids[0]\n",
    "            if isinstance(single_timestamp, datetime):\n",
    "                timestamp = single_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            else:\n",
    "                timestamp = str(single_timestamp)\n",
    "            \n",
    "            # Append each batch's data to the lists\n",
    "            predictions_list.append(batch_predictions)\n",
    "            pv_targets_list.append(batch_pv_targets)  # Append pv_targets to its list\n",
    "            batch_timestamps = [timestamp] * batch_predictions.shape[0]\n",
    "            timestamps_list.extend(batch_timestamps)\n",
    "\n",
    "    # Concatenate all collected arrays into single numpy arrays\n",
    "    predictions = np.concatenate(predictions_list, axis=0)\n",
    "    pv_targets = np.concatenate(pv_targets_list, axis=0)  # Concatenate all pv_targets\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    pv_targets_df = pd.DataFrame(pv_targets, columns=[f'target_{i}' for i in range(pv_targets.shape[1])])\n",
    "    timestamps_df = pd.DataFrame(timestamps_list, columns=['timestamp'])\n",
    "\n",
    "    # Combine timestamps, predictions, and targets by using index alignment\n",
    "    final_df = pd.concat([timestamps_df, predictions_df, pv_targets_df], axis=1)\n",
    "    # final_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "prediction_df = _eval_visual(validation_dataloader, model, device)\n",
    "prediction_df[['date', 'time']] = prediction_df['timestamp'].str.split('T', expand=True)\n",
    "timestamp_index = prediction_df.columns.get_loc('timestamp')\n",
    "prediction_df.insert(timestamp_index, 'date', prediction_df.pop('date'))\n",
    "prediction_df.insert(timestamp_index + 1, 'time', prediction_df.pop('time'))\n",
    "prediction_df.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "prediction_df.to_csv(f'models/experiments/MSEvMAE/week 2/{model_name}_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
